{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindReddy123/Aravind_INFO5731_Spring2023/blob/main/In_class_exercise/In_class_exercise_02_02072023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfflrXmBLN-C"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SmZPYayLN-D"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j0lBM7dLN-D"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZRiBojALN-D"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "1. I have selected the subject of the recent WhatsApp data breach and how people have reported it on Twitter. \n",
        "   Based on the tweets on Twitter, we will determine if it is a disaster or not.\n",
        "\n",
        "2. We are gathering information from Twitter to diagnose the issue and then search for various solutions to the data leak reported by the public.\n",
        "\n",
        "3. We have employed web scraping as our method for gathering the data. \n",
        "   Data is taken from websites using a technique called web scraping. \n",
        "   The browser acts as the software's primary means of internet access. \n",
        "   This allows us to copy certain data types that we need.\n",
        "   \n",
        "4. Following data extraction, the model is trained using 80% of the dataset's data, and tested using 20% of the dataset's data.\n",
        "\n",
        "5. With the help of connected API keys and security tokens, we are collecting data using the web scraping technique. \n",
        "   After we successfully retrieve the data, we print it out to better comprehend and conduct additional study.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynza2plfLN-E"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mITPT0MNLN-E"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "#Module used to connect twitter from python\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "#Replace your creatential here, which are generated by creating a project in the twitter developer account.\n",
        "api_key = 'xkyCS3wiuTC25utUghkEi2qFT'\n",
        "api_key_secret = 'ifMyisEehlizBll9uv7csxN1FpmucUtlC5LxuTCGRvnUJFE5dg'\n",
        "access_token = '1622090667827552256-nlShHmkHKoCgwboNsKvHrH0ukApU1Y'\n",
        "access_token_secret = 'JFwsHi2ZBS9Fm5XJTSbgy6CXVlbvthuAB9hHL3kG3sbDc'\n",
        "\n",
        "\n",
        "#OauthHandler is the authetication function to autheticate the twitter developer users to acees the twitter data.\n",
        "authentication = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "#we are storing the access tokens because we don't neee to fetch the tokens everytime.\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "#Connecteing to twitter API using the authetication from previous steps\n",
        "twitter_api = tweepy.API(auth,wait_on_rate_limit=True)\n",
        "\n",
        "#Looping through pagination\n",
        "cursor = tweepy.Cursor(twitter_api.search_tweets, q=\"WhatsApp data leak\", tweet_mode = 'extended').items(1000)\n",
        "\n",
        "# making tabular structure format\n",
        "tweets = [[i. created_at, i.user.screen_name, i.full_text] for i in cursor]\n",
        "\n",
        "# Created the pandas dataframe from the lists above\n",
        "tweetsDf = pd.DataFrame(data=tweets,columns=['Time','Username','Tweet'])\n",
        "tweetsDf.to_csv('whatsapp.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCkXe8xXLN-E"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pghF2dBQLN-F"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "\n",
        "from selenium import webdriver\n",
        "import pandas as pd\n",
        "import re\n",
        "from selenium.common.exceptions import NoSuchElementException\n",
        "\n",
        "\n",
        "def sigir_proceeding(base_url, proceeding_ids):\n",
        "        for id in proceeding_ids:\n",
        "        comments = pd.DataFrame(\n",
        "            columns=['article_type', 'article_title', 'article_author', 'article_year', 'article_id',\n",
        "                     'article_abstract', 'total_citation', 'total_download', 'pdf_link'])\n",
        "        year = id[0]\n",
        "        url = base_url + id[1]\n",
        "        driver = webdriver.Chrome(r'chromedriver.exe')\n",
        "        driver.get(url)\n",
        "        session = driver.find_element_by_xpath(\"/html/body/div[1]/div/main/div[4]/div/div[2]/div[1]/div[1]/div[2]/div/div\")\n",
        "        every_sessions_in_url = session.text.split('\\n')\n",
        "        print(len(every_sessions_in_url))\n",
        "        for i in range(len(every_sessions_in_url)):\n",
        "            url_session = url+\"?tocHeading=heading\"+str(i+1)\n",
        "            driver.get(url_session)\n",
        "            articles_from_session = driver.find_elements_by_class_name(\"issue-item-container\")\n",
        "            for article in articles_from_session:\n",
        "                type_article = article.find_element_by_class_name(\"issue-heading\").text\n",
        "                if type_article == 'SECTION':\n",
        "                    continue\n",
        "                else:\n",
        "                    # meta-information\n",
        "                    article_info = article.find_element_by_class_name(\"issue-item__content\").text.split('\\n')\n",
        "                    print(article_info)\n",
        "                    # title\n",
        "                    title = article_info[0]\n",
        "                    #  author\n",
        "                    author = article_info[1]\n",
        "                    #  year\n",
        "                    if len(re.findall(r'2\\d\\d\\d', article_info[2])) > 0:\n",
        "                        year = re.findall(r'2\\d\\d\\d', article_info[2])[0]\n",
        "                    else:\n",
        "                        year = ''\n",
        "\n",
        "                    if len(re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "                                      article_info[2])) > 0:\n",
        "                        url_of_article = \\\n",
        "                            re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "                                       article_info[2])[0]\n",
        "                    else:\n",
        "                        url_of_article = ''\n",
        "                    if len(article_info)>3:\n",
        "                        abstract_of_article = article_info[3]\n",
        "                    else:\n",
        "                        abstract_of_article = ''\n",
        "                    if len(article_info)>4:\n",
        "                        total_citation = article_info[4]\n",
        "                    else:\n",
        "                        total_citation = ''\n",
        "                    if len(article_info)>5:\n",
        "                        total_download = article_info[5]\n",
        "                    else:\n",
        "                        total_download = ''\n",
        "                    try:\n",
        "                        link_of_pdf = article.find_element_by_class_name(\"btn--icon.simple-tooltip__block--b.red.btn\")\n",
        "                        if link_of_pdf != None:\n",
        "                            download_link = link_of_pdf.get_attribute('href')\n",
        "                            print(\"Download link\",download_link)\n",
        "                        else:\n",
        "                            download_link = ''\n",
        "                    except NoSuchElementException:\n",
        "                        download_link = ''\n",
        "                    comments.loc[len(comments)] = [type_article, title, author,year,url_of_article,abstract_of_article,total_citation,total_download,download_link]\n",
        "        comments.to_csv(str(year)+'_sigir_article_info.csv')\n",
        "\n",
        "\n",
        "\n",
        "    # proceeding_ids = [[2020,'10.1145/3397271']]\n",
        "proceeding_ids = [[2002,'10.1145/564376'],[2003,'10.1145/860435'],[2004,'10.1145/1008992'],[2005,'10.1145/1076034'], \n",
        "                      [2006,'10.1145/1148170'],[2007,'10.1145/1277741'], [2008,'10.1145/1390334'],[2009,'10.1145/1571941'],\n",
        "                      [2010,'10.1145/1835449'],[2011,'10.1145/2009916'],[2012,'10.1145/2348283'],[2013,'10.1145/2484028'],\n",
        "                      [2014,'10.1145/2600428'],[2015,'10.1145/2766462'],[2016,'10.1145/2911451'],[2017,'10.1145/3077136'],\n",
        "                      [2018,'10.1145/3209978'],[2019,'10.1145/3331184'],[2020,'10.1145/3397271']]\n",
        "base_url = 'https://dl.acm.org/doi/proceedings/'\n",
        "sigir_proceeding(base_url,proceeding_ids)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whlpSaEjLN-F"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhgrKMTsLN-F"
      },
      "outputs": [],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "#Module used to connect twitter from python\n",
        "import tweepy           \n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "\n",
        "\n",
        "api_key = 'xkyCS3wiuTC25utUghkEi2qFT'\n",
        "api_key_secret = 'ifMyisEehlizBll9uv7csxN1FpmucUtlC5LxuTCGRvnUJFE5dg'\n",
        "access_token = '1622090667827552256-nlShHmkHKoCgwboNsKvHrH0ukApU1Y'\n",
        "access_token_secret = 'JFwsHi2ZBS9Fm5XJTSbgy6CXVlbvthuAB9hHL3kG3sbDc'\n",
        "\n",
        "\n",
        "#OauthHandler is the authetication function to autheticate the twitter developer users to acees the twitter data.\n",
        "auth = tweepy.OAuthHandler(api_key, api_key_secret)\n",
        "#we are storing the access tokens because we don't neee to fetch the tokens everytime.\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "#Connecteing to twitter API using the authetication from previous steps\n",
        "twitter_api = tweepy.API(auth,wait_on_rate_limit=True) \n",
        "\n",
        "\n",
        "#Looping through pagination\n",
        "\n",
        "cursor = tweepy.Cursor(twitter_api.search, q=\"BBC News\", tweet_mode = 'extended').items(1000)\n",
        "\n",
        "# making tabular structure format\n",
        "tweets = [[i. created_at, i.user.screen_name, i.full_text] for i in cursor]\n",
        "\n",
        "# Created the pandas dataframe from the lists above\n",
        "tweetsDf = pd.DataFrame(data=tweets,columns=['Username','Time','Tweet'])\n",
        "tweetsDf.to_csv('BBC_NEWS.csv')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}