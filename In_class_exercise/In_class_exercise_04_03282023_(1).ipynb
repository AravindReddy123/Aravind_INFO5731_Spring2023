{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindReddy123/Aravind_INFO5731_Spring2023/blob/main/In_class_exercise/In_class_exercise_04_03282023_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2t_Gq3cMVV"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vszS_AFUcMVW"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PQIJbh5cMVX"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvM7gJqxcMVX",
        "outputId": "f844151c-976d-43db-b2f0-c47cb9ff68d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of topics: 2, coherence score: 0.04137097856439224\n",
            "Number of topics: 3, coherence score: 0.04137097856439224\n",
            "Number of topics: 4, coherence score: 0.04137097856439224\n",
            "Number of topics: 5, coherence score: 0.04137097856439224\n",
            "Number of topics: 6, coherence score: 0.04137097856439224\n",
            "Number of topics: 7, coherence score: 0.04137097856439224\n",
            "Number of topics: 8, coherence score: 0.04137097856439224\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of topics: 9, coherence score: 0.04137097856439224\n",
            "Number of topics: 10, coherence score: 0.041370978564392244\n",
            "Number of topics: 11, coherence score: 0.041370978564392244\n",
            "Number of topics: 12, coherence score: 0.041370978564392244\n",
            "Number of topics: 13, coherence score: 0.041370978564392244\n",
            "Number of topics: 14, coherence score: 0.041370978564392244\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of topics: 15, coherence score: 0.04137097856439224\n",
            "Number of topics: 16, coherence score: 0.04137097856439224\n",
            "Number of topics: 17, coherence score: 0.04137097856439224\n",
            "Number of topics: 18, coherence score: 0.04137097856439223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of topics: 19, coherence score: 0.04137097856439223\n",
            "Topic 0: content target\n",
            "Topic 1: target content\n",
            "Topic 2: target content\n",
            "Topic 3: target content\n",
            "Topic 4: target content\n",
            "Topic 5: target content\n",
            "Topic 6: content target\n",
            "Topic 7: content target\n",
            "Topic 8: content target\n",
            "Topic 9: content target\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "documents = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "#documents = [...] # your list of documents\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Preprocess the documents\n",
        "stop_words = stopwords.words('english')\n",
        "preprocessed_documents = []\n",
        "for document in documents:\n",
        "    tokens = nltk.word_tokenize(document.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    preprocessed_documents.append(tokens)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "dictionary = corpora.Dictionary(preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(document) for document in preprocessed_documents]\n",
        "\n",
        "# Compute coherence scores for different numbers of topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 20):\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "    print(f\"Number of topics: {k}, coherence score: {coherence_score}\")\n",
        "\n",
        "# Select the value of K that maximizes the coherence score\n",
        "best_k = coherence_scores.index(max(coherence_scores)) + 2\n",
        "\n",
        "# Train the final LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=best_k)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.show_topics(num_topics=best_k, formatted=False)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f\"Topic {i}: {' '.join([word[0] for word in topic[1]])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gWnQ7-VcMVY"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSSS0C3VcMVY",
        "outputId": "e815c998-9191-4359-d9ea-3539912fd12a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(0, '1.000*\"content\" + -0.000*\"target_names\" + -0.000*\"target\"')\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the corpus and preprocess the text\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "documents = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "texts = [[word for word in document.lower().split() if word not in stop_words] for document in documents]\n",
        "\n",
        "# Create a dictionary and a corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Determine the optimal number of topics based on coherence score\n",
        "coherence_scores = []\n",
        "for k in range(1, 10):\n",
        "    lsa_model = models.LsiModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lsa_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_scores.append(coherence_model.get_coherence())\n",
        "\n",
        "optimal_k = coherence_scores.index(max(coherence_scores)) + 1\n",
        "\n",
        "# Generate K topics using LSA\n",
        "lsa_model = models.LsiModel(corpus, num_topics=optimal_k, id2word=dictionary)\n",
        "topics = lsa_model.print_topics(num_topics=optimal_k)\n",
        "\n",
        "# Print the topics\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LztMlTX6gbW8",
        "outputId": "82b58b93-20c9-43f2-d6b9-82a81a5be930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.0-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.3.4 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.4.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.2.2)\n",
            "Collecting funcy\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (67.6.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (4.3.1)\n",
            "Collecting joblib>=1.2.0\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 KB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (1.10.1)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.9/dist-packages (from pyLDAvis) (2.8.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyLDAvis) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.3.4->pyLDAvis) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.1.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim->pyLDAvis) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->pyLDAvis) (2.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->pyLDAvis) (1.16.0)\n",
            "Installing collected packages: funcy, joblib, pyLDAvis\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.1\n",
            "    Uninstalling joblib-1.1.1:\n",
            "      Successfully uninstalled joblib-1.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 joblib-1.2.0 pyLDAvis-3.4.0\n"
          ]
        }
      ],
      "source": [
        "pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCaAem69cMVZ"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "FpUaGNkAcMVZ",
        "outputId": "bf9bb106-4f6c-4c7d-e7c6-622394e897b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-36c7c87ca17c>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'-PRON-'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Set up corpus dictionary and mapping for LDA2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import lda2vec\n",
        "import lda2vec.utils\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Load your corpus, in this example, I'll use a sample dataset of news articles\n",
        "#corpus_df = pd.read_csv('news_corpus.csv')\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "print(newsgroups)\n",
        "corpus_df = newsgroups.data\n",
        "print(corpus_df)\n",
        "\n",
        "# Preprocess the corpus (tokenization, stopword removal, stemming, etc.)\n",
        "# Here's an example using spaCy library\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-']\n",
        "\n",
        "corpus = [tokenize(doc) for doc in corpus_df['text']]\n",
        "\n",
        "# Set up corpus dictionary and mapping for LDA2Vec\n",
        "vocab = lda2vec.utils.flatten(corpus)\n",
        "word2id = dict(zip(vocab, range(len(vocab))))\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "corpus = [np.array([word2id[word] for word in doc]) for doc in corpus]\n",
        "\n",
        "# Train LDA2Vec model and determine optimal number of topics using coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 20):\n",
        "    # Train LDA2Vec with k topics\n",
        "    model = lda2vec.LDA2Vec(n_topics=k, n_iter=500, random_state=42)\n",
        "    model.fit(corpus)\n",
        "\n",
        "    # Calculate coherence score\n",
        "    m = model.mixture_means\n",
        "    s = model.mixture_variances\n",
        "    topics = m.dot(s).argsort()[:, ::-1]\n",
        "    tokens = collections.defaultdict(list)\n",
        "    for i, topic in enumerate(topics):\n",
        "        tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "    coherence_score = lda2vec.utils.evaluate(tokens)\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Select number of topics with highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train LDA2Vec model with optimal number of topics\n",
        "model = lda2vec.LDA2Vec(n_topics=optimal_k, n_iter=500, random_state=42)\n",
        "model.fit(corpus)\n",
        "\n",
        "# Print topics and top 10 words in each topic\n",
        "m = model.mixture_means\n",
        "s = model.mixture_variances\n",
        "topics = m.dot(s).argsort()[:, ::-1]\n",
        "tokens = collections.defaultdict(list)\n",
        "for i, topic in enumerate(topics):\n",
        "    tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "for i, words in tokens.items():\n",
        "    print(\"Topic {}: {}\".format(i+1, \", \".join(words)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6da4c4fb-bda4-4f35-af66-67413ba6c4cf",
        "id": "xlZNWtOCGhSY"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-8bd278269930>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_20newsgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlda2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# Load your corpus, in this example, I'll use a sample dataset of news articles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#corpus_df = pd.read_csv('news_corpus.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Corpus' from 'lda2vec' (/usr/local/lib/python3.9/dist-packages/lda2vec/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import lda2vec\n",
        "import lda2vec.utils\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from lda2vec import Corpus\n",
        "# Load your corpus, in this example, I'll use a sample dataset of news articles\n",
        "#corpus_df = pd.read_csv('news_corpus.csv')\n",
        "#categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "#newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "#print(newsgroups)\n",
        "corpus_df =documents = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(corpus_df)\n",
        "\n",
        "# Preprocess the corpus (tokenization, stopword removal, stemming, etc.)\n",
        "# Here's an example using spaCy library\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-']\n",
        "\n",
        "corpus = [tokenize(doc) for doc in corpus_df['content']]\n",
        "\n",
        "# Set up corpus dictionary and mapping for LDA2Vec\n",
        "corpus1 = Corpus()\n",
        "vocab = corpus1.compact_to_flat(corpus)\n",
        "word2id = dict(zip(vocab, range(len(vocab))))\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "corpus = [np.array([word2id[word] for word in doc]) for doc in corpus]\n",
        "\n",
        "# Train LDA2Vec model and determine optimal number of topics using coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 20):\n",
        "    # Train LDA2Vec with k topics\n",
        "    model = lda2vec.LDA2Vec(n_topics=k, n_iter=500, random_state=42)\n",
        "    model.fit(corpus)\n",
        "\n",
        "    # Calculate coherence score\n",
        "    m = model.mixture_means\n",
        "    s = model.mixture_variances\n",
        "    topics = m.dot(s).argsort()[:, ::-1]\n",
        "    tokens = collections.defaultdict(list)\n",
        "    for i, topic in enumerate(topics):\n",
        "        tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "    coherence_score = lda2vec.utils.evaluate(tokens)\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Select number of topics with highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train LDA2Vec model with optimal number of topics\n",
        "model = lda2vec.LDA2Vec(n_topics=optimal_k, n_iter=500, random_state=42)\n",
        "model.fit(corpus)\n",
        "\n",
        "# Print topics and top 10 words in each topic\n",
        "m = model.mixture_means\n",
        "s = model.mixture_variances\n",
        "topics = m.dot(s).argsort()[:, ::-1]\n",
        "tokens = collections.defaultdict(list)\n",
        "for i, topic in enumerate(topics):\n",
        "    tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "for i, words in tokens.items():\n",
        "    print(\"Topic {}: {}\".format(i+1, \", \".join(words)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e8RRh2h2iAw6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "488adc86-2e45-47cc-ce40-93ed0b38aa0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bertopic in /usr/local/lib/python3.9/dist-packages (0.14.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (4.65.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (0.5.3)\n",
            "Requirement already satisfied: hdbscan>=0.8.29 in /usr/local/lib/python3.9/dist-packages (from bertopic) (0.8.29)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.4.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.9/dist-packages (from bertopic) (1.2.2)\n",
            "Requirement already satisfied: plotly>=4.7.0 in /usr/local/lib/python3.9/dist-packages (from bertopic) (5.13.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (1.10.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (0.29.33)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.9/dist-packages (from hdbscan>=0.8.29->bertopic) (1.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->bertopic) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=4.7.0->bertopic) (8.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (3.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.27.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (1.13.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.13.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.97)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.14.1+cu116)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.5.0->bertopic) (0.56.4)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.9/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.10.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (67.6.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.39.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->bertopic) (1.16.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.1.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (8.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2022.12.7)\n"
          ]
        }
      ],
      "source": [
        "pip install bertopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gZ2G9scMVa"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Mg_t9_QacMVa"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Load the corpus\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "texts = newsgroups.data\n",
        "# Create an instance of BERTopic\n",
        "model = BERTopic()\n",
        "# Determine the optimal number of topics based on coherence score\n",
        "coherence_scores = {}\n",
        "model.fit(texts)\n",
        "topics, _ = model.transform(texts)\n",
        "topic_words = model.get_topic_info()\n",
        "topics_words_list = [topic_words[topic][\"tokens\"].split() for topic in topic_words.index]\n",
        "dictionary = gensim.corpora.Dictionary(topics_words_list)\n",
        "corpus = [dictionary.doc2bow(words) for words in topics_words_list]\n",
        "    #coherence_scores[k] = model.get_coherence_per_topic()\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):\n",
        "    # Create coherence model\n",
        "    coherence_model = gensim.models.CoherenceModel(topics=topics_words_list, corpus=corpus, dictionary=dictionary, coherence='c_v', num_topics=k)\n",
        "    \n",
        "    # Calculate coherence score\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "    print(f\"K={k}: Coherence Score={coherence_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3VewjwGcMVb"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osCRqGePcMVb"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}