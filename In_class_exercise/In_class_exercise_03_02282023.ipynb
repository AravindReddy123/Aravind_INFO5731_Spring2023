{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindReddy123/Aravind_INFO5731_Spring2023/blob/main/In_class_exercise/In_class_exercise_03_02282023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzKP_qbWhbua"
      },
      "source": [
        "## The third In-class-exercise (2/28/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAf1dyeThbub"
      },
      "source": [
        "The purpose of this exercise is to understand text representation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQbcWZ2Fhbub"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqDWohPXhbub"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Sentiment analysis, which entails detecting whether a particular piece of text reflects a positive or negative sentiment, is an intriguing task in text classification.\n",
        "\n",
        "Different features might be useful for sentiment analysis are:\n",
        " 1. Emotions of a given sentence: Since emoticons commonly convey emotional meaning, they can be used to replace emotions in text. Certain emoticons may appear to indicate either positive or negative emotion. For instance, a happy smile would convey something positive, but a sad face might suggest something negative.\n",
        " 2. Parts of a speech: Each word's part-of-speech (POS) tags can be used as features for sentiment analysis. This strategy can assist in capturing some of the grammatical structure of the text, which could be useful information for sentiment analysis. For instance, a sentence with a lot of adjectives may be conveying a cheerful attitude.\n",
        " 3.N-grams: N-grams can be used in addition to bag-of-words to help readers understand some of the context surrounding each word in the text. A bigram feature, for instance, can detect the co-occurrence of two words in the text, such as \"excellent movie,\" which may imply a favorable attitude.\n",
        " 4. Lexicons: There are a lot of lexicons that give words or phrases sentiment scores, and these lexicons can be used as features to capture the overall sentiment of a text. For instance, a high frequency of words with high sentiment ratings may indicate that the content is generally positive.\n",
        " 5. Bag-of-words: For text classification tasks, the bag-of-words feature representation technique is frequently used. It involves presenting a text as a collection of its words without taking into account their grammatical structure or sequence. Each word's inclusion or exclusion from the text might be used as a feature. This method can help in identifying various lexical and semantic cues that could point to positive or negative sentiment.\n",
        " \n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctObF_0ghbuc"
      },
      "source": [
        "Question 2 (10 points): Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Meite_enhbuc",
        "outputId": "d3f9a589-5b37-43e7-8a6b-b51a2fa223ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.sentiment.util import mark_negation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#Example text data\n",
        "text_data = [\n",
        "    \"This Special Issue of the IEEE Transactions on Plasma Science (TPS) follows the first American Physical Society Division of Plasma Physics (APS-DPP) mini-conference on Machine Learning, Data Science, and Artificial Intelligence in Plasma Research held during the 60th APS-DPP Meeting in Portland, OR, USA (November 5â€“9, 2018).\",\n",
        "    \"It contains selected highlights from not only the mini-conference but also the broader plasma physics community\",\n",
        "    \"Although data science has a long and rich history in plasma physics, dating back at least three decades, it is experiencing a renaissance, thanks in large part to the advances outside of plasma physics.\",\n",
        "    \"Emerging data-driven methods could have a transformative effect across the full spectrum of plasma research.\",\n",
        "    \"The DPP mini-conference and the articles herein represent only a tiny cross section of contemporary research on data-driven plasma science.\",\n",
        "    \"Furthermore, Plasma Science is not unique in its exploration of Scientific Machine Learning: the Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019, Vancouver, BC, Canada, December 2019) and it illustrates a trend in cross disciplinary collaboration with contributions from plasma research.\"\n",
        "]\n",
        "\n",
        "# Defining stop words of English and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# bag-of-words feature definition\n",
        "def bag_of_words(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    return words\n",
        "\n",
        "#ngrams feature definition\n",
        "def ngrams(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    bigrams = list(nltk.ngrams(words, 2))\n",
        "    trigrams = list(nltk.ngrams(words, 3))\n",
        "    return bigrams + trigrams\n",
        "\n",
        "#parts of speech tagging feature definition\n",
        "def pos_tags(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    tags = [tag[1] for tag in pos_tags if tag[0].isalpha() and tag[0].lower() not in stop_words]\n",
        "    return tags\n",
        "\n",
        "#Sentiment lexicons feature definition\n",
        "def sentiment_lexicons(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
        "    negated_words = mark_negation(words)\n",
        "    positive_words = [word for word in negated_words if word in positive_lexicon]\n",
        "    negative_words = [word for word in negated_words if word in negative_lexicon]\n",
        "    sentiment_score = len(positive_words) - len(negative_words)\n",
        "    return sentiment_score\n",
        "\n",
        "#Punctuation feature definition\n",
        "def punctuation(text):\n",
        "    exclamation_marks = text.count('!')\n",
        "    question_marks = text.count('?')\n",
        "    emoticons = len(re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text))\n",
        "    hashtags = len(re.findall(r'#\\w+', text))\n",
        "    return [exclamation_marks, question_marks, emoticons, hashtags]\n",
        "\n",
        "# Loading sentiment lexicons\n",
        "positive_lexicon = set(nltk.corpus.opinion_lexicon.positive())\n",
        "negative_lexicon = set(nltk.corpus.opinion_lexicon.negative())\n",
        "\n",
        "# Extracting bag-of-words from text\n",
        "bow_vectorizer = CountVectorizer(analyzer=bag_of_words)\n",
        "bow_features = bow_vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Extracting ngrams features from text\n",
        "ngrams_vectorizer = CountVectorizer(analyzer=ngrams)\n",
        "ngrams_features = ngrams_vectorizer.fit_transform(text_data)\n",
        "\n",
        "#Extracting parts of speech features from text\n",
        "pos_vectorizer = CountVectorizer(analyzer=pos_tags)\n",
        "pos_features = pos_vectorizer.fit_transform(text_data)\n",
        "\n",
        "sentiment_lexicons_features = [sentiment_lexicons(text) for text in text_data]\n",
        "\n",
        "#Extracting punctuation feature from text\n",
        "punctuation_features = [punctuation(text) for text in text_data]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGaVNdHhbuc"
      },
      "source": [
        "Question 3 (10 points): Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\" Select the most important features you extracted above, rank the features based on their importance in the descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr_b6koThbuc",
        "outputId": "7f824939-cac3-4abe-f067-3fc9631297c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 features based on Mutual Information:\n",
            "service 0.7833333333333331\n",
            "sentiment_score 0.7833333333333331\n",
            "('good', 'feature') 0.6833333333333331\n",
            "exclamation_marks 0.6166666666666664\n",
            "('acting', 'fantastic') 0.44999999999999973\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "#importing required libraries\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "import numpy as np\n",
        "\n",
        "#Combining bag-of-words, parts of speech, ngrams,punctuation, sentiment lexicons all features into single matrix\n",
        "all_features = []\n",
        "for i in range(len(text_data)):\n",
        "    features = list(bow_features[i].toarray()[0]) + list(ngrams_features[i].toarray()[0]) + list(pos_features[i].toarray()[0]) + [sentiment_lexicons_features[i]] + punctuation_features[i]\n",
        "    all_features.append(features)\n",
        "\n",
        "#Printing all text features\n",
        "print(all_features)\n",
        "\n",
        "# Select top 5 features\n",
        "k_best = SelectKBest(mutual_info_classif, k=5)\n",
        "k_best.fit(all_features, [1, 1, 0, 1, 0, 2])\n",
        "top_features = [i for i, score in sorted(enumerate(k_best.scores_), key=lambda x: x[1], reverse=True)][:5]\n",
        "\n",
        "\n",
        "# Top 5 features with their MI scores\n",
        "print(\"Top 5 features:\")\n",
        "for feature_index in top_features:\n",
        "  feature_names = list(bow_vectorizer.get_feature_names_out()) + list(ngrams_vectorizer.get_feature_names_out()) + list(pos_vectorizer.get_feature_names_out()) + list(['sentiment_score', 'exclamation_marks', 'question_marks', 'emoticons', 'hashtags'])\n",
        "  #feature_names = np.concatenate(bow_vectorizer.get_feature_names_out() + ngrams_vectorizer.get_feature_names_out() + pos_vectorizer.get_feature_names_out() + ['sentiment_score', 'exclamation_marks', 'question_marks', 'emoticons', 'hashtags'])\n",
        "  print(feature_names[feature_index], k_best.scores_[feature_index])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDg8G4wnhbuc"
      },
      "source": [
        "Question 4 (10 points): Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2KmeSw7Whbuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1383dd90-3471-4b5f-defc-1403367419a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.2.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.13.1+cu116)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.26.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from sentence-transformers) (0.14.1+cu116)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->sentence-transformers) (8.1.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->sentence-transformers) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\n",
            "Ranked text data based on similarity to query:\n",
            "Similarity score: 0.6244\tText: Emerging data-driven methods could have a transformative effect across the full spectrum of plasma research.\n",
            "Similarity score: 0.5856\tText: It contains selected highlights from not only the mini-conference but also the broader plasma physics community\n",
            "Similarity score: 0.5196\tText: Furthermore, Plasma Science is not unique in its exploration of Scientific Machine Learning: the Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019, Vancouver, BC, Canada, December 2019) and it illustrates a trend in cross disciplinary collaboration with contributions from plasma research.\n",
            "Similarity score: 0.5188\tText: The DPP mini-conference and the articles herein represent only a tiny cross section of contemporary research on data-driven plasma science.\n",
            "Similarity score: 0.4563\tText: This Special Issue of the IEEE Transactions on Plasma Science (TPS) follows the first American Physical Society Division of Plasma Physics (APS-DPP) mini-conference on Machine Learning, Data Science, and Artificial Intelligence in Plasma Research held during the 60th APS-DPP Meeting in Portland, OR, USA (November 5â€“9, 2018).\n",
            "Similarity score: 0.3876\tText: Although data science has a long and rich history in plasma physics, dating back at least three decades, it is experiencing a renaissance, thanks in large part to the advances outside of plasma physics.\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers\n",
        "#Importing requiured libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Example text data\n",
        "text_data = [\n",
        "    \"This Special Issue of the IEEE Transactions on Plasma Science (TPS) follows the first American Physical Society Division of Plasma Physics (APS-DPP) mini-conference on Machine Learning, Data Science, and Artificial Intelligence in Plasma Research held during the 60th APS-DPP Meeting in Portland, OR, USA (November 5â€“9, 2018).\",\n",
        "    \"It contains selected highlights from not only the mini-conference but also the broader plasma physics community\",\n",
        "    \"Although data science has a long and rich history in plasma physics, dating back at least three decades, it is experiencing a renaissance, thanks in large part to the advances outside of plasma physics.\",\n",
        "    \"Emerging data-driven methods could have a transformative effect across the full spectrum of plasma research.\",\n",
        "    \"The DPP mini-conference and the articles herein represent only a tiny cross section of contemporary research on data-driven plasma science.\",\n",
        "    \"Furthermore, Plasma Science is not unique in its exploration of Scientific Machine Learning: the Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019, Vancouver, BC, Canada, December 2019) and it illustrates a trend in cross disciplinary collaboration with contributions from plasma research.\"\n",
        "]\n",
        "# Defining query\n",
        "query = \"Exploring plasms physics through Data driven approach.\"\n",
        "\n",
        "# Loading BERT model\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "\n",
        "# Embedding query and text data\n",
        "query_embedding = model.encode([query], convert_to_tensor=True)\n",
        "text_embeddings = model.encode(text_data, convert_to_tensor=True)\n",
        "\n",
        "# Here we are calculating cosine similarity between query and text data\n",
        "similarity_scores = cosine_similarity(query_embedding, text_embeddings)\n",
        "\n",
        "#Ranking input text data based on similarity scores\n",
        "ranked_text_data = sorted(zip(similarity_scores.squeeze().tolist(), text_data), reverse=True)\n",
        "\n",
        "# Finally Printing ranked text data\n",
        "print(\"Ranked text data based on similarity to query:\")\n",
        "for score, text in ranked_text_data:\n",
        "    print(f\"Similarity score: {score:.4f}\\tText: {text}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}