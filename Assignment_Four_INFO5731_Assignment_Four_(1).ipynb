{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindReddy123/Aravind_INFO5731_Spring2023/blob/main/Assignment_Four_INFO5731_Assignment_Four_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "(1) Features (text representation) used for topic modeling.\n",
        "\n",
        "(2) Top 10 clusters for topic modeling.\n",
        "\n",
        "(3) Summarize and describe the topic for each cluster. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFPKhC0m1fd",
        "outputId": "37423430-5866-464d-d7c9-bca9fafcb908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n",
            "{\"total\": 102806, \"offset\": 0, \"next\": 100, \"data\": [{\"paperId\": \"1e85606f75af68884122f0d1d8393c000e915194\", \"abstract\": \"This Special Issue of the IEEE Transactions on Plasma Science (TPS) follows the first American Physical Society Division of Plasma Physics (APS-DPP) mini-conference on Machine Learning, Data Science, and Artificial Intelligence in Plasma Research held during the 60th APS-DPP Meeting in Portland, OR, USA (November 5\\u20139, 2018). It contains selected highlights from not only the mini-conference but also the broader plasma physics community. Although data science has a long and rich history in plasma physics, dating back at least three decades, it is experiencing a renaissance, thanks in large part to the advances outside of plasma physics. Novel algorithms, hardware, and analytic techniques (buoyed by the open source software ecosystem) have led plasma scientists to explore ways in which the data revolution could accelerate and inform scientific discovery. Emerging data-driven methods could have a transformative effect across the full spectrum of plasma research. For fusion energy research, some areas of opportunities [item 1) in the Appendix] include using machine learning (ML) or data methods for scientific discoveries, augmented instrumentation, accelerated model development and simulations, data-informed intelligent controls of the experiment, and data-enhanced predictions. The DPP mini-conference and the articles herein represent only a tiny cross section of contemporary research on data-driven plasma science. The 3rd International Conference on Data-Driven Plasma Science (ICDDPS-3) will be held in Okinawa, Japan, in April 2020 [item 2) in the Appendix], with expected presentations on fusion plasmas and low-temperature plasmas and beyond. Furthermore, Plasma Science is not unique in its exploration of Scientific Machine Learning: the Second Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019, Vancouver, BC, Canada, December 2019) and it illustrates a trend in cross disciplinary collaboration with contributions from plasma research.\"}, {\"paperId\": \"4440fcc65213f1ad176cc11c857304ef628468c2\", \"abstract\": null}, {\"paperId\": \"72d3ddf1f7210d7e70144bbc09f770ec411fe909\", \"abstract\": \"Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.\"}, {\"paperId\": \"a20c39f5439b84b57d883a21b698394d2347800d\", \"abstract\": \"Over the years, many clinical and engineering methods have been adapted for testing and screening for the presence of diseases. The most commonly used methods for diagnosis and analysis are computed tomography (CT) and X-ray imaging. Manual interpretation of these images is the current gold standard but can be subject to human error, is tedious, and is time-consuming. To improve efficiency and productivity, incorporating machine learning (ML) and deep learning (DL) algorithms could expedite the process. This article aims to review the role of artificial intelligence (AI) and its contribution to data science as well as various learning algorithms in radiology. We will analyze and explore the potential applications in image interpretation and radiological advances for AI. Furthermore, we will discuss the usage, methodology implemented, future of these concepts in radiology, and their limitations and challenges.\"}, {\"paperId\": \"ddb9ba8651e39d3c2e7a990c4b27049c20aff228\", \"abstract\": \"Machine learning (ML) is becoming capable of transforming biomolecular interaction description and calculation, promising an impact on molecular and drug design, chemical biology, toxicology, among others. The first improvements can be seen from biomolecule structure prediction to chemical synthesis, molecular generation, mechanism of action elucidation, inverse design, polypharmacology, organ or issue targeting of compounds, property and multiobjective optimization. Chemical design proposals from an algorithm may be inventive and feasible. Challenges remain, with the availability, diversity, and quality of data being critical for developing useful ML models; marginal improvement seen in some cases, as well as in the interpretability, validation, and reuse of models. The ultimate aim of ML should be to facilitate options for the scientist to propose and undertake ideas and for these to proceed faster. Applications are ripe for transformative results in understudied, neglected, and rare diseases, where new data and therapies are strongly required. Progress and outlook on these themes are provided in this study.\"}, {\"paperId\": \"27e84f9f18b0e5c3d7adee291135e0bdedbbe7dc\", \"abstract\": null}, {\"paperId\": \"db0fcfe01edbcd286e229197371faedc542bf2ad\", \"abstract\": null}, {\"paperId\": \"b877cd27c510454b6a7f734f5c698e5ba0683668\", \"abstract\": null}, {\"paperId\": \"a7ed05b7a88625f45cb4799054b868456885862c\", \"abstract\": null}, {\"paperId\": \"12d89245440d8c2a57c0741d10177189adf230d3\", \"abstract\": \"Abstract The Petaflops supercomputer \\u201cZhores\\u201d recently launched in the \\u201cCenter for Computational and Data-Intensive Science and Engineering\\u201d (CDISE) of Skolkovo Institute of Science and Technology (Skoltech) opens up new exciting opportunities for scientific discoveries in the institute especially in the areas of data-driven modeling, machine learning and artificial intelligence. This supercomputer utilizes the latest generation of Intel and NVidia processors to provide resources for the most compute intensive tasks of the Skoltech scientists working in digital pharma, predictive analytics, photonics, material science, image processing, plasma physics and many more. Currently it places 7th in the Russian and CIS TOP-50 (2019) supercomputer list. In this article we summarize the cluster properties and discuss the measured performance and usage modes of this new scientific instrument in Skoltech.\"}, {\"paperId\": \"e38ebef37b3046b19cbaec7a92ef03379ee3cc24\", \"abstract\": null}, {\"paperId\": \"405b18bd27021a0f9a845f9580c2572c9319543e\", \"abstract\": \"Over more than a decade ago, Baumeister, Vohs, and Funder (2007) asked: whatever happened to actual behaviour in psychology? Shortly after, Furr (2009a) published a target article here in EJP on personality psychology as a truly behavioural science and discussed the (low) prevalence, meaning, importance, and measurement of behavioural assessment (see also Furr, 2009b). Now, it is time to take stock: have we, as personality psychologists, moved towards using more behavioural assessment and understanding what people are actually doing in their personal, social, and occupational lives? Has new technology helped us achieve these goals? From the available literature, it would seem that behavioural measurement has been slowly on the rise but also that we still only rarely use actual behaviour as a data source (Nave, Feeney, & Furr, 2018). However, rapid technological advancements, the digitization of daily life, the ubiquity of data\\u2010gathering tools (e.g. smartphones), and the introduction of multi\\u2010modal sensing methodology, \\u2018big data\\u2019 applications, machine learning, and artificial intelligence approaches to psychological science promise more and perhaps even better behavioural assessment than was possible decades or even just some years ago. For example, what people speak (e.g. from audio snippets), write (e.g. on online social networks, text\\u2010massaging apps, and emails), or do (e.g. captured in videos via minicameras, geospatial movements via GPS, app usage on the smartphone and internet, gameplay, or pictures of activities posted on online social media platforms) in their daily lives yields intensive and massive amounts of data that need to be mined and modelled appropriately (Blake, Lee, de la Rosa, & Sherman, in press; Harari et al., 2016; Ilmini & Fernando, 2017; Kosinski, Stillwell, & Graepel, 2013; Onnela & Rauch, 2016; Vinciarelli & Mohammadi, 2014). Thus, to some, it may seem that personality research and assessment in particular, but probably basic and applied psychological science in general, may be entering a new age where large amounts of data on people\\u2019s actual behaviours, or traces of their behaviours, can be sampled or mined and modelled with sophisticated algorithms (Chamorro\\u2010Premuzic, Akhtar, Winsborough, & Sherman, 2017; Kosinski, Wang, Lakkaraju, & Leskovec, 2016; Mahmoodi, Leckelt, van Zalk, Geukes, & Back, 2017; Montag & Elhai, 2019; Tracey, 2020; Woo, Tay, & Proctor, 2020; Wright, 2014). OVERVIEW OF THE SPECIAL ISSUE\"}, {\"paperId\": \"5d5829723fb240543ff15ffeda1f63fff47f628d\", \"abstract\": \"In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.\"}, {\"paperId\": \"bdeead3785099725de95d2d3cd81fcd85211a1bb\", \"abstract\": \"About Zack and Yunus: Dr. Zacharias Voulgaris was born in Athens, Greece. He studied Production Engineering and Management at the Technical University of Crete, shifted to Computer Science through a Masters in Information Systems & Technology, and then to Data Science through a PhD on Machine Learning. He has worked at Georgia Tech as a Research Fellow, at an e-marketing startup in Cyprus as an SEO manager, and as a Data Scientist in both Elavon (GA) and G2 Web Services (WA). He also was a Program Manager at Microsoft, on a data analytics pipeline for Bing. Currently he is the CTO of Data Science Partnership Ltd. Zacharias has authored several books on Data Science, he mentors aspiring data scientists through Thinkful, and maintains a Data Science / AI blog.\"}, {\"paperId\": \"c5a639f438e1e94a8ff9214f4bbfbf29fcd11230\", \"abstract\": \"In this review, we aim to assess the current state of science in relation to the integration of patient\\u2010generated health data (PGHD) and patient\\u2010reported outcomes (PROs) into routine clinical care with a focus on surgical oncology populations. We will also describe the critical role of artificial intelligence and machine\\u2010learning methodology in the efficient translation of PGHD, PROs, and traditional outcome measures into meaningful patient care models.\"}, {\"paperId\": \"bb330ec0ff1e641b9890cf449c2d506d33517cd1\", \"abstract\": \"Business analytics use techniques from data science, data mining, artificial intelligence (especially, machine learning), mathematics and statistics to gain insights and understanding on the performance of business processes. The gained insights and knowledge help driving the business planning. As employees play important roles in the business process, having a tool to classify and predict their wage levels is desirable. Such classification and prediction enables the public or private sector to offer competitive wages for recruiting and retaining employees. In this paper, we present a tool for classifying and predicting wage levels. It incorporates fuzzy logic into a machine-learning tool to support business analytics on big data. Evaluation results show the applicability of our tool for classification and prediction of wages levels in the business world, which in turn supports business analytics in complex artificial intelligence environments.\"}, {\"paperId\": \"680ee0f2d99b2a45684229fd70a4444484d30670\", \"abstract\": null}, {\"paperId\": \"f927d665f4f7e52a54997181cc82c3d3378240f8\", \"abstract\": \"ion to accomplish the learning task. DL methods do not require the manual step of extracting/engineering features; however, it requires us to provide large amounts of data along with high-performance computing to obtain reliable results in a timely manner. This reference helps the engineers, geophysicists, and geoscientists get familiar with data science and analytics terminology relevant to subsurface characterization and demonstrates the use of datadriven methods for outlier detection, geomechanical/electromagnetic characterization, image analysis, fluid saturation estimation, and pore-scale characterization in the subsurface. Learn from 13 practical case studies using field, laboratory, and simulation data Become knowledgeable with data science and analytics terminology relevant to subsurface\"}, {\"paperId\": \"7bf0c5324275eab2f7d135d357ad348e8b29da15\", \"abstract\": \"The recent advancement in data science coupled with the revolution in digital and satellite technology has improved the potential for artificial intelligence (AI) applications in the forestry and wildlife sectors. India shares 7% of global forest cover and is the 8th most biodiverse region in the world. However, rapid expansion of developmental projects, agriculture, and urban areas threaten the country\\u2019s rich biodiversity. Therefore, the adoption of new technologies like AI in Indian forests and biodiversity sectors can help in effective monitoring, management, and conservation of biodiversity and forest resources. We conducted a systematic search of literature related to the application of artificial intelligence (AI) and machine learning algorithms (ML) in the forestry sector and biodiversity conservation across globe and in India (using ISI Web of Science and Google Scholar). Additionally, we also collected data on AI-based startups and non-profits in forest and wildlife sectors to understand the growth and adoption of AI technology in biodiversity conservation, forest management, and related services. Here, we first provide a global overview of AI research and application in forestry and biodiversity conservation. Next, we discuss adoption challenges of AI technologies in the Indian forestry and biodiversity sectors. Overall, we find that adoption of AI technology in Indian forestry and biodiversity sectors has been slow compared to developed, and to other developing countries. However, improving access to big data related to forest and biodiversity, cloud computing, and digital and satellite technology can help improve adoption of AI technology in India. We hope that this synthesis will motivate forest officials, scientists, and conservationists in India to explore AI technology for biodiversity conservation and forest management.\"}, {\"paperId\": \"6038eddd944c924a3240cab6ec3617bb761f746f\", \"abstract\": \"From Bluetooth enabled hearing aids to robotic caretakers, wearable and smart devices industries are immensely contributing to the development of the healthcare industry with the help of Internet of Things (IoT). Latest technologies like Artificial Intelligence, 3D Printing, Big data, Machine Learning, Advanced Sensors, Mobile Applications and other technologies will continue to generate lot of opportunities for Medtech organizations. Some of the latest healthcare innovations practiced at present might have been seen or read by some of us only in science fiction movies or science fiction stories a long ago. Presently, IoT and Artificial Intelligence is creating a revolution in healthcare industry when it comes to diagnosis and treatment of varied diseases. From smartphones to robots, artificial intelligence is already making its presence felt in healthcare industry and as such it is progressively recognizing the transformative nature of IoT technologies which drives innovation in the development of connected medical devices. Gradual increase in the number of connected medical devices with the advent of technology advancements helps to capture and transmit medical related data wherever and whenever required to the concerned people and thus, it gave birth to the Internet of Medical Things (IoMT), where the Internet of Things (IoT) and healthcare meet. The IoMT helps to constantly monitor and alter (if required) the behvaiour of the patient and his/her health status in real time and also supports healthcare organizations to effectively streamline clinical processes, patient information and related work flows to enhance its operational productivity. The IoMT has made and continues to make the delivery of P4 Medicine (Predictive, Preventive, Personalized and Participatory) even for remote locations with the help of connected sensors and devices helping in real-time patient care. IoMT helps doctors and caregivers to provide patient care and support by constantly monitoring data related to patients through mobile apps and connected medical devices even when patient(s) or doctor(s) are located at remote locations. This research paper discusses about six use cases explaining how IoMT is applied in healthcare industry.\"}, {\"paperId\": \"ee18ec08a0cdfd23e8cb85bb9c2421b88453e314\", \"abstract\": null}, {\"paperId\": \"8469b25830a0cbb3c1d238a00e5cfec28dc7222a\", \"abstract\": \"Artificial intelligence (AI) is the field of computer science that aims to build smart devices performing tasks that currently require human intelligence. Through machine learning (ML), the deep learning (DL) model is teaching computers to learn by example, something that human beings are doing naturally. AI is revolutionizing healthcare. Digital pathology is becoming highly assisted by AI to help researchers in analyzing larger data sets and providing faster and more accurate diagnoses of prostate cancer lesions. When applied to diagnostic imaging, AI has shown excellent accuracy in the detection of prostate lesions as well as in the prediction of patient outcomes in terms of survival and treatment response. The enormous quantity of data coming from the prostate tumor genome requires fast, reliable and accurate computing power provided by machine learning algorithms. Radiotherapy is an essential part of the treatment of prostate cancer and it is often difficult to predict its toxicity for the patients. Artificial intelligence could have a future potential role in predicting how a patient will react to the therapy side effects. These technologies could provide doctors with better insights on how to plan radiotherapy treatment. The extension of the capabilities of surgical robots for more autonomous tasks will allow them to use information from the surgical field, recognize issues and implement the proper actions without the need for human intervention.\"}, {\"paperId\": \"2cb5b20e852cdde009a8570c064fb6021f51b161\", \"abstract\": \"In the field of computer science, known as artificial intelligence, algorithms imitate reasoning tasks that are typically performed by humans. The techniques that allow machines to learn and get better at tasks such as recognition and prediction, which form the basis of clinical practice, are referred to as machine learning, which is a subfield of artificial intelligence. The number of artificial intelligence-and machine learnings-related publications in clinical journals has grown exponentially, driven by recent developments in computation and the accessibility of simple tools. However, clinicians are often not included in data science teams, which may limit the clinical relevance, explanability, workflow compatibility, and quality improvement of artificial intelligence solutions. Thus, this results in the language barrier between clinicians and artificial intelligence developers. Healthcare practitioners sometimes lack a basic understanding of artificial intelligence research because the approach is difficult for non-specialists to understand. Furthermore, many editors and reviewers of medical publications might not be familiar with the fundamental ideas behind these technologies, which may prevent journals from publishing high-quality artificial intelligence studies or, worse still, could allow for the publication of low-quality works. In this review, we aim to improve readers\\u2019 artificial intelligence literacy and critical thinking. As a result, we concentrated on what we consider the 10 most important qualities of artificial intelligence research: valid scientific purpose, high-quality data set, robust reference standard, robust input, no information leakage, optimal bias-variance tradeoff, proper model evaluation, proven clinical utility, transparent reporting, and open science. Before designing a study, one should have defined a sound scientific purpose. Then, it should be backed by a high-quality data set, robust input, and a solid reference standard. The artificial intelligence development pipeline should prevent information leakage. For the models, optimal bias-variance tradeoff should be achieved, and generalizability assessment must be adequately performed. The clinical value of the final models must also be established. After the study, thought should be given to transparency in publishing the process and results as well as open science for sharing data, code, and models. We hope this work may improve the artificial intelligence literacy and mindset of the readers.\"}, {\"paperId\": \"440492a4d6b7faf05e0302a0a2401d7109dc403c\", \"abstract\": \"and access data, select tools and workflows, and run analyses through collaborative workspaces. It will also deliver the organisational framework for the direction and management of the initiative\\u2019s common activities; (3) Research training programmes will create multi- tiered curricula to build skills in foundational health data science, with options ranging from master\\u2019s and doctoral degree tracks to postdoctoral training and faculty development; and (4) The ethical, legal and social implications (ELSI) projects will address data science issues that present challenges in Africa such as data privacy and ownership, cybersecurity and sensitivities concerning the use of geospatial information for research or public health surveillance. (B) Led by the Aga Khan University\\u2014East Africa, Kenya Medical Research Institute- Wellcome Trust Research Programme, and the University of Michigan, the research hub will implement two research projects around maternal, newborn and child health as well as mental health, which will be supported by three cores: Admin core, Data Management and Analysis Core (DMAC) and Dissemination and Sustainability Core (DSC). The Admin Core will lead the UZIMA- DS researchhub, fostering synergy and integration of all hub components and partnerships and facilitating participation in DS- I cross- consortium activities. The DMAC will employ FAIR (Findable, Accessible, Interoperable, Reusable) principles to support the hub\\u2019s data ecosystem through data governance, facilitating data analytics within the projects, and fostering data sharing and interoperability throughout the greater DS- I Africa consortium. The DSC will promote engagement with stakeholders to identify sustainable model dissemination pathways into target communities. Through multisectoral partnerships with government, healthcare and non- profit sectors, the core will: facilitate the development of best practices and policies with stakeholders using data- driven approaches to inform guidelines; and promote engagement with private sectors to explore sustainable commercialisation opportunities and pathways. UZIMA- DS, UtiliZing Health Information for Meaningful Impact in East Africa Through Data Science.\"}, {\"paperId\": \"f5be6caa62af24e3556c162065efd29c7a3d062f\", \"abstract\": \"The food processing and handling industry is the most significant business among the various manufacturing industries in the entire world that subsidize the highest employability. The human workforce plays an essential role in the smooth execution of the production and packaging of food products. Due to the involvement of humans, the food industries are failing to maintain the demand-supply chain and also lacking in food safety. To overcome these issues of food industries, industrial automation is the best possible solution. Automation is completely based on artificial intelligence (AI) or machine learning (ML) or deep learning (DL) algorithms. By using the AI-based system, food production and delivery processes can be efficiently handled and also enhance the operational competence. This article is going to explain the AI applications in the food industry which recommends a huge amount of capital saving with maximizing resource utilization by reducing human error. Artificial intelligence with data science can improve the quality of restaurants, cafes, online delivery food chains, hotels, and food outlets by increasing production utilizing different fitting algorithms for sales prediction. AI could significantly improve packaging, increasing shelf life, a combination of the menu by using AI algorithms, and food safety by making a more transparent supply chain management system. With the help of AI and ML, the future of food industries is completely based on smart farming, robotic farming, and drones.\"}, {\"paperId\": \"1b26a37789a7fd09cc5c164257a97490276c782c\", \"abstract\": \"Machine learning, as a tool, has become critical for decision-making mechanisms in the modern world. It has applications in a wide range of areas, including finance, healthcare, justice, and transportation. Unfortunately, machine learning is often considered as a \\u201cblack box\\u201d. As such, recommendations made by machine learning techniques, as well as the reasoning behind those recommendations, are not easily understood by humans. In this paper, we present an explainable artificial intelligence (XAI) solution that integrates and enhances state-of-the-art techniques to produce understandable and practical explanations to end-users. To evaluate the effectiveness of our XAI solution for data science, we conduct a case study on applying our solution to explaining a random forest-based predictive model on customer churn. Results show the practicality and usefulness of our XAI solution in practical applications such as data science on customer churn.\"}, {\"paperId\": \"a034a02ec75437b30e2438efa05ded5f174afc5f\", \"abstract\": \"With the exponential growth of computational systems and increased patient data acquisition, dental research faces new challenges to manage a large quantity of information. For this reason, data science approaches are needed for the integrative diagnosis of multifactorial diseases, such as Temporomandibular joint (TMJ) Osteoarthritis (OA). The Data science spectrum includes data capture/acquisition, data processing with optimized web-based storage and management, data analytics involving in-depth statistical analysis, machine learning (ML) approaches, and data communication. Artificial intelligence (AI) plays a crucial role in this process. It consists of developing computational systems that can perform human intelligence tasks, such as disease diagnosis, using many features to help in the decision-making support. Patient's clinical parameters, imaging exams, and molecular data are used as the input in cross-validation tasks, and human annotation/diagnosis is also used as the gold standard to train computational learning models and automatic disease classifiers. This paper aims to review and describe AI and ML techniques to diagnose TMJ OA and data science approaches for imaging processing. We used a web-based system for multi-center data communication, algorithms integration, statistics deployment, and process the computational machine learning models. We successfully show AI and data-science applications using patients' data to improve the TMJ OA diagnosis decision-making towards personalized medicine.\"}, {\"paperId\": \"e8a89cffda5e1182eede4db17f3159002e8086b7\", \"abstract\": null}, {\"paperId\": \"4692960b0423e297fe9404d0315a899dbcac7357\", \"abstract\": \"As we are fast approaching the beginning of a paradigm shift in the field of science, Data driven science (the so called fourth science paradigm) is going to be the driving force in research and innovation. From medicine to biodiversity and astronomy to geology, all these terms are somehow going to be affected by this paradigm shift. The huge amount of data to be processed under this new paradigm will be a major concern in the future and one will strongly require cloud based services in all the aspects of these computations (from storage to compute and other services). Another aspect will be energy consumption and performance of prediction jobs and tasks within such a scientific paradigm which will change the way one sees computation. Data science has heavily impacted or rather triggered the emergence of Machine Learning, Signal/Image/Video processing related algorithms, Artificial intelligence, Robotics, health informatics, geoinformatics, and many more such areas of interest. Hence, we envisage an era where Data science can deliver its promises with the help of the existing cloud based platforms and services with the addition of new services. In this article, we discuss about data driven science and Machine learning and how they are going to be linked through cloud based services in the future. It also discusses the rise of paradigms like approximate computing, quantum computing and many more in recent times and their applicability in big data processing, data science, analytics, prediction and machine learning in the cloud environments.\"}, {\"paperId\": \"47657d9d7ae5bf99ed44b5deac86b0ce833dc2e0\", \"abstract\": \"Trends of teaching and learning has changed its shape from offline teaching to online teaching as full mode and physical mode of teaching may become substitution of academic keeping in view the pandemic covid-19. Data science has become part of parcel of our daily life and most of the technical apps we are using contains machine Learning algorithms and helps us in many ways. With rising conditions, artificial intelligence will be the most prominent transformative technology and enabler for society in the present era. here is no uncertainty that AI and analogous frameworks are built to change global efficiency, working habits, and lifestyles and support healthcare, Pharma Industry and Transformation in diagnosis process, disease treatment and early identification of symptoms has been fuelled machine learning techniques and tools such as Generative Adversarial Networks (GAN), Deep Convolutional Networks, Deep Reinforcement Learning (DRL), Gradient-boosted-tree models (GBM), etc. MRI and other sophisticated imaging systems immensely used for neural disorders, cancer diagnostics. In this chapter we are discussing various resources of medical datasets which can be used for diagnosis of dementia with the usage of machine learning approaches. We are presenting how various machine learning approaches can be useful in early diagnosis of many diseases and explained where machine learning and deep learning can be used on electronically stored medical data. Recent developments are achieved in what way machine learning can be applicable in multi-disciplinary research areas. The main emphasis of this chapter is to elaborate on the applicability of machine learning in the domain of healthcare. In the past, there had been substantial signs of progress in the way where machine learning can apply in innumerable research and industries. This chapter deliberates the prospect of using machine learning technologies in the healthcare sector and sketches several industry ingenuities implementing machine learning initiatives. \\u00a9 2021 Scrivener Publishing LLC.\"}, {\"paperId\": \"e99a572e5c809f979339aceacebb81dab8ec7c10\", \"abstract\": null}, {\"paperId\": \"0018e888e83bfd993bc49455a8787577ab28a19c\", \"abstract\": \"Abstract Background Artificial intelligence (AI) and machine learning (ML) are poised to transform infectious disease testing. Uniquely, infectious disease testing is technologically diverse spaces in laboratory medicine, where multiple platforms and approaches may be required to support clinical decision-making. Despite advances in laboratory informatics, the vast array of infectious disease data is constrained by human analytical limitations. Machine learning can exploit multiple data streams, including but not limited to laboratory information and overcome human limitations to provide physicians with predictive and actionable results. As a quickly evolving area of computer science, laboratory professionals should become aware of AI/ML applications for infectious disease testing as more platforms are become commercially available. Content In this review we: (a) define both AI/ML, (b) provide an overview of common ML approaches used in laboratory medicine, (c) describe the current AI/ML landscape as it relates infectious disease testing, and (d) discuss the future evolution AI/ML for infectious disease testing in both laboratory and point-of-care applications. Summary The review provides an important educational overview of AI/ML technique in the context of infectious disease testing. This includes supervised ML approaches, which are frequently used in laboratory medicine applications including infectious diseases, such as COVID-19, sepsis, hepatitis, malaria, meningitis, Lyme disease, and tuberculosis. We also apply the concept of \\u201cdata fusion\\u201d describing the future of laboratory testing where multiple data streams are integrated by AI/ML to provide actionable clinical knowledge.\"}, {\"paperId\": \"0fa420a8dc7aaba6177419fc220a5edf03729c18\", \"abstract\": \"As the concept and implementation of cutting-edge technologies like artificial intelligence and machine learning has become relevant, academics, researchers and information professionals involve research in this area. The objective of this systematic literature review is to provide a synthesis of empirical studies exploring application of artificial intelligence and machine learning in libraries. To achieve the objectives of the study, a systematic literature review was conducted based on the original guidelines proposed by Kitchenham et al. (2009). Data was collected from Web of Science, Scopus, LISA and LISTA databases. Following the rigorous/ established selection process, a total of thirty-two articles were finally selected, reviewed and analyzed. Thirty-two papers were identified, analyzed and summarized on the application of AI and ML domain and techniques which are most often used. Findings show that The current state of the AI and ML research that is relevant with the LIS domain mainly focuses on theoretical works. However, some researchers also emphasized on implementation projects or case studies. For collection management in libraries, several Ml techniques like logistic regression, KNN, AdaBoost have been widely used for Metadata generation, resource discovery; and Book acquisition. Whereas for circulation (book recommendation, user rating, bibliographic data etc.) recommender system, SVM, association rule have been utilized. Library in-house activities like; cataloguing, classification, indexing, document analysis, text recognition etc., have been supported by both AI and ML technologies. Some advanced AI and ML techniques like pattern recognition and MAS are also being used to ensure library security; user identification; book title recognition; RFID management, and other administration activities. Deep learning, neural network algorithms, convolutional neural networks have also been proved as powerful tools for scholarship, collections discovery, search and analysis. Besides, an artificially intelligent conversational agent or chatbot works as a virtual reference librarian. It enhances face-to-face human interaction for library web site tour guides, automated virtual reference assistants, readers\\u2019 advisory-librarians, and virtual story-tellers. This study could help in the development of new ideas and models or tools to support and enhance the existing service ecologies of libraries. This study will provide a panoramic view of AI and ML in libraries for researchers, practitioners and educators for furthering the more technology-oriented approaches, and anticipating future innovation pathways.\"}, {\"paperId\": \"8ce43ac9574435ec0fb9d3d4d09731ad9ff817d5\", \"abstract\": \"A general issue in climate science is the handling of big data and running complex and computationally heavy simulations. In this paper, we explore the potential of using machine learning (ML) to spare computational time and optimize data usage. The paper analyzes the effects of changes in land cover (LC), such as deforestation or urbanization, on local climate. Along with green house gas emission, LC changes are known to be important causes of climate change. ML methods were trained to learn the relation between LC changes and temperature changes. The results showed that random forest (RF) outperformed other ML methods, and especially linear regression models representing current practice in the literature. Explainable artificial intelligence (XAI) was further used to interpret the RF method and analyze the impact of different LC changes on temperature. The results mainly agree with the climate science literature, but also reveal new and interesting findings, demonstrating that ML methods in combination with XAI can be useful in analyzing the climate effects of LC changes. All parts of the analysis pipeline are explained including data pre-processing, feature extraction, ML training, performance evaluation, and XAI.\"}, {\"paperId\": \"cfdb62f6d3909dea930b7a4491dc7bab8b8cd06d\", \"abstract\": null}, {\"paperId\": \"0d5ecb20f40ee1aefdff04543d1f7d508c61ffd5\", \"abstract\": \"Artificial intelligence and machine learning (AI-ML) have taken center stage in medical imaging. To develop as leaders in AI-ML, radiology residents may seek a formative data science experience. The authors piloted an elective Data Science Pathway (DSP) for 4th-year residents at the authors' institution in collaboration with the MGH & BWH Center for Clinical Data Science (CCDS). The goal of the DSP was to provide an introduction to AI-ML through a flexible schedule of educational, experiential, and research activities. The study describes the initial experience with the DSP tailored to the AI-ML interests of three senior radiology residents. The authors also discuss logistics and curricular design with common core elements and shared mentorship. Residents were provided dedicated, full-time immersion into the CCDS work environment. In the initial DSP pilot, residents were successfully integrated into AI-ML projects at CCDS. Residents were exposed to all aspects of AI-ML application development, including data curation, model design, quality control, and clinical testing. Core concepts in AI-ML were taught through didactic sessions and daily collaboration with data scientists and other staff. Work during the pilot period led to 12 accepted abstracts for presentation at national meetings. The DSP is a feasible, well-rounded introductory experience in AI-ML for senior radiology residents. Residents contributed to model and tool development at multiple stages and were academically productive. Feedback from the pilot resulted in establishment of a formal AI-ML curriculum for future residents. The described logistical, planning, and curricular considerations provide a framework for DSP implementation at other institutions. Supplemental material is available for this article. \\u00a9 RSNA, 2020.\"}, {\"paperId\": \"91cf25a2d684008f32be0867b34697bc6ec31063\", \"abstract\": \"Today Google collects big data, and develops new business using both cloud computing such as search engine and artificial intelligence such as machine learning. The study of artificial intelligence mainly on deep learning is prosperous and is expected to make smart life now and in the future. The mainstream of the present artificial intelligence is machine learning, and the foundation is inductive inference. On the other hand, artificial intelligence from old days is deductive inference, and the deductive inference is the foundation of software science. In this paper, from the viewpoint of \\u201cartificial intelligence = machine learning + software science\\u201d, we grasp computer technologies. We pay attention to software science, especially deductive inference of embedded systems, and explain deductive verifications for guaranteeing the reliability of embedded systems.\"}, {\"paperId\": \"bb45416703a14cf1ed3354ecafe7551d0fde51db\", \"abstract\": null}, {\"paperId\": \"a94ce8d1d37be4c9226028c490a7820fe712ac8c\", \"abstract\": \"As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics.\"}, {\"paperId\": \"f2ff2b36bb3e0da625c042bd639f71a06e327699\", \"abstract\": \"Although machine learning is frequently associated with neural networks, it also comprises econometric regression approaches and other statistical techniques whose accuracy enhances with increasing observation. What constitutes high quality machine learning is yet unclear though. Proponents of deep learning (i.e. neural networks) value computational efficiency over human interpretability and tolerate the \\u2018black box\\u2019 appeal of their algorithms, whereas proponents of explainable artificial intelligence (xai) employ traceable \\u2018white box\\u2019 methods (e.g. regressions) to enhance explainability to human decision makers. We extend Brooks et al.\\u2019s [2019. \\u2018Financial Data Science: The Birth of a New Financial Research Paradigm Complementing Econometrics?\\u2019 European Journal of Finance 25 (17): 1627\\u201336.] work on significance and relevance as assessment critieria in econometrics and financial data science to contribute to this debate. Specifically, we identify explainability as the Achilles heel of classic machine learning approaches such as neural networks, which are not fully replicable, lack transparency and traceability and therefore do not permit any attempts to establish causal inference. We conclude by suggesting routes for future research to advance the design and efficiency of \\u2018white box\\u2019 algorithms.\"}, {\"paperId\": \"4a135d2cddf5101ac64925440d96de5c7926537e\", \"abstract\": \"We describe the current and future plans for using artificial intelligence and machine learning (AI/ML) methods at the National Synchrotron Light Source II (NSLS-II), a scientific user facility at the Brookhaven National Laboratory. We discuss the opportunity for using the AI/ML tools and techniques developed in the data and computational science areas to greatly improve the scientific output of large scale experimental user facilities. We describe our current and future plans in areas including from detecting and recovering from faults, optimizing the source and instrument configurations, streamlining the pipeline from measurement to insight, through data acquisition, processing, analysis. The overall strategy and direction of the NSLS-II facility in relation to AI/ML is presented.\"}, {\"paperId\": \"be63d5e541df10e6ea3f671a8114f72521defe90\", \"abstract\": \"This paper seeks to provide a thorough account of the ubiquitous nature of the Bayesian paradigm in modern statistics, data science and artificial intelligence. Once maligned, on the one hand by those who philosophically hated the very idea of subjective probability used in prior specification, and on the other hand because of the intractability of the computations needed for Bayesian estimation and inference, the Bayesian school of thought now permeates and pervades virtually all areas of science, applied science, engineering, social science and even liberal arts, often in unsuspected ways. Thanks in part to the availability of powerful computing resources, but also to the literally unavoidable inherent presence of the quintessential building blocks of the Bayesian paradigm in all walks of life, the Bayesian way of handling statistical learning, estimation and inference is not only mainstream but also becoming the most central approach to learning from the data. This paper explores some of the most relevant elements to help to the reader appreciate the pervading power and presence of the Bayesian paradigm in statistics, artificial intelligence and data science, with an emphasis on how the Gospel according to Reverend Thomas Bayes has turned out to be the truly good news, and in some cases the amazing saving grace, for all who seek to learn statistically from the data.\"}, {\"paperId\": \"f7a3199da31504a23b0a59e4186ea4d98ed7276e\", \"abstract\": \"Machine learning (ML) is revolutionizing anesthesiology research. Unlike classical research methods that are largely inference-based, ML is geared more towards making accurate predictions. ML is a field of artificial intelligence concerned with developing algorithms and models to perform prediction tasks in the absence of explicit instructions. Most ML applications, despite being highly variable in the topics that they deal with, generally follow a common workflow. For classification tasks, a researcher typically tests various ML models and compares the predictive performance with the reference logistic regression model. The main advantage of ML lies in its ability to deal with many features with complex interactions and its specific focus on maximizing predictive performance. However, emphasis on data-driven prediction can sometimes neglect mechanistic understanding. This article mainly focuses on the application of supervised ML to electronic health record (EHR) data. The main limitation of EHR-based studies is in the difficulty of establishing causal relationships. However, the associated low cost and rich information content provide great potential to uncover hitherto unknown correlations. In this review, the basic concepts of ML are introduced along with important terms that any ML researcher should know. Practical tips regarding the choice of software and computing devices are also provided. Towards the end, several examples of successful ML applications in anesthesiology are discussed. The goal of this article is to provide a basic roadmap to novice ML researchers working in the field of anesthesiology.\"}, {\"paperId\": \"28e8c447f393884e231e1724996581cab8e8b326\", \"abstract\": null}, {\"paperId\": \"44e5a5f9761180a32d1f3d84c760c27997d13c28\", \"abstract\": \"Artificial intelligence (AI) and machine learning (ML) are powerful tools that can be utilized to overcome some of the clinical problems that orthodontists face daily. With the availability of more data, better AI and ML systems should be expected to be developed that will help orthodontists to practise more efficiently and improve the quality of care. AI is a subfield of computer science concerned with developing computers and programs that have the ability to perceive information and reason, and ultimately, convert that information into intelligent actions. The future may be purely digitized, at the comforts of our home, with orthodontists developing neural programs with orthodontic decision markers to aid in developing AI for patients to take less visits, make more use of their time using orthodontic appliances, and enhance the quality of work. This article will briefly discuss the contributions AI and ML in orthodontics, its history and various uses in orthodontics in specific, and the possibility of development.\"}, {\"paperId\": \"7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea\", \"abstract\": null}, {\"paperId\": \"89e5b63ade995059cf3dfd9580a59b2291e63564\", \"abstract\": \"Electrocatalysts and photocatalysts are key to a sustainable future, generating clean fuels, reducing the impact of global warming, and providing solutions to environmental pollution. Improved processes for catalyst design and a better understanding of electro/photocatalytic processes are essential for improving catalyst effectiveness. Recent advances in data science and artificial intelligence have great potential to accelerate electrocatalysis and photocatalysis research, particularly the rapid exploration of large materials chemistry spaces through machine learning. Here a comprehensive introduction to, and critical review of, machine learning techniques used in electrocatalysis and photocatalysis research are provided. Sources of electro/photocatalyst data and current approaches to representing these materials by mathematical features are described, the most commonly used machine learning methods summarized, and the quality and utility of electro/photocatalyst models evaluated. Illustrations of how machine learning models are applied to novel electro/photocatalyst discovery and used to elucidate electrocatalytic or photocatalytic reaction mechanisms are provided. The review offers a guide for materials scientists on the selection of machine learning methods for electrocatalysis and photocatalysis research. The application of machine learning to catalysis science represents a paradigm shift in the way advanced, next-generation catalysts will be designed and synthesized.\"}, {\"paperId\": \"4f0035818ee8ff659ef0da726cf80bcc2322d2eb\", \"abstract\": \"Artificial intelligence (AI) has been referred to as the \\u201cfourth paradigm of science,\\u201d and as part of a coherent toolbox of data\\u2010driven approaches, machine learning (ML) dramatically accelerates the computational discoveries. As the machinery for ML algorithms matures, significant advances have been made not only by the mainstream AI researchers, but also those work in computational materials science. The number of ML and artificial neural network (ANN) applications in the computational materials science is growing at an astounding rate. This perspective briefly reviews the state\\u2010of\\u2010the\\u2010art progress in some supervised and unsupervised methods with their respective applications. The characteristics of primary ML and ANN algorithms are first described. Then, the most critical applications of AI in computational materials science such as empirical interatomic potential development, ML\\u2010based potential, property predictions, and molecular discoveries using generative adversarial networks (GAN) are comprehensively reviewed. The central ideas underlying these ML applications are discussed, and future directions for integrating ML with computational materials science are given. Finally, a discussion on the applicability and limitations of current ML techniques and the remaining challenges are summarized.\"}, {\"paperId\": \"be6089edff6e15e8e8759c398aa316ce2cdd8cb8\", \"abstract\": \"The history of Artificial Intelligence (AI) development dates to the 40s. The researchers showed strong expectations until the 70s, when they began to encounter serious difficulties and investments were greatly, reduced. With the introduction of the Industry 4.0, one of the techniques adopted for AI implementation is Machine Learning (ML) that focuses on the machines ability to receive data series and learn on their own. Given the considerable importance of the subject, researchers have completed many studies on ML to ensure that machines are able to replace or relieve human tasks. This research aims to analyze, systematically, the literature on several aspects, including publication year, authors, scientific sector, country, institution, keywords. Analyzing existing literature on AI is a necessary stage to recommend policy on the matter. The analysis has been done using Web of Science and SCOPUS database. Furthermore, UCINET and NVivo 12 software have been used to complete them. Literature review on ML and AI empirical studies published in the last century was carried out to highlight the evolution of the topic before and after Industry 4.0 introduction, from 1999 to now. Eighty-two articles were reviewed and classified. A first interesting result is the greater number of works published by USA and the increasing interest after the birth of Industry 4.0.\"}, {\"paperId\": \"67168b98ee46e1ebd23d4b51bfadbaa2de145558\", \"abstract\": \"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of \\u201clearning\\u201d through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2\\u20137 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.\"}, {\"paperId\": \"74d922999942307462eb8f6206c421f59ced3218\", \"abstract\": null}, {\"paperId\": \"7f5271c79af7ea6806770c8febf441b55dcfd554\", \"abstract\": \"Pharmaceutical industry and the art and science of drug development are sorely in need of novel transformative technologies in the current age of digital health and artificial intelligence (AI). Often described as game-changing technologies, AI and machine learning algorithms have slowly but surely begun to revolutionize pharmaceutical industry and drug development over the past 5 years. In this expert review, we describe the most frequently used machine learning algorithms in drug development pipelines and the -omics databases well poised to support machine learning and drug discovery. Subsequently, we analyze the emerging new computational approaches to drug discovery and the in silico pipelines for drug repositioning and the synergies among -omics system sciences, AI and machine learning. As with system sciences, AI and machine learning embody a system scale and Big Data driven vision for drug discovery and development. We conclude with a future outlook on the ways in which machine learning approaches can be implemented to buttress and expedite drug discovery and precision medicine. As AI and machine learning are rapidly entering pharmaceutical industry and the art and science of drug development, we need to critically examine the attendant prospects and challenges to benefit patients and public health.\"}, {\"paperId\": \"4a18be86816222465d51ed3e903369df754f4005\", \"abstract\": \"Objectives The incidence of type 2 diabetes mellitus has increased significantly in recent years. With the development of artificial intelligence applications in healthcare, they are used for diagnosis, therapeutic decision making, and outcome prediction, especially in type 2 diabetes mellitus. This study aimed to identify the artificial intelligence (AI) applications for type 2 diabetes mellitus care. Methods This is a review conducted in 2018. We searched the PubMed, Web of Science, and Embase scientific databases, based on a combination of related mesh terms. The article selection process was based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). Finally, 31 articles were selected after inclusion and exclusion criteria were applied. Data gathering was done by using a data extraction form. Data were summarized and reported based on the study objectives. Results The main applications of AI for type 2 diabetes mellitus care were screening and diagnosis in different stages. Among all of the reviewed AI methods, machine learning methods with 71% (n = 22) were the most commonly applied techniques. Many applications were in multi method forms (23%). Among the machine learning algorithms applications, support vector machine (21%) and naive Bayesian (19%) were the most commonly used methods. The most important variables that were used in the selected studies were body mass index, fasting blood sugar, blood pressure, HbA1c, triglycerides, low-density lipoprotein, high-density lipoprotein, and demographic variables. Conclusions It is recommended to select optimal algorithms by testing various techniques. Support vector machine and naive Bayesian might achieve better performance than other applications due to the type of variables and targets in diabetes-related outcomes classification.\"}, {\"paperId\": \"60e801e3dfc9812e294ed9de6d579e0293d61643\", \"abstract\": null}, {\"paperId\": \"c3b35e0f95222ecd7dd258cd6068bf79bc935b15\", \"abstract\": \"Artificial intelligence and machine learning have attracted the attention of many commercial and non-profit organizations aiming to leverage advanced analytics, in order to provide a better service to their customers, increase their revenues through creating new or improving their existing internal processes, and better exploit their data by discovering complex hidden patterns. Such advanced solutions require data scientists with rare (and generally expensive) skill sets. Moreover, such solutions are often perceived as complex black boxes to decision-makers. Automated machine learning tools aim to reduce the expertise gap between the technical teams and stakeholders involved in business data science projects, by reducing the amount of time and specialized skills required to generate predictive models. We systematically benchmarked five automated machine learning tools against seven supervised learning problems of a business nature. Our results suggest that such tools, in fully automated mode, must be used cautiously, only where predictive models support low-impact decisions and do not need to be explainable, and only by data scientists capable to ensure that all phases of the data mining process have been performed adequately.\"}, {\"paperId\": \"d79f2d80fe60ff773b926b02dbb8940482728772\", \"abstract\": \"The five articles in this special section focus on data science and artificial intelligence (AI) for communications. The innovation in AI, machine learning (ML), and network data analytics provides a huge opportunity to revolutionize the world\\u2019s communications systems and user experience. Through gathering, processing, learning, and controlling the vast amounts of information in an intelligent manner, these analytics tools enable the possibility to automate and optimize systems in a way that was not previously possible. This is particularly important as the communications infrastructures evolve to support increasingly more complex human type as well as machine type communications and enable, through the connectedness of meters, sensors, and things, a plethora of new services. Supporting such immensely diverse traffic patterns and applications will require dynamic, highly adaptive network environments, all the while ensuring highly reliable, secure, and ultra-low-latency service performance guarantees.\"}, {\"paperId\": \"d422df8bff4e677a3077635db116679d25142bfc\", \"abstract\": \"Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today\\u2019s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.\"}, {\"paperId\": \"3bf4c8361af6174d1916224f7c4b5f5598026455\", \"abstract\": null}, {\"paperId\": \"6975dd8fab72879873e2a7a948a07f5cdbe7ac1e\", \"abstract\": \"Machine learning solutions, in particular those based on deep learning methods, form an underpinning of the current revolution in \\\"artificial intelligence\\\" that has dominated popular press headlines and is having a significant influence on the wider tech agenda. Here we give an overview of the 3Ds of ML systems design: Data, Design and Deployment. By considering the 3Ds we can move towards \\\\emph{data first} design.\"}, {\"paperId\": \"0b5895b0b89c852c8d8b69e670520e40765bbf24\", \"abstract\": \"Over the last decade, the availability of computational power and advances in machine learning algorithms have facilitated the development of artificial intelligence (AI) systems with high performance at tasks for which large well-annotated datasets are available. Recent progress was shown in dermato-oncology with computer vision models matching or exceeding dermatologists\\u2019 performance. The introduction of such innovation in routine healthcare requires collaboration between clinicians and data scientists to close the gap in skill sets and perspective. We briefly discuss some points of attention, including a shared understanding of the limitations of the tools, the challenge of collecting large unbiased datasets, and the need for a guided introduction in clinical practice. The incidence of cutaneous melanoma is rising worldwide. Localized melanoma is effectively cured by surgical excision, reflected in a 5-year survival rate of up to 99%. In contrast, the prognosis of metastatic melanoma patients remains poor with a 5-year survival rate of only 15\\u201320%, indicating the importance of early diagnosis. The melanoma-related cost per patient-year is 10 times higher in stage IV than in stage I disease. Early detection of melanoma can contribute to a lower financial burden on the healthcare system. A unique feature of cutaneous melanoma is the direct visibility of the tumor, potentially detectable through a simple skin examination. The diagnosis of melanoma starts with a clinical screening, followed by dermoscopy, a biopsy, and histopathological confirmation. The efficiency of this diagnostic process can possibly be improved by automated image analysis of the clinical and dermoscopic images. Current computer vision systems are predominantly based on deep learning models. Deep learning is a form of machine learning that uses multiple processing layers to automatically identify increasingly abstract patterns present in data. These models have the advantage of being fairly domain averse as model architectures can be reused for a wide variety of problem scenarios. The basic premise of these setups is that given enough labeled data and computational resources, a deep learning model is able to determine which visual properties characterize certain pathologies. Two key publications illustrate the progress made in this field. In 2017, Esteva et al. trained a convolutional neural network (CNN), a type of deep learning model, on more than 100,000 images of 2032 different diseases. The images were decomposed by the CNN starting from pixel level, and the algorithm subsequently created its own diagnostic clues. The CNN was thus not limited to man-made segmentation criteria (e.g. multiple colors, irregular vascular structures, and pseudopods) to diagnose a melanoma. The model was compared to the performance of 21 dermatologists using a combination of biopsy proven clinical and dermoscopic images. The receiver operating characteristic (ROC) area was used to illustrate the diagnostic ability of the CNN, integrating sensitivity and specificity, with a maximum performance of 1. The CNN had a ROC area of over 0.91, which was superior to the average dermatologist\\u2019s ROC area. Note that no clinical information was provided as input to the dermatologists. Haenssle et al. compared the diagnostic performance of 58 dermatologists with a similar CNN model as reported by Esteva et al., only leveraging image data. In a comparative cross sectional reader study, a 100-image biopsy proven test-set was used. Two levels of input data were provided to the dermatologists: dermoscopic images only or dermoscopy combined with patient age, gender, and body site. The resulting CNN ROC area was 0.86. This compared favorably with the average ( SD) ROC area of 0.79 ( 0.06) for the dermatologists based on imaging alone and of 0.82 ( 0.06) when clinical information was added. Although the above-mentioned methods obtain performance with a ROC curve superior to dermatologists, it is important to stress two properties of deep learning models before interpreting these results. First, deep learning models are data-driven. For the success of CNN models, the large datasets used to train the algorithm should be representative for the target use (e.g. cases referred to a dermatologist vs. evaluation by nonspecialists) and cover the complete spectrum of skin disorders, including images from different skin types. The training set should include images with a proven histopathological\"}, {\"paperId\": \"9b3ebb7bdfeb37cc5e18579fd680ef518afc728b\", \"abstract\": \"The advancement of science and technology has facilitated adaptation of human intelligence into its computerized platform for logical analysis of any event. This porting of human intelligence to machine is known as Artificial Intelligence (AI). AI enhances human life since inception with the help of these intelligent machines, human potentials will be augmented in multiple spheres. An enormous improvement in this area of AI has been noticed in the past two decades that has given rise to expert systems. AI has huge impact on different fields of business, engineering, law, medicine, science, weather forecasting, etc. to enhance the quality and efficiency in our day to day life to solve complex problems. For the past few decades, AI has been playing an emerging role in the legal field and will definitely have an effect on the legal practices over the next few years. AI has the potential to analyses legal information based on semantics and make legal predictions from the legal data set, and hence it helps the judiciary system in automation thereby increasing the efficiency within affordable budget. For better understanding of the concept, in this paper authors have performed relevant survey on this field.\"}, {\"paperId\": \"2b155ab61257dc7ae020cd384fb8ca8694bba858\", \"abstract\": \"Computational methods in fluid research have been progressing during the past few years, driven by the incorporation of massive amounts of data, either in textual or graphical form, generated from multi-scale simulations, laboratory experiments, and real data from the field. Artificial Intelligence (AI) and its adjacent field, Machine Learning (ML), are about to reach standardization in most fields of computational science and engineering, as they provide multiple ways for extracting information from data that turn into knowledge, with the aid of portable software implementations that are easy to adopt. There is ample information on the historical and mathematical background of all aspects of AI/ML in the literature. Thus, this review article focuses mainly on their impact on fluid research at present, highlighting advances and opportunities, recognizing techniques and methods having been proposed, tabulating, and testing some of the most popular algorithms that have shown significant accuracy and performance on fluid applications. We also investigate algorithmic accuracy on several fluid datasets that correspond to simulation results for the transport properties of fluids and suggest that non-linear, decision tree-based methods have shown remarkable performance on reproducing fluid properties.\"}, {\"paperId\": \"2ca14fe14f0bd2f1363f3b735e788d12c3f9f332\", \"abstract\": \"Today, the science of artificial intelligence has become one of the most important sciences in creating intelligent computer programs that simulate the human mind. The goal of artificial intelligence in the medical field is to assist doctors and health care workers in diagnosing diseases and clinical treatment, reducing the rate of medical error, and saving lives of citizens. The main and widely used technologies are expert systems, machine learning and big data. In the article, a brief overview of the three mentioned techniques will be provided to make it easier for readers to understand these techniques and their importance.\"}, {\"paperId\": \"d14ac2acf1b18e815385c631216eb4ee3a4fc842\", \"abstract\": \"vol. 7, No. 3, MaRch 2018 223 First proposed by Professor John Mccarthy at Dartmouth college in the summer of 1956,1 artificial Intelligence (aI) \\u2013 human intelligence exhibited by machines \\u2013 has occupied the lexicon of successive generations of computer scientists, science fiction fans, and medical researchers. The aim of countless careers has been to build intelligent machines that can interpret the world as humans do, understand language, and learn from realworld examples. In the early part of this century, two events coincided that transformed the field of aI. The advent of widely available Graphic Processing Units (GPUs) meant that parallel processing was faster, cheaper, and more powerful. at the same time, the era of \\u2018Big Data\\u2019 \\u2013 images, text, bioinformatics, medical records, and financial transactions, among others \\u2013 was moving firmly into the mainstream, along with almost limitless data storage. These factors led to a dramatic resurgence in interest in aI in both academic circles and industries outside traditional computer science. once again, aI occupies the zeitgeist, and is poised to transform medicine at a basic science, clinical, healthcare management, and financial level. Terminology surrounding these technologies continues to evolve and can be a source of confusion for non-computer scientists. aI is broadly classified as: general aI, machines that replicate human thought, emotion, and reason (and remain, for now, in the realm of science fiction); and narrow aI, technologies that can perform specific tasks as well as, or better than, humans. Machine learning (Ml) is the study of computer algorithms that can learn complex relationships or patterns from empirical data and make accurate decisions.2 Rather than coding specific sets of instructions to accomplish a task, the machine is \\u2018trained\\u2019 using large amounts of data and algorithms that confer it the ability to learn how to perform the task. Unlike normal algorithms, it is the data that \\u2018tells\\u2019 the machine what the \\u2018good answer\\u2019 is, and learning occurs without explicit programming. Ml problems can be classified as supervised learning or unsupervised learning.3 In a supervised machine learning algorithm, such as face recognition, the machine is shown several examples of \\u2018face\\u2019 or \\u2018non-face\\u2019 and the algorithm learns to predict whether an unseen image is a face or not. In unsupervised learning, the images shown to the machine are not labelled as \\u2018face\\u2019 or \\u2018non-face\\u2019. artificial Neural Networks (aNN)4 are one group of algorithms used for machine learning. While aNNs have existed for over 60 years, they fell out of favour during the 1990s and 2000s. In the last half-decade, aNNs have had a resurgence under a new name: deep artificial networks (or \\u2018Deep learning\\u2019). aNNs are uniquely poised to take full advantage of the computational boost offered by GPUs, allowing them to crunch through data sets of enormous sizes. These range from computer vision tasks, such as image classification, object detection, face recognition, and optical character recognition (ocR), to natural language processing and even gameplaying problems (from mastering simple atari games to the recent alphaGo victory against human grandmasters).5 aNNs work by constructing layers upon layers of simple processing units (often referred to as \\u2018neurons\\u2019), interconnected via many differentially weighted connections. aNNs are \\u2018trained\\u2019 by using backpropagation algorithms, essentially telling the machine how to alter the internal parameters that are used to compute the representation in each layer from the representation in the previous Artificial intelligence, machine learning and the evolution of healthcare\"}, {\"paperId\": \"4b34c0a57552b531d9d56f571194df2559b76e24\", \"abstract\": \"Artificial intelligence (AI) and machine learning (ML) are trending topics. AI is a broad term that represents the general concept of machines being able to carry out decisionmaking and perform human-like complex tasks, such as problem solving, understanding languages, recognizing voices and images and other \\u201csmart\\u201d tasks. ML is a field of computer science that uses statistical techniques to give computer systems the ability to \\u201clearn\\u201d with data, feeding the algorithm with a massive amount of data so that it can adjust and improve itself. ML is a specific subset of algorithms for AI. Both provide us with unbelievable advanced technology and tools to improve Health Care. The 2018 Chief Medical Officer\\u2019s report fully embraces AI and ML (Davies, 2018). It reminds us that big data and computing power to predict, and AI to diagnose and improve diagnostics, are already here. Examples of big data unique to the United Kingdom are efforts such as the 100 000 genomes and the 500 000 participants in the UK Biobank. The report advocates the embedding of these innovations to accelerate and implement of what works across England (Davies, 2018). Three areas have potential for the implementation of ML and AI within haematology. The first field could be in the form of decision support for incoming referrals to the haematologist. Intelligently designed software could guide referring clinicians, including General Practitioners and junior doctors, through an algorithm based on their initial query. Subsequently, based on their answers, incorporating patient specific data in the hospital system, it would suggest the next steps for investigation. High volume, low complex queries around, e.g. paraprotein, thrombocytopenia or neutropenia, could be dealt with through this route. Only when there are pre-defined red flag signals, such as hypercalcaemia in cases with a paraprotein, red cell fragments in a case with low platelet count, blasts, or very urgent and/or complex queries, the algorithm will notify the on-call haematology team. It might, based on the complexity, also suggest that the referrer contacts the local haematology team or an (inter)national expert. This system will not be perfect the first time around, but, like a good assistant, will need to be trained. Communication could take place through innovative secure messaging applications, such as Forward (www. forwardhealth.co), enabling the exchange of patient-identifiable information, rather than old fashioned pagers or (unsafe) WhatsApp to then further distinguish urgent from nonurgent queries. Secondly, ML and AI could be used in automated blood film reporting. The initial step is to digitize all blood films, which is already technically possible. The major National Institute for Health Research bioresource hub in Milton Keynes does this on all samples processed. This is generating a vast library of normal and abnormal blood films needed for automation. In a next step, an intelligent system will also cross check clinical records, biochemistry results and pharmacy notes. In addition, it could then use pattern recognition to decide if human interference is required because it doesn\\u2019t fit any category. These films, and any set as urgent flags, would be distributed with a provisional report to yourself via email or app on your phone. All other automated reported film reports would be sent in the same way as electrocardiogram reports are sent now, but wait in a non-urgent queue on your devices. Thirdly, modelling based on large data sets could aid in prediction and risk stratification. The systems will integrate the many data points routinely collected. It could, for instance, assist in improved prediction regarding how laboratory parameters will develop, e.g. paraprotein levels and white blood counts in monoclonal gammopathy of undetermined significance (MGUS) or chronic myeloid leukaemia (CML). This could aid in deciding which MGUS or CML patient needs to be seen every 3 months versus every 6 months, or even every 12 months, beyond the classical markers, making more efficient use of outpatient clinic time. But also, based on data integration, it will be more accurate to predict when a patient is going to encounter complications of therapy or recover their cell counts. This will be a very useful tool to anticipate these complications and thus improve patient outcome. Despite the large potential of these techniques in haematology as sketched above, only a few papers have been published to date. A systematic literature search using the terms \\u201cArtificial intelligence\\u201d, \\u201cHaematology\\u201d, and \\u201cHematology\\u201d revealed that in the last 5 years only 12 papers with original work have been published (L. Barry, Royal Army Medical Corps, British Army, Centre of Defence Pathology, Birmingham, UK, personal communication). Correspondence: Dr Suthesh Sivapalaratnam, Department of Haematology, University of Cambridge, Long Road, Cambridge CB2 0PT, UK. E-mail: ss2314@medschl.cam.ac.uk editorial comment\"}, {\"paperId\": \"f3542470da73d444f3f63030517ec918553caadc\", \"abstract\": null}, {\"paperId\": \"fb516f88c21242b27ce01344fe5a9aa9a3eb5a75\", \"abstract\": null}, {\"paperId\": \"5f5dfef286fb33afdeff6e09ecaa998e57b78f46\", \"abstract\": \"Rapid advancements of artificial intelligence of things (AIoT) technology pave the way for developing a digital\\u2010twin\\u2010based remote interactive system for advanced robotic\\u2010enabled industrial automation and virtual shopping. The embedded multifunctional perception system is urged for better interaction and user experience. To realize such a system, a smart soft robotic manipulator is presented that consists of a triboelectric nanogenerator tactile (T\\u2010TENG) and length (L\\u2010TENG) sensor, as well as a poly(vinylidene fluoride) (PVDF) pyroelectric temperature sensor. With the aid of machine learning (ML) for data processing, the fusion of the T\\u2010TENG and L\\u2010TENG sensors can realize the automatic recognition of the grasped objects with the accuracy of 97.143% for 28 different shapes of objects, while the temperature distribution can also be obtained through the pyroelectric sensor. By leveraging the IoT and artificial intelligence (AI) analytics, a digital\\u2010twin\\u2010based virtual shop is successfully implemented to provide the users with real\\u2010time feedback about the details of the product. In general, by offering a more immersive experience in human\\u2013machine interactions, the proposed remote interactive system shows the great potential of being the advanced human\\u2013machine interface for the applications of the unmanned working space.\"}, {\"paperId\": \"ecbfed4af77b1c94149acb03f941174e59cd47ff\", \"abstract\": \"Citation: Arunachalam Muthuraman., et al. \\u201cIntegration of Artificial Intelligence in Pharmacological Research with Deep and Machine Learning Process\\u201d. EC Pharmacology and Toxicology 8.4 (2020): 78-83. Abstract Artificial intelligence (AI) is a science of machine capability to mimics human behavior with intelligent analysis of data. It covers two major groups i.e., applied and general. In addition, it has integrated subset functions like deep and machine learning process. This is completely functioning with specialized algorithm. Last few decades; deep learning has been reached tremendous success in the development of AI with machine learning process. It is evolved with multiple artificial neural networks and shown superior application in pharmaceutical sciences including pharmacological research. In pharmacological research deal with preclinical (laboratory animals) and clinical (in human). AI has wide variety of application like drug discovery/manufacturing process (prediction of lead compound with analysis of ligand-protein interactions), diagnosis of big data for disease identification/diagnosis, personalized treatment/behavioral modification, clinical trial research, radiotherapy, surgical robotics, smart electronic health records, and epidemic outbreak prediction. Moreover, AI also has specialized role in the pharmacological research. The current challenges of AI with machine learnings are recognized of the correct algorithms for the analysis/interpretation of image, voice, and natural language data. Recently, the AI application of wave in deep learning process of machine learning is employed in pharmaceutical research and it has ample scope of application in pharmaceutical sciences beyond bioactivity of lead molecules. Furthermore, AI plays a vital role in diagnosis of biomarkers and disease. Hence the integrated function of AI with deep and machine learning process can be a future tool for the better enhancement of pharmacological research.\"}, {\"paperId\": \"4979f5d75808a40234c4815dbcacc5b657847da2\", \"abstract\": null}, {\"paperId\": \"dc2fce15d1e22d92c7096078a5f1d9ee6487b99c\", \"abstract\": \"Machine learning, as a study of algorithms that automate prediction and decision\\u2010making based on complex data, has become one of the most effective tools in the study of artificial intelligence. In recent years, scientific communities have been gradually merging data\\u2010driven approaches with research, enabling dramatic progress in revealing underlying mechanisms, predicting essential properties, and discovering unconventional phenomena. It is becoming an indispensable tool in the fields of, for instance, quantum physics, organic chemistry, and medical imaging. Very recently, machine learning has been adopted in the research of photonics and optics as an alternative approach to address the inverse design problem. In this report, the fast advances of machine\\u2010learning\\u2010enabled photonic design strategies in the past few years are summarized. In particular, deep learning methods, a subset of machine learning algorithms, dealing with intractable high degrees\\u2010of\\u2010freedom structure design are focused upon.\"}, {\"paperId\": \"cd6146a5d2a099dfab0de7783439ef8164d3cf18\", \"abstract\": \"Abstract Faced with increased competitive pressures from online businesses and rapidly changing consumer behaviors, traditional businesses with an online channel as well as a brick and mortar presence increasingly turn to data science and artificial intelligence to make better decisions and enhance their operations. The changing market dynamics are heavily influenced by the overwhelming adoption of mobile phones and data connections. As a result, consumers are better informed, make more real time purchase decisions, and share their experience through review sites and social media. Businesses leverage this large amount of user generated content by extracting the most useful pieces of information from it with text analytics and machine learning algorithms that identify patterns and consumer sentiment at scale. They use the critical pieces of customer feedback to market their products and services better, attract more customers, and offer them a better experience. Four uncertainties and challenges that these business encounter are the following: (1) how to improve online reputation; (2) discoverability and engagement levels of online traffic; (3) how to measure and improve the customer experience; (4) monitoring and benchmarking against competitors. The direct benefits of using Data Science and Artificial Intelligence to address those challenges are: use of standard metrics to measure strengths and weaknesses in online reputation; understanding patterns of customer behavior; listening to customer feedback at scale and extracting actionable insights directly applicable in improving operations; taking corrective actions to avoid losing customers to competitors. This paper will also analyze the competitive landscape in the field of Data Analytics and Insights solutions based on consumer generated feedback. Additionally, it will illustrate a series of case studies across multiple industries, with the purpose of exposing best practices that can be leveraged by businesses in their decision-making process.\"}, {\"paperId\": \"bb569334c2ecd6710ad441bb56ab9746f1550592\", \"abstract\": \"Artificial intelligence (AI) has been described as one of the extremely effective and promising scientific tools available to mankind. AI and its associated innovations are becoming more popular in industry and culture, and they are starting to show up in healthcare. Numerous facets of healthcare, as well as regulatory procedures within providers, payers, and pharmaceutical companies, may be transformed by these innovations. As a result, the purpose of this review is to identify the potential machine learning applications in the field of infectious diseases and the general healthcare system. The literature on this topic was extracted from various databases, such as Google, Google Scholar, Pubmed, Scopus, and Web of Science. The articles having important information were selected for this review. The most challenging task for AI in such healthcare sectors is to sustain its adoption in daily clinical practice, regardless of whether the programs are scalable enough to be useful. Based on the summarized data, it has been concluded that AI can assist healthcare staff in expanding their knowledge, allowing them to spend more time providing direct patient care and reducing weariness. Overall, we might conclude that the future of \\u201cconventional medicine\\u201d is closer than we realize, with patients seeing a computer first and subsequently a doctor.\"}, {\"paperId\": \"c33d23b7516549fff36a5abe921e5cafbe410fba\", \"abstract\": \"This paper presents application of the binomial and factorial identities and expansions that are used in artificial intelligence, machine learning, and cybersecurity. The factorial and binomial identities can be used as methodological advances for various algorithms and applications in information and computational science. Cybersecurity is the practice of protecting the computing systems, communication networks, data and programs from cyber-attacks. Its objective is to reduce the risk of cyber-attacks and protect against the unauthorized exploitation of systems and networks. For this purposes, we need a strong cryptographic algorithms like RSA algorithm and Elliptic Curve Cryptography. In this connection, computing and combinatorial techniques based on factorials and binomial distributions are developed for the researchers who are working in artificial intelligence and cybersecurity.\"}, {\"paperId\": \"84440f8302ac15b0bc0da99408f8fcd37557d286\", \"abstract\": \"Advances in artificial intelligence (AI) and the extension of citizen science to various scientific areas, as well as the generation of big citizen science data, are resulting in AI and citizen science being good partners, and their combination benefits both fields. The integration of AI and citizen science has mostly been used in biodiversity projects, with the primary focus on using citizen science data to train machine learning (ML) algorithms for automatic species identification. In this article, we will look at how ML techniques can be used in citizen science and how they can influence volunteer engagement, data collection, and data validation. We reviewed several use cases from various domains and categorized them according to the ML technique used and the impact of ML on citizen science in each project. Furthermore, the benefits and risks of integrating ML in citizen science are explored, and some recommendations are provided on how to enhance the benefits while mitigating the risks of this integration. Finally, because this integration is still in its early phases, we have proposed some potential ideas and challenges that can be implemented in the future to leverage the power of the combination of citizen science and AI, with the key emphasis being on citizen science in this article.\"}, {\"paperId\": \"9fc2abfa181759fc2185968dbac250ced358c5fd\", \"abstract\": \"Plant stress is one of the most significant factors affecting plant fitness and, consequently, food production. However, plant stress may also be profitable since it behaves hormetically; at low doses, it stimulates positive traits in crops, such as the synthesis of specialized metabolites and additional stress tolerance. The controlled exposure of crops to low doses of stressors is therefore called hormesis management, and it is a promising method to increase crop productivity and quality. Nevertheless, hormesis management has severe limitations derived from the complexity of plant physiological responses to stress. Many technological advances assist plant stress science in overcoming such limitations, which results in extensive datasets originating from the multiple layers of the plant defensive response. For that reason, artificial intelligence tools, particularly Machine Learning (ML) and Deep Learning (DL), have become crucial for processing and interpreting data to accurately model plant stress responses such as genomic variation, gene and protein expression, and metabolite biosynthesis. In this review, we discuss the most recent ML and DL applications in plant stress science, focusing on their potential for improving the development of hormesis management protocols.\"}, {\"paperId\": \"ac0e320544bee00f6b21a2d213a96fe04f45603a\", \"abstract\": \"Pattern recognition using machine learning methods is an area that exploded in recent years, given the increasing amount of available data. JAMIA has published an increased number of articles in this area in the past few years. In this issue, Zhang (p. 1351) reports on a data resource for sleep research, Feller (p. 1366) proposes a visual analytics approach for pattern recognition in personal health records, while Xiao (p. 1419) reports on a systematic review of deep learning models applied to electronic health record (EHR) data. Machine learning models to detect pulmonary nodules in CT scans are described by Grutzemacher (p. 1301), while models to predict adverse drug events are reported by Davazdahema (p. 1311) and Mower (p. 1339). Albers (p. 1392) provides a broad perspective on mechanistic machine learning using physiologic knowledge, while Pencina (p. 1273) simulates models to predict incremental value of biomarkers. Developing new approaches to facilitate automation of clinical research is another area in which informatics has evolved considerably in the past few years. In particular, biomedical natural language processing and other methods to structure narrative text and voice recordings have motivated informatics research. Sarker (p. 1274) reports on systems for medication-related text classification, Zamjahn (p. 1284) proposes a method to streamline the evaluation of video recordings, and Parr (p. 1292) describes the automated mapping of laboratory tests to standardized codes in EHRs. Algorithms and tools for processing and linking EHRs are reported by Hoopes (p. 1322) and Klann (p. 1331). Sinnott (p. 1359) proposes a method to improve the power of genetic association studies, while Aronson (p. 1375) describes how the eMERGE consortium established data flows that are particularly relevant to genomic medicine. Some new modalities of health data are not yet integrated into EHRs though are increasingly being used in research. Streaming data from sensors, for example, in the form of continuous heart rate, activity and location tracking is becoming more common. Donevant (p. 1407) reviews the literature on mHealth studies, Speier (p. 1351) evaluates utility of activity-related data, while Goldenholtz (p. 1402) proposes a way to utilize location data without compromising privacy. Regardless of their area of sub-specialization, informaticians worldwide now have a wealth of opportunities ahead that were not available to their predecessors. The rapid accumulation of data and knowledge due to new techniques and approaches brings opportunities as well as challenges related to the protection of privacy, inadequacy of computational resources to store, process and integrate large amounts of multi-modality data, antiquated regulatory frameworks, and a relatively low number of trained professionals. However, recognition of the value informatics brings to clinical practice and research has been facilitated by general understanding of the value of data science and artificial intelligence that is now pervasive in our daily lives. These are exciting times and we have a unique opportunity to make a difference towards better health for all.\"}, {\"paperId\": \"d612ae048e30cab9c14c79b993621332685475ac\", \"abstract\": \"Artificial intelligence (AI) programs are applied to methods such as diagnostic procedures, treatment protocol development, patient monitoring, drug development, personalized medicine in healthcare, and outbreak predictions in global health, as in the case of the current COVID-19 pandemic. Machine learning (ML) is a field of AI that allows computers to learn and improve without being explicitly programmed. ML algorithms can also analyze large amounts of data called Big data through electronic health records for disease prevention and diagnosis. Wearable medical devices are used to continuously monitor an individual\\u2019s health status and store it in cloud computing. In the context of a newly published study, the potential benefits of sophisticated data analytics and machine learning are discussed in this review. We have conducted a literature search in all the popular databases such as Web of Science, Scopus, MEDLINE/PubMed and Google Scholar search engines. This paper describes the utilization of concepts underlying ML, big data, blockchain technology and their importance in medicine, healthcare, public health surveillance, case estimations in COVID-19 pandemic and other epidemics. The review also goes through the possible consequences and difficulties for medical practitioners and health technologists in designing futuristic models to improve the quality and well-being of human lives.\"}, {\"paperId\": \"e1d85a50a78aa2890d2844f8e4a648c038f18b80\", \"abstract\": \"There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. Artificial Intelligence has made it possible. Deep learning is a subset of machine learning, and machine learning is a subset of AI, which is an umbrella term for any computer program that does something smart. In other words, all machine learning is AI, but not all AI is machine learning, and so forth. Machine Learning represents a key evolution in the fields of computer science, data analysis, software engineering, and artificial intelligence.\\u00a0Machine learning (ML)is a vibrant field of research,\\u00a0with a range of exciting areas for further development across different methods\\u00a0and applications. These areas include algorithmic interpretability,\\u00a0robustness, privacy, fairness, inference of causality, human-machine interaction, and security. The goal of ML is never to make \\u201cperfect\\u201d guesses, because ML deals in domains where there is no such thing. The goal is to make guesses that are good enough to be useful. Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones. This paper gives an overview of artificial intelligence, machine learning & deep learning techniques and compare these techniques.\"}, {\"paperId\": \"d9a34ce248ded179ac52592830449cc58881a81b\", \"abstract\": \"This research aimed at utilizing artificial intelligence in STEM-based creative learning in the society 5.0 era. The researchers investigated how an educator can utilize artificial intelligence and optimize it into a STEM-based learning process. STEM stands for Science, Technology, Engineering, and Math. The United States initiated it to combine the four disciplines integrated into a problem-based learning method and everyday contextual events. Artificial intelligence is an intelligence added to a system managed in a scientific context. Artificial intelligence is created and put into a machine (computer) to do work like humans. Several fields that use artificial intelligence include expert systems, computer games (games), fuzzy logic, artificial neural networks, and robotics. The researchers employed the literature review or library research by reviewing the results of various studies and collecting data from assorted references and sources. In conclusion, implementing artificial intelligence in STEM-based creative learning can be an alternative for an educator in the learning process. Artificial intelligence (AI) is expected to help educators in the creative learning process by implementing long-life education and showing behavioral changes in a better direction cognitively, affectively, and psychometrically, especially in the era of society 5.0.\"}, {\"paperId\": \"5f39b2eb7816c02a544c392666e7f5aabd5df683\", \"abstract\": null}, {\"paperId\": \"69ece526e5d9dced2cdbc31d3e23212ed217a7b5\", \"abstract\": \"Digital pathology is a technology that allows pathological information created from a digital slide to be accessed, handled, and interpreted. Using optical pathology scanners, glass slides are collected and transformed to digitized glass slides that can be viewed on your computer monitor. Relevant support for education and the practice of human anatomy is offered by digital pathology. With the recent developments in digital pathology led to computer-aided diagnosis using machine learning approaches. So, machine learning frameworks assist physicians in diagnosing critical cases such as cancer, tumors, etc and improve patient management. With an ever growing number of choices, it can be hard to pick a better machine learning method for pathological data. Big potential attempts are made in this paper to research the full context of digital pathology with the specifics of how artificial intelligence has contributed to digital pathology. This review also analyzes various machine learning frameworks by providing as much information as possible and quantifying what the tradeoffs will be. This paper ultimately provides the improvements in the frameworks available that will be required in the near future applications.\"}, {\"paperId\": \"e67197b8539ee72f51577cab17ba879e4eafc6bc\", \"abstract\": null}, {\"paperId\": \"ea5b545d19adc9d39d75ca1ceb8780830d02e86f\", \"abstract\": \"Data science is an interdisciplinary field that applies numerous techniques, such as machine learning (ML), neural networks (NN) and artificial intelligence (AI), to create value, based on extracting knowledge and insights from available 'big' data [...].\"}, {\"paperId\": \"05fefdf6ea882a43b1418f039f7434323ec38da5\", \"abstract\": \"Purpose of review The practice of neurology is undergoing a paradigm shift because of advances in the field of data science, artificial intelligence, and machine learning. To ensure a smooth transition, physicians must have the knowledge and competence to apply these technologies in clinical practice. In this review, we describe physician perception and preparedness, as well as current state for clinical applications of artificial intelligence and machine learning in neurology. Recent findings Digital health including artificial intelligence-based/machine learning-based technology has made significant inroads into various aspects of healthcare including neurological care. Surveys of physicians and healthcare stakeholders suggests an overall positive perception about the benefits of artificial intelligence/machine learning in clinical practice. This positive perception is tempered by concerns for lack of knowledge and limited opportunities to build competence in artificial intelligence/machine learning technology. Literature about neurologist's perception and preparedness towards artificial intelligence/machine learning-based technology is scant. There are very few opportunities for physicians particularly neurologists to learn about artificial intelligence/machine learning-based technology. Summary Neurologists have not been surveyed about their perception and preparedness to adopt artificial intelligence/machine learning-based technology in clinical practice. We propose development of a practical artificial intelligence/machine learning curriculum to enhance neurologists\\u2019 competence in these newer technologies.\"}, {\"paperId\": \"be3662cfb4dd47782a70ed80334cf21bfb9a0fc8\", \"abstract\": \"Throughout the history of humanity, the way that humans transmit intelligence from generation to generation has changed multiple times. Beginning verbally and through manuscripts, continuing with patented inventions, official and private documents, nowadays, the different ways of adapting and implementing the knowledge acquired through data are being highlighted. Whether with regards to human, artificial, or mixed intelligence, data can provide consistent and meaningful answers to address the challenges of today's businesses. This scientific paper contributes to the vision of a hybrid human and artificial intelligence approach, thus explaining, exemplifying, and presenting research on how today's organizations apply the concept of data efficiency and effectiveness from a business intelligence perspective. The fact that decision-makers can be more performant with the help of data science and machine learning has the power of unlocking strengths and opportunities at an unprecedented rate and therefor is the new norm in the modern business world.\"}, {\"paperId\": \"f09350989612f263621bac540f605e80e8bdd228\", \"abstract\": \"Changes in society related to the development of science, technology, computing power, cloud services, artificial intelligence, increasing general access to huge amounts of open data, lead to increased global investment in technology and services. Appropriate training is required by specialists to create a workforce to work with artificial intelligence. On the one hand, it puts forward new requirements for the training of young people, and educational content, on the other hand, provides opportunities for the use of cloud technologies during the educational process. Widespread use of AI in various fields and everyday life poses the task of understanding the basic terms related to Artificial intelligence (AI), such as Machine learning (ML), Neural network (NN), Artificial neural networks (ANN), Deep Learning, Data Science, Big Data, mastering the basic skills of using and understanding the AI principles, which is possible during the study in the school course of computer science. Cloud technologies allow you to use the power of a remote server (open information systems, digital resources, software, etc.) regardless of the location of the consumer and provide ample opportunities for the study of artificial intelligence. In this article we reveal the possibilities of cloud technologies as a means of studying artificial intelligence at school, consider the need for three stages of training and provide development of tasks and own experience of using cloud technologies to study artificial intelligence on the example of DALL-E, Google QuickDraw, cloud technologies Makeblock, PictoBlox, Teachable Machine at different stages of AI study.\"}, {\"paperId\": \"8c8705aad931f5bd3843814056a06ab02a65c7aa\", \"abstract\": \"Artificial intelligence (AI), also known as machine intelligence, is a branch of science that empowers machines using human intelligence. AI refers to the technology of rendering human intelligence through computer programs. From healthcare to the precise prevention, diagnosis, and management of diseases, AI is progressing rapidly in various interdisciplinary fields, including ophthalmology. Ophthalmology is at the forefront of AI in medicine because the diagnosis of ocular diseases heavy reliance on imaging. Recently, deep learning-based AI screening and prediction models have been applied to the most common visual impairment and blindness diseases, including glaucoma, cataract, age-related macular degeneration (ARMD), and diabetic retinopathy (DR). The success of AI in medicine is primarily attributed to the development of deep learning algorithms, which are computational models composed of multiple layers of simulated neurons. These models can learn the representations of data at multiple levels of abstraction. The Inception-v3 algorithm and transfer learning concept have been applied in DR and ARMD to reuse fundus image features learned from natural images (non-medical images) to train an AI system with a fraction of the commonly used training data (<1%). The trained AI system achieved performance comparable to that of human experts in classifying ARMD and diabetic macular edema on optical coherence tomography images. In this study, we highlight the fundamental concepts of AI and its application in these four major ocular diseases and further discuss the current challenges, as well as the prospects in ophthalmology.\"}, {\"paperId\": \"e4cd6766cd724fa89a8ceee19aca6dfc1510c673\", \"abstract\": null}, {\"paperId\": \"387825b6d25a6f8224254b88f48dd546f5cd1d5f\", \"abstract\": null}, {\"paperId\": \"870da55ec826bcfeae42ca0f72e06be3660ebcea\", \"abstract\": \"Globally, there is a substantial unmet need to diagnose various diseases effectively. The complexity of the different disease mechanisms and underlying symptoms of the patient population presents massive challenges in developing the early diagnosis tool and effective treatment. Machine learning (ML), an area of artificial intelligence (AI), enables researchers, physicians, and patients to solve some of these issues. Based on relevant research, this review explains how machine learning (ML) is being used to help in the early identification of numerous diseases. Initially, a bibliometric analysis of the publication is carried out using data from the Scopus and Web of Science (WOS) databases. The bibliometric study of 1216 publications was undertaken to determine the most prolific authors, nations, organizations, and most cited articles. The review then summarizes the most recent trends and approaches in machine-learning-based disease diagnosis (MLBDD), considering the following factors: algorithm, disease types, data type, application, and evaluation metrics. Finally, in this paper, we highlight key results and provides insight into future trends and opportunities in the MLBDD area.\"}, {\"paperId\": \"d4cbcc27b9491b17beac683228db2e6be0a7f1d1\", \"abstract\": \"In order to further the application,research and development of artificial intelligence (AI), The Office of Science and Technology Policy of the White House released two reports commissioned \\u201cPreparation for the Future of Artificial Intelligence\\u201c and \\u201cThe National Artificial Intelligence Research and Development Strategic Plan\\u201d in October 2016. According to the reports, AI has been playing a growing role in every area of society, among which education is an important one. Educational artistic intelligence (EAI) is a new field which combinations AI with learning science. Current, the key technologies of EAI are knowledge representation, machine learning, deep learning, natural language processing, intelligent agent, affective computing, etc. The development of EAI in education focuses on the intelligent tutor and assistant, intelligent evaluation, learning partner, data mining, learning analysis, etc.On this account, there is a urgent need to strengthen the training of the workforce of EAI at all levels, in accordance with the rapid development of AI.\"}, {\"paperId\": \"08cf21f0076ccb263edded6b1ace7fe4a9ee626c\", \"abstract\": \"Everybody wants to analyse their data, but only few posses the data science expertise to do this. Motivated by this observation, we introduce a novel framework and system VisualSynth for human-machine collaboration in data science. Its aim is to democratize data science by allowing users to interact with standard spreadsheet software in order to perform and automate various data analysis tasks ranging from data wrangling, data selection, clustering, constraint learning, predictive modeling and auto-completion. VisualSynth relies on the user providing colored sketches, i.e., coloring parts of the spreadsheet, to partially specify data science tasks, which are then determined and executed using artificial intelligence techniques.\"}, {\"paperId\": \"20cc87bba097ae665a23a1afd56c8c6268bcfcef\", \"abstract\": \"Abstract Given the growing use of Artificial intelligence (AI) and machine learning (ML) methods across all aspects of environmental sciences, it is imperative that we initiate a discussion about the ethical and responsible use of AI. In fact, much can be learned from other domains where AI was introduced, often with the best of intentions, yet often led to unintended societal consequences, such as hard coding racial bias in the criminal justice system or increasing economic inequality through the financial system. A common misconception is that the environmental sciences are immune to such unintended consequences when AI is being used, as most data come from observations, and AI algorithms are based on mathematical formulas, which are often seen as objective. In this article, we argue the opposite can be the case. Using specific examples, we demonstrate many ways in which the use of AI can introduce similar consequences in the environmental sciences. This article will stimulate discussion and research efforts in this direction. As a community, we should avoid repeating any foreseeable mistakes made in other domains through the introduction of AI. In fact, with proper precautions, AI can be a great tool to help reduce climate and environmental injustice. We primarily focus on weather and climate examples but the conclusions apply broadly across the environmental sciences.\"}, {\"paperId\": \"caafeef2739c94e74ed5ca636b9ed861d8852815\", \"abstract\": \"Data science and machine learning are the key technologies when it comes to the processes and products with automatic learning and optimization to be used in the automotive industry of the future. This article defines the terms \\\"data science\\\" (also referred to as \\\"data analytics\\\") and \\\"machine learning\\\" and how they are related. In addition, it defines the term \\\"optimizing analytics\\\" and illustrates the role of automatic optimization as a key technology in combination with data analytics. It also uses examples to explain the way that these technologies are currently being used in the automotive industry on the basis of the major subprocesses in the automotive value chain (development, procurement; logistics, production, marketing, sales and after-sales, connected customer). Since the industry is just starting to explore the broad range of potential uses for these technologies, visionary application examples are used to illustrate the revolutionary possibilities that they offer. Finally, the article demonstrates how these technologies can make the automotive industry more efficient and enhance its customer focus throughout all its operations and activities, extending from the product and its development process to the customers and their connection to the product.\"}, {\"paperId\": \"e0bfdf780d14421118ae24754035b7173357712e\", \"abstract\": null}, {\"paperId\": \"b5501ca7d40be9a87cff3486ea2f14c77a9d2244\", \"abstract\": null}, {\"paperId\": \"3fd9763cd8d74939b95b541b569e3e98d597e729\", \"abstract\": \"Machine learning is a fast-growing field with various applications in artificial intelligence and data science. Recently, a new machine learning program have been integrated into the Israeli high school computer science curriculum and thus we added a new machine learning module to the Methods of Teaching Computer Science (MTCS) course, which is part of the teachers' preparation program. This machine learning module provides us a unique opportunity to teach both pedagogy and content with the same subject matter. After teaching the basics of machine learning, we asked the students to find similarities between human learning theories and machine learning algorithms. Students identified several interesting parallels: (a) Supervised learning is similar to behavioral learning as the machine learns to connect training examples (stimuli) with labels (behavior). Also, the learning is based on minimizing error (punishment) function, (b) Reinforcement learning is similar to behavioral learning as learning is based on feedback from the environment, (c) Constructivism can be identified in the iterative convergence of the algorithm; the inner model improves each iteration based on the current knowledge, and (d) Social learning is reflected in clustering as each cluster affects the learning of the other clusters. In our talk, we present the idea that computational mental models may be used to reinforce pedagogical mental models and vice versa.\"}, {\"paperId\": \"943731a0bce0888ff50a0563aebdba8549f97e9c\", \"abstract\": \"Machine learning (ML), a branch of artificial intelligence, where machines learn from big data, is at the crest of a technological wave of change sweeping society. Cardiovascular medicine is at the forefront of many ML applications, and there is a significant effort to bring them into mainstream clinical practice. In the field of cardiac electrophysiology, ML applications have also seen a rapid growth and popularity, particularly the use of ML in the automatic interpretation of ECGs, which has been extensively covered in the literature. Much lesser known are the other aspects of ML application in cardiac electrophysiology and arrhythmias, such as those in basic science research on arrhythmia mechanisms, both experimental and computational; in the development of better techniques for mapping of cardiac electrical function; and in translational research related to arrhythmia management. In the current review, we examine comprehensively such ML applications as they match the scope of this journal. The current review is organized in 3 parts. The first provides an overview of general ML principles and methodologies that will afford readers of the necessary information on the subject, serving as the foundation for inviting further ML applications in arrhythmia research. The basic information we provide can serve as a guide on how one might design and conduct an ML study. The second part is a review of arrhythmia and electrophysiology studies in which ML has been utilized, highlighting the broad potential of ML approaches. For each subject, we outline comprehensively the general topics, while reviewing some of the research advances utilizing ML under the subject. Finally, we discuss the main challenges and the perspectives for ML-driven cardiac electrophysiology and arrhythmia research.\"}, {\"paperId\": \"ec3c032af64b365dcd8e3938f74930e13f19695c\", \"abstract\": \"Osteoporosis and its clinical consequence, bone fracture, is a multifactorial disease that has been the object of extensive research. Recent advances in machine learning (ML) have enabled the field of artificial intelligence (AI) to make impressive breakthroughs in complex data environments where human capacity to identify high\\u2010dimensional relationships is limited. The field of osteoporosis is one such domain, notwithstanding technical and clinical concerns regarding the application of ML methods. This qualitative review is intended to outline some of these concerns and to inform stakeholders interested in applying AI for improved management of osteoporosis. A systemic search in PubMed and Web of Science resulted in 89 studies for inclusion in the review. These covered one or more of four main areas in osteoporosis management: bone properties assessment (n =\\u200913), osteoporosis classification (n =\\u200934), fracture detection (n =\\u200932), and risk prediction (n =\\u200914). Reporting and methodological quality was determined by means of a 12\\u2010point checklist. In general, the studies were of moderate quality with a wide range (mode score 6, range 2 to 11). Major limitations were identified in a significant number of studies. Incomplete reporting, especially over model selection, inadequate splitting of data, and the low proportion of studies with external validation were among the most frequent problems. However, the use of images for opportunistic osteoporosis diagnosis or fracture detection emerged as a promising approach and one of the main contributions that ML could bring to the osteoporosis field. Efforts to develop ML\\u2010based models for identifying novel fracture risk factors and improving fracture prediction are additional promising lines of research. Some studies also offered insights into the potential for model\\u2010based decision\\u2010making. Finally, to avoid some of the common pitfalls, the use of standardized checklists in developing and sharing the results of ML models should be encouraged. \\u00a9 2021 American Society for Bone and Mineral Research (ASBMR).\"}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 100, \"next\": 200, \"data\": [{\"paperId\": \"ec3c032af64b365dcd8e3938f74930e13f19695c\", \"abstract\": \"Osteoporosis and its clinical consequence, bone fracture, is a multifactorial disease that has been the object of extensive research. Recent advances in machine learning (ML) have enabled the field of artificial intelligence (AI) to make impressive breakthroughs in complex data environments where human capacity to identify high\\u2010dimensional relationships is limited. The field of osteoporosis is one such domain, notwithstanding technical and clinical concerns regarding the application of ML methods. This qualitative review is intended to outline some of these concerns and to inform stakeholders interested in applying AI for improved management of osteoporosis. A systemic search in PubMed and Web of Science resulted in 89 studies for inclusion in the review. These covered one or more of four main areas in osteoporosis management: bone properties assessment (n =\\u200913), osteoporosis classification (n =\\u200934), fracture detection (n =\\u200932), and risk prediction (n =\\u200914). Reporting and methodological quality was determined by means of a 12\\u2010point checklist. In general, the studies were of moderate quality with a wide range (mode score 6, range 2 to 11). Major limitations were identified in a significant number of studies. Incomplete reporting, especially over model selection, inadequate splitting of data, and the low proportion of studies with external validation were among the most frequent problems. However, the use of images for opportunistic osteoporosis diagnosis or fracture detection emerged as a promising approach and one of the main contributions that ML could bring to the osteoporosis field. Efforts to develop ML\\u2010based models for identifying novel fracture risk factors and improving fracture prediction are additional promising lines of research. Some studies also offered insights into the potential for model\\u2010based decision\\u2010making. Finally, to avoid some of the common pitfalls, the use of standardized checklists in developing and sharing the results of ML models should be encouraged. \\u00a9 2021 American Society for Bone and Mineral Research (ASBMR).\"}, {\"paperId\": \"2a0ff39fa53c8fc18f5a2ab2a82f476a209d03b0\", \"abstract\": \"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods \\u2014 including deep learning and agent-based learning \\u2014 in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.\"}, {\"paperId\": \"107e2f795aa2f9c22bd0efae38d1c8a51d4d2735\", \"abstract\": \"With each passing day and gradually as we move into future, smart or intelligent machines will slowly replace and enhance human capabilities in many areas. The intelligence exhibited by machines or softwares are often termed as \\u201cArtificial Intelligence\\u201d which is a subfleld of computer science. Artificial intelligence along with machine learning is now a potential game changer in the history of computing backed with strong data analytics. Study in this area of artificial intelligence has rapidly influenced the emergence of smart technologies that has a huge impact on our daily lives. The field of science, engineering, business and medicine has become smarter with prediction capabilities to smoothen our lives in our daily activities. The areas employing artificial intelligence has seen an increase in the quality and efficiency which has been illustrated in this paper.\"}, {\"paperId\": \"05b2f43a5380ddcc3b77457690069a7b5bbe69dc\", \"abstract\": \"In recent years, the rapid improvement in computing facilities combined with that achieved in algorithms and the immense amount of available data led to a great interest in machine learning (ML), which is a subset of artificial intelligence. Nowadays, the ML technique is used mostly in all applications for various purposes, whereby ML will be possible to learn from data, predict, identify patterns, and make decisions. In this regard, the ML was successfully used to predict the oxygen uptake during physical activity without the need for complicated procedures used in the direct measurement. Accordingly, in the present work, the state-of-art and recent advances related to the oxygen uptake prediction using ML were presented. Various exercise and non-exercise predictive models also were discussed.\"}, {\"paperId\": \"b2eb214439bdcb1e7bef12ab2ae69b1f0a3cc3a9\", \"abstract\": \"Clinical trials for Alzheimer\\u2019s disease (AD) face multiple challenges, such as the high screen failure rate and the even allocation of heterogeneous participants. Artificial intelligence (AI), which has become a potent tool of modern science with the expansion in the volume, variety, and velocity of biological data, offers promising potential to address these issues in AD clinical trials. In this review, we introduce the current status of AD clinical trials and the topic of machine learning. Then, a comprehensive review is focused on the potential applications of AI in the steps of AD clinical trials, including the prediction of protein and MRI AD biomarkers in the prescreening process during eligibility assessment and the likelihood stratification of AD subjects into rapid and slow progressors in randomization. Finally, this review provides challenges, developments, and the future outlook on the integration of AI into AD clinical trials.\"}, {\"paperId\": \"6b7a3eec40b797143e0c56f7752161c3e94d7eb9\", \"abstract\": \"Low Back Pain (LBP) is currently the first cause of disability in the world, with a significant socioeconomic burden. Diagnosis and treatment of LBP often involve a multidisciplinary, individualized approach consisting of several outcome measures and imaging data along with emerging technologies. The increased amount of data generated in this process has led to the development of methods related to artificial intelligence (AI), and to computer-aided diagnosis (CAD) in particular, which aim to assist and improve the diagnosis and treatment of LBP. In this manuscript, we have systematically reviewed the available literature on the use of CAD in the diagnosis and treatment of chronic LBP. A systematic research of PubMed, Scopus, and Web of Science electronic databases was performed. The search strategy was set as the combinations of the following keywords: \\u201cArtificial Intelligence\\u201d, \\u201cMachine Learning\\u201d, \\u201cDeep Learning\\u201d, \\u201cNeural Network\\u201d, \\u201cComputer Aided Diagnosis\\u201d, \\u201cLow Back Pain\\u201d, \\u201cLumbar\\u201d, \\u201cIntervertebral Disc Degeneration\\u201d, \\u201cSpine Surgery\\u201d, etc. The search returned a total of 1536 articles. After duplication removal and evaluation of the abstracts, 1386 were excluded, whereas 93 papers were excluded after full-text examination, taking the number of eligible articles to 57. The main applications of CAD in LBP included classification and regression. Classification is used to identify or categorize a disease, whereas regression is used to produce a numerical output as a quantitative evaluation of some measure. The best performing systems were developed to diagnose degenerative changes of the spine from imaging data, with average accuracy rates >80%. However, notable outcomes were also reported for CAD tools executing different tasks including analysis of clinical, biomechanical, electrophysiological, and functional imaging data. Further studies are needed to better define the role of CAD in LBP care.\"}, {\"paperId\": \"891dc5bcde184f7d96528417acb6fd680d8dd598\", \"abstract\": \"Drug repurposing is of interest for therapeutics innovation in many human diseases including coronavirus disease 2019 (COVID-19). Methodological innovations in drug repurposing are currently being empowered by convergence of omics systems science and digital transformation of life sciences. This expert review article offers a systematic summary of the application of artificial intelligence (AI), particularly machine learning (ML), to drug repurposing and classifies and introduces the common clustering, dimensionality reduction, and other methods. We highlight, as a present-day high-profile example, the involvement of AI/ML-based drug discovery in the COVID-19 pandemic and discuss the collection and sharing of diverse data types, and the possible futures awaiting drug repurposing in an era of AI/ML and digital technologies. The article provides new insights on convergence of multi-omics and AI-based drug repurposing. We conclude with reflections on the various pathways to expedite innovation in drug development through drug repurposing for prompt responses to the current COVID-19 pandemic and future ecological crises in the 21st century.\"}, {\"paperId\": \"d9816528767d38dbcbc4956c14e59d4114a7c0ea\", \"abstract\": \"Machine learning has been an emerging scientific field serving the modern multidisciplinary needs in the Materials Science and Manufacturing sector. The taxonomy and mapping of nanomaterial properties based on data analytics is going to ensure safe and green manufacturing with consciousness raised on effective resource management. The utilization of predictive modelling tools empowered with artificial intelligence (AI) has proposed novel paths in materials discovery and optimization, while it can further stimulate the cutting-edge and data-driven design of a tailored behavioral profile of nanomaterials to serve the special needs of application environments. The previous knowledge of the physics and mathematical representation of material behaviors, as well as the utilization of already generated testing data, received specific attention by scientists. However, the exploration of available information is not always manageable, and machine intelligence can efficiently (computational resources, time) meet this challenge via high-throughput multidimensional search exploration capabilities. Moreover, the modelling of bio-chemical interactions with the environment and living organisms has been demonstrated to connect chemical structure with acute or tolerable effects upon exposure. Thus, in this review, a summary of recent computational developments is provided with the aim to cover excelling research and present challenges towards unbiased, decentralized, and data-driven decision-making, in relation to increased impact in the field of advanced nanomaterials manufacturing and nanoinformatics, and to indicate the steps required to realize rapid, safe, and circular-by-design nanomaterials.\"}, {\"paperId\": \"b23588842cc13627c3c01a72eb9fcd86cf594bc6\", \"abstract\": \"Nowadays, sheer amount of data is being generated by plethora of sources, like science, business, medicine, sports, geography, environment, etc. This produced data may be formless, bigger sized, and in the raw format and has no importance at all, until analyzed. Conventional techniques of data analysis may be inappropriate due to vast data diverse nature, high dimensionality of data, and much of the data is never explored. So, in order to get relevant data, some techniques need to be incorporated on the existing data, which would be effective for the real-world applications. Artificial intelligence, machine learning, and deep learning are the extensively used technologies with the utmost buzz. Machine learning is a subfield of AI that designs the intelligent model based on past and current trends. The only concentration of this ground is pre-programmed learning techniques without any human interference/intervention. In addition to this, deep learning processes the data and creates pattern for decision use after imitating the working of human brain. So, the objective of this paper is to explore the research application areas and the widely used approaches/techniques in the domain of machine learning and deep learning.\"}, {\"paperId\": \"4924764d3e41fd4f898b1239ae424fc57f643e81\", \"abstract\": \"Objective The objective of this systematic review was to investigate the quality and outcome of studies into artificial intelligence techniques, analysis, and effect in dentistry. Materials and Methods Using the MeSH keywords: artificial intelligence (AI), dentistry, AI in dentistry, neural networks and dentistry, machine learning, AI dental imaging, and AI treatment recommendations and dentistry. Two investigators performed an electronic search in 5 databases: PubMed/MEDLINE (National Library of Medicine), Scopus (Elsevier), ScienceDirect databases (Elsevier), Web of Science (Clarivate Analytics), and the Cochrane Collaboration (Wiley). The English language articles reporting on AI in different dental specialties were screened for eligibility. Thirty-two full-text articles were selected and systematically analyzed according to a predefined inclusion criterion. These articles were analyzed as per a specific research question, and the relevant data based on article general characteristics, study and control groups, assessment methods, outcomes, and quality assessment were extracted. Results The initial search identified 175 articles related to AI in dentistry based on the title and abstracts. The full text of 38 articles was assessed for eligibility to exclude studies not fulfilling the inclusion criteria. Six articles not related to AI in dentistry were excluded. Thirty-two articles were included in the systematic review. It was revealed that AI provides accurate patient management, dental diagnosis, prediction, and decision making. Artificial intelligence appeared as a reliable modality to enhance future implications in the various fields of dentistry, i.e., diagnostic dentistry, patient management, head and neck cancer, restorative dentistry, prosthetic dental sciences, orthodontics, radiology, and periodontics. Conclusion The included studies describe that AI is a reliable tool to make dental care smooth, better, time-saving, and economical for practitioners. AI benefits them in fulfilling patient demand and expectations. The dentists can use AI to ensure quality treatment, better oral health care outcome, and achieve precision. AI can help to predict failures in clinical scenarios and depict reliable solutions. However, AI is increasing the scope of state-of-the-art models in dentistry but is still under development. Further studies are required to assess the clinical performance of AI techniques in dentistry.\"}, {\"paperId\": \"e6c4af7dab36d36e149ead3937e8dc1e73b3263a\", \"abstract\": \"Concepts like Machine Learning, Data Mining or Artificial Intelligence have become part of our daily life. This is mostly due to the incredible advances made in computation (hardware and software), the increasing capabilities of generating and storing all types of data and, especially, the benefits (societal and economical) that generate the analysis of such data. Simultaneously, Chemometrics has played an important role since the late 1970s, analyzing data within natural science (and especially in Analytical Chemistry). Even with the strong parallelisms between all of the abovementioned terms and being popular with most of us, it is still difficult to clearly define or differentiate the meaning of Machine Learning, Data Mining, Artificial Intelligence, Deep Learning and Chemometrics. This manuscript brings some light to the definitions of Machine Learning, Data Mining, Artificial Intelligence and Big Data Analysis, defines their application ranges and seeks an application space within the field of analytical chemistry (a.k.a. Chemometrics). The manuscript is full of personal, sometimes probably subjective, opinions and statements. Therefore, all opinions here are open for constructive discussion with the only purpose of Learning (like the Machines do nowadays).\"}, {\"paperId\": \"5ff1700928df55e617be83c2f62827fc31858889\", \"abstract\": \"Mathematical and combinatorial techniques with nonnegative integers are used as computing algorithms for the programs development to apply in artificial intelligence and cybersecurity. Methodological advances in combinatorics and mathematics play a vital role in artificial intelligence and machine learning for data analysis and artificial intelligence-based cybersecurity for protection of the computing systems, devices, networks, programs and data from cyber-attacks. In connection with these ideas, this article is prepared for applications in computing science and cybersecurity. This paper presents computing and combinatorial formulae such as theorems on factorials, binomial, and multinomial coefficients and probability and binomial distributions.\"}, {\"paperId\": \"18ad2f0c2cfdcfcf376a28fed3226c33a4ad343f\", \"abstract\": \"Machine learning is a branch of artificial intelligence and computer science. The purpose of machine learning is to predict new data by using the existing data. In this study, two different machine learning methods which are Polynomial Regression (PR) and Artificial Neural Network (ANN) are applied to the neutron transport problems which are albedo problem, the Milne problem, and the criticality problem. ANN applications contain two different activation functions, Leaky Relu and Elu. The training data set is calculated by using the HN method. PR and ANN results are compared with the literature data. The study is only based on the existing data; therefore, the study could be thought only data mining on the one-speed neutron transport problems for isotropic scattering.\\u00a0\"}, {\"paperId\": \"af49e23f4866d3a1e9b47114e9ffe47e7a3defa4\", \"abstract\": null}, {\"paperId\": \"d9a64723c5b12d806ec509c5774fd78224e54c78\", \"abstract\": \"Artificial intelligence (AI) is a branch of science concerned with developing programs and computers that can gather data, reason about it, and then translate it into intelligent actions. AI is a broad area that includes reasoning, typical linguistic dispensation, machine learning, and planning. In the area of medicine and dentistry, machine learning is currently the most widely used AI application. This narrative review is aimed at giving an outline of cephalometric analysis in orthodontics using AI. Latest algorithms are developing rapidly, and computational resources are increasing, resulting in increased efficiency, accuracy, and reliability. Current techniques for completely automatic identification of cephalometric landmarks have considerably improved efficiency and growth prospects for their regular use. The primary considerations for effective orthodontic treatment are an accurate diagnosis, exceptional treatment planning, and good prognosis estimation. The main objective of the AI technique is to make dentists' work more precise and accurate. AI is increasingly being used in the area of orthodontic treatment. It has been evidenced to be a time-saving and reliable tool in many ways. AI is a promising tool for facilitating cephalometric tracing in routine clinical practice and analyzing large databases for research purposes.\"}, {\"paperId\": \"26c9627f2376ee3eaad0bcd1e738fdbdbd65eb9c\", \"abstract\": \"Artificial Intelligence (AI) is increasingly used within plant science, yet it is far from being routinely and effectively implemented in this domain. Particularly relevant to the development of novel food and agricultural technologies is the development of validated, meaningful and usable ways to integrate, compare and visualise large, multi-dimensional datasets from different sources and scientific approaches. After a brief summary of the reasons for the interest in data science and AI within plant science, the paper identifies and discusses eight key challenges in data management that must be addressed to further unlock the potential of AI in crop and agronomic research, and particularly the application of Machine Learning (AI) which holds much promise for this domain.\"}, {\"paperId\": \"49810b5c41898438a5f79c99c9e3579af02d8c97\", \"abstract\": \"The development of Artificial Intelligence (AI) pushes the boundaries of new computing paradigms to become actual realities for many science and engineering challenges. The delicacy and agility of a computing instrument do not mean anything when we cannot use it to create value and solve problems. Machine learning, a trendy subfield of AI, focuses on extracting and identifying insightful and actionable information from big and complex data using different types of neural networks. Data-hungry by its nature, machine learning algorithms usually excel in the practical fields that generate and possess abundant data. The two application areas we are interested in are drug repositioning and bioinformatics, both of which are the very fields often producing a large volume of data. This thematic issue aims to cover the recent advancement in artificial intelligence methods and applications that are developed and introduced in the field of bioinformatics and drug repurposing and provide a comprehensive and up-to-date collection of research and experiment works in these areas.\"}, {\"paperId\": \"812044a755c8790b47c0f19335200bbad736df09\", \"abstract\": \"Traditional decline curve analyses (DCAs), both deterministic and probabilistic, use specific models to fit production data for production forecasting. Various decline curve models have been applied for unconventional wells, including the Arps model, stretched exponential model, Duong model, and combined capacitance-resistance model. However, it is not straightforward to determine which model should be used, as multiple models may fit a dataset equally well but provide different forecasts, and hastily selecting a model for probabilistic DCA can underestimate the uncertainty in a production forecast. Data science, machine learning, and artificial intelligence are revolutionizing the oil and gas industry by utilizing computing power more effectively and efficiently. We propose a data-driven approach in this paper to performing short term predictions for unconventional oil production. Two states of the art level models have tested: DeepAR and used Prophet time series analysis on petroleum production data. Compared with the traditional approach using decline curve models, the machine learning approach can be regarded as\\u201d model-free\\u201d (non-parametric) because the pre-determination of decline curve models is not required. The main goal of this work is to develop and apply neural networks and time series techniques to oil well data without having substantial knowledge regarding the extraction process or physical relationship between the geological and dynamic parameters. For evaluation and verification purpose, The proposed method is applied to a selected well of Midland fields from the USA. By comparing our results, we can infer that both DeepAR and Prophet analysis are useful for gaining a better understanding of the behavior of oil wells, and can mitigate over/underestimates resulting from using a single decline curve model for forecasting. In addition, the proposed approach performs well in spreading model uncertainty to uncertainty in production forecasting; that is, we end up with a forecast which outperforms the standard DCA methods.\"}, {\"paperId\": \"c339a5a2c8e359f8dc025607bc9f0404da51ff58\", \"abstract\": \"This article reviews the history and applications of artificial intelligence and its capacity to support clinical research and practice in the field of obstetrics and gynecology. In the digital age of the 21st century, we have witnessed an explosion in data matched by remarkable progress in the field of computer science and engineering, with the development of powerful and portable artificial intelligence\\u2013powered technologies. At the same time, global connectivity powered by mobile technology has led to an increasing number of connected users and connected devices. In just the past 5 years, the convergence of these technologies in obstetrics and gynecology has resulted in the development of innovative artificial intelligence\\u2013powered digital health devices that allow easy and accurate patient risk stratification for an array of conditions spanning early pregnancy, labor and delivery, and care of the newborn. Yet, breakthroughs in artificial intelligence and other new and emerging technologies currently have a slow adoption rate in medicine, despite the availability of large data sets that include individual electronic health records spanning years of care, genomics, and the microbiome. As a result, patient interactions with health care remain burdened by antiquated processes that are inefficient and inconvenient. A few health care institutions have recognized these gaps and, with an influx of venture capital investments, are now making in-roads in medical practice with digital products driven by artificial intelligence algorithms. In this article, we trace the history, applications, and ethical challenges of the artificial intelligence that will be at the forefront of digitally transforming obstetrics and gynecology and medical practice in general.\"}, {\"paperId\": \"d7701e78e0bfc92b03a89582e80cfb751ac03f26\", \"abstract\": \"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.\"}, {\"paperId\": \"38f41c7a64530dbee6223d87306eff24681c6508\", \"abstract\": null}, {\"paperId\": \"08658fb44198da75de5f501d18bbe6bb2fc00eac\", \"abstract\": \"Machine learning (ML), a branch of artificial intelligence (AI), is a method that enables systems to learn from data for the purpose of recognizing patterns and making decisions without being explicitly programmed. In the past decade machine learning has been growing rapidly, and ML technologies such as speech recognition, spam filters, smart email reply, online recommendations, face recognition, fake news detection, and self-driving cars have become pivotal in modern daily life. However, computer science education has not yet fully adjusted to the tremendous growth in the sub-field of AI. This paper describes an approach of introducing K-12 students to ML through an on-line summer camp. The students are introduced to the concept of ML by hands-on activities of developing applications for recognizing text, numbers, sounds, images, and video data using a web-based cloud service tool \\\"Machine Learning for Kids\\\" and Scratch 3 programming language combined with Lego Mindstorms EV3 robots. The results show that the tools and technologies used in the camp are suitable for K-12 students, also when used in the form of online training. Pre and post surveys show that students express basic knowledge in ML and higher interest in coding and STEM after being exposed to the proposed training.\"}, {\"paperId\": \"40967c1c2adbfb200ff593b1fad1614b4a77d49f\", \"abstract\": \"Correspondence *Iain Barclay, Crime and Security Research Institute, School of Computer Science and Informatics, Cardiff University, Queen\\u2019s Buildings, Cardiff, CF24 3AA, UK. Email: BarclayIS@cardiff.ac.uk Summary Adopting shared data resources requires scientists to place trust in the originators of the data. When shared data is later used in the development of artificial intelligence (AI) systems or machine learning (ML) models, the trust lineage extends to the users of the system, typically practitioners in fields such as healthcare and finance. Practitioners rely on AI developers to have used relevant, trustworthy data, but may have limited insight and recourse. This paper introduces a software architecture and implementation of a system based on design patterns from the field of self-sovereign identity. Scientists can issue signed credentials attesting to qualities of their data resources. Data contributions to ML models are recorded in a bill of materials (BOM), which is stored with the model as a verifiable credential. The BOM provides a traceable record of the supply chain for an AI system, which facilitates ongoing scrutiny of the qualities of the contributing components. The verified BOM, and its linkage to certified data qualities, is used in the AI Scrutineer, a web-based tool designed to offer practitioners insight into ML model constituents and highlight any problems with adopted datasets, should they be found to have biased data or be otherwise discredited.\"}, {\"paperId\": \"3e0eff57dadf3a36b90672f973e1405200016df3\", \"abstract\": null}, {\"paperId\": \"38be58a9f51ed2af7e9e75793c38529de8aa5267\", \"abstract\": \"Machine learning is a subcategory of artificial intelligence, which aims to make computers capable of solving complex problems without being explicitly programmed. Availability of large datasets, development of effective algorithms, and access to the powerful computers have resulted in the unprecedented success of machine learning in recent years. This powerful tool has been employed in a plethora of science and engineering domains including mining and minerals industry. Considering the ever-increasing global demand for raw materials, complexities of the geological structure of ore deposits, and decreasing ore grade, high-quality and extensive mineralogical information is required. Comprehensive analyses of such invaluable information call for advanced and powerful techniques including machine learning. This paper presents a systematic review of the efforts that have been dedicated to the development of machine learning-based solutions for better utilizing mineralogical data in mining and mineral studies. To that end, we investigate the main reasons behind the superiority of machine learning in the relevant literature, machine learning algorithms that have been deployed, input data, concerned outputs, as well as the general trends in the subject area.\"}, {\"paperId\": \"024b960a1d833e495f0230c3d789571dae7ec0b1\", \"abstract\": \"Public healthcare has a history of cautious adoption for artificial intelligence (AI) systems. The rapid growth of data collection and linking capabilities combined with the increasing diversity of the data-driven AI techniques, including machine learning (ML), has brought both ubiquitous opportunities for data analytics projects and increased demands for the regulation and accountability of the outcomes of these projects. As a result, the area of interpretability and explainability of ML is gaining significant research momentum. While there has been some progress in the development of ML methods, the methodological side has shown limited progress. This limits the practicality of using ML in the health domain: the issues with explaining the outcomes of ML algorithms to medical practitioners and policy makers in public health has been a recognized obstacle to the broader adoption of data science approaches in this domain. This study builds on the earlier work which introduced CRISP-ML, a methodology that determines the interpretability level required by stakeholders for a successful real-world solution and then helps in achieving it. CRISP-ML was built on the strengths of CRISP-DM, addressing the gaps in handling interpretability. Its application in the Public Healthcare sector follows its successful deployment in a number of recent real-world projects across several industries and fields, including credit risk, insurance, utilities, and sport. This study elaborates on the CRISP-ML methodology on the determination, measurement, and achievement of the necessary level of interpretability of ML solutions in the Public Healthcare sector. It demonstrates how CRISP-ML addressed the problems with data diversity, the unstructured nature of data, and relatively low linkage between diverse data sets in the healthcare domain. The characteristics of the case study, used in the study, are typical for healthcare data, and CRISP-ML managed to deliver on these issues, ensuring the required level of interpretability of the ML solutions discussed in the project. The approach used ensured that interpretability requirements were met, taking into account public healthcare specifics, regulatory requirements, project stakeholders, project objectives, and data characteristics. The study concludes with the three main directions for the development of the presented cross-industry standard process.\"}, {\"paperId\": \"a9148c2edaeae70155bd4b272527e8d24d9daf30\", \"abstract\": \"Digital technologies and data science have laid down the promise to revolutionize healthcare by transforming the way health and disease are analyzed and managed in the future. Digital health applications in healthcare include telemedicine, electronic health records, wearable, implantable, injectable and ingestible digital medical devices, health mobile apps as well as the application of artificial intelligence and machine learning algorithms to medical and public health prognosis and decision-making. As is often the case with technological advancement, progress in digital health raises compelling ethical, legal, and social implications (ELSI). This article aims to succinctly map relevant ELSI of the digital health field. The issues of patient autonomy; assessment, value attribution, and validation of health innovation; equity and trustworthiness in healthcare; professional roles and skills and data protection and security are highlighted against the backdrop of the risks of dehumanization of care, the limitations of machine learning-based decision-making and, ultimately, the future contours of human interaction in medicine and public health. The running theme to this article is the underlying tension between the promises of digital health and its many challenges, which is heightened by the contrasting pace of scientific progress and the timed responses provided by law and ethics. Digital applications can prove to be valuable allies for human skills in medicine and public health. Similarly, ethics and the law can be interpreted and perceived as more than obstacles, but also promoters of fairness, inclusiveness, creativity and innovation in health.\"}, {\"paperId\": \"478eb603e63f0c3a05566274e85d63f363a3ed38\", \"abstract\": \"Artificial intelligence (AI) has received widespread attention over the last few decades due to its potential to increase automation and accelerate productivity. In recent years, a large number of training data, improved computing power, and advanced deep learning algorithms are conducive to the wide application of AI, including material research. The traditional trial\\u2010and\\u2010error method is inefficient and time\\u2010consuming to study materials. Therefore, AI, especially machine learning, can accelerate the process by learning rules from datasets and building models to predict. This is completely different from computational chemistry where a computer is only a calculator, using hard\\u2010coded formulas provided by human experts. Herein, the application of AI in material innovation is reviewed, including material design, performance prediction, and synthesis. The realization details of AI techniques and advantages over conventional methods are emphasized in these applications. Finally, the future development direction of AI is expounded from both algorithm and infrastructure aspects.\"}, {\"paperId\": \"0adcfd0e9c5e8f55b93f40a0fe8537bc68ccc822\", \"abstract\": \"The most mature aspect of applying artificial intelligence (AI)/machine learning (ML) to problems in the atmospheric sciences is likely post-processing of model output. This article provides some history and current state of the science of post-processing with AI for weather and climate models. Deriving from the discussion at the 2019 Oxford workshop on Machine Learning for Weather and Climate, this paper also presents thoughts on medium-term goals to advance such use of AI, which include assuring that algorithms are trustworthy and interpretable, adherence to FAIR data practices to promote usability, and development of techniques that leverage our physical knowledge of the atmosphere. The coauthors propose several actionable items and have initiated one of those: a repository for datasets from various real weather and climate problems that can be addressed using AI. Five such datasets are presented and permanently archived, together with Jupyter notebooks to process them and assess the results in comparison with a baseline technique. The coauthors invite the readers to test their own algorithms in comparison with the baseline and to archive their results. This article is part of the theme issue \\u2018Machine learning for weather and climate modelling\\u2019.\"}, {\"paperId\": \"6935b0226829ae32f82f55447992f9d045716dfa\", \"abstract\": \"Background Advances in computer processing and improvements in data availability have led to the development of machine learning (ML) techniques for mammographic imaging. Purpose To evaluate the reported performance of stand-alone ML applications for screening mammography workflow. Materials and Methods Ovid Embase, Ovid Medline, Cochrane Central Register of Controlled Trials, Scopus, and Web of Science literature databases were searched for relevant studies published from January 2012 to September 2020. The study was registered with the PROSPERO International Prospective Register of Systematic Reviews (protocol no. CRD42019156016). Stand-alone technology was defined as a ML algorithm that can be used independently of a human reader. Studies were quality assessed using the Quality Assessment of Diagnostic Accuracy Studies 2 and the Prediction Model Risk of Bias Assessment Tool, and reporting was evaluated using the Checklist for Artificial Intelligence in Medical Imaging. A primary meta-analysis included the top-performing algorithm and corresponding reader performance from which pooled summary estimates for the area under the receiver operating characteristic curve (AUC) were calculated using a bivariate model. Results Fourteen articles were included, which detailed 15 studies for stand-alone detection (n = 8) and triage (n = 7). Triage studies reported that 17%-91% of normal mammograms identified could be read by adapted screening, while \\\"missing\\\" an estimated 0%-7% of cancers. In total, an estimated 185\\u2009252 cases from three countries with more than 39 readers were included in the primary meta-analysis. The pooled sensitivity, specificity, and AUC was 75.4% (95% CI: 65.6, 83.2; P = .11), 90.6% (95% CI: 82.9, 95.0; P = .40), and 0.89 (95% CI: 0.84, 0.98), respectively, for algorithms, and 73.0% (95% CI: 60.7, 82.6), 88.6% (95% CI: 72.4, 95.8), and 0.85 (95% CI: 0.78, 0.97), respectively, for readers. Conclusion Machine learning (ML) algorithms that demonstrate a stand-alone application in mammographic screening workflows achieve or even exceed human reader detection performance and improve efficiency. However, this evidence is from a small number of retrospective studies. Therefore, further rigorous independent external prospective testing of ML algorithms to assess performance at preassigned thresholds is required to support these claims. \\u00a9RSNA, 2021 Online supplemental material is available for this article. See also the editorial by Whitman and Moseley in this issue.\"}, {\"paperId\": \"ac8c3bb4455d71ef2d86cff9b9ae34ae238cb6ee\", \"abstract\": \"Machine learning (ML) has taken the world by a tornado with its prevalent applications in automating ordinary tasks and using turbulent insights throughout scientific research and design strolls. ML is a massive area within artificial intelligence (AI) that focuses on obtaining valuable information out of data, explaining why ML has often been related to stats and data science. An advanced meta-heuristic optimization algorithm is proposed in this work for the optimization problem of antenna architecture design. The algorithm is designed, depending on the hybrid between the Sine Cosine Algorithm (SCA) and the Grey Wolf Optimizer (GWO), to train neural networkbased Multilayer Perceptron (MLP). The proposed optimization algorithm is a practical, versatile, and trustworthy platform to recognize the design parameters in an optimal way for an endorsement double T-shaped monopole antenna. The proposed algorithm likewise shows a comparative and statistical analysis by different curves in addition to the ANOVA and T-Test. It offers the superiority and validation stability evaluation of the predicted results to verify the procedures\\u2019 accuracy.\"}, {\"paperId\": \"e9eb90bda3bbfd990519818d31e839a9be5318e8\", \"abstract\": \"Advancements in technology and data collection generated immense amounts of information from various sources such as health records, clinical examination, imaging, medical devices, as well as experimental and biological data. Proper management and analysis of these data via high-end computing solutions, artificial intelligence and machine learning approaches can assist in extracting meaningful information that enhances population health and well-being. Furthermore, the extracted knowledge can provide new avenues for modern healthcare delivery via clinical decision support systems. This manuscript\\u00a0presents a narrative review of data science approaches for clinical decision support systems in orthodontics. We describe the fundamental components of data science approaches including (1) Data collection, storage and management; (2) Data processing; (3) In-depth data analysis; and (4) Data communication. Then, we introduce a web-based data management platform, the Data Storage for Computation and Integration, for temporomandibular joint and dental clinical decision support systems.\"}, {\"paperId\": \"3e6219db2f620a008fe9f47ffb390aaad6c5bab6\", \"abstract\": null}, {\"paperId\": \"44c5cbd0a367fdc58d90192c7a2e353c9fd932bb\", \"abstract\": null}, {\"paperId\": \"3017663daf20f71c3a17637f041de6c5c3507745\", \"abstract\": \"\\n \\n \\nCOVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Machine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets, tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international cooperation to maximize the potential of AI in this and future pandemics. \\n \\n \\n\"}, {\"paperId\": \"cd60d06e2e9ad4d3e56f998b34beb14e8e585c05\", \"abstract\": \"There is increasing popularity in the use of artificial intelligence and machine-learning techniques to provide diagnostic and prognostic models for various aspects of Trauma & Orthopaedic surgery. However, correct interpretation of these models is difficult for those without specific knowledge of computing or health data science methodology. Lack of current reporting standards leads to the potential for significant heterogeneity in the design and quality of published studies. We provide an overview of machine-learning techniques for the lay individual, including key terminology and best practice reporting guidelines. Cite this article: Bone Joint J\\u00a02021;103-B(12):1754-1758.\"}, {\"paperId\": \"4123d49d11bd5fe91b8b418020eb0028e0e48a79\", \"abstract\": \"In recent years, the rapid development of artificial intelligence and data science has given rise to the study of data driven algorithms in highly volatile systems. The scheduling of complex shop floor resources falls into such a category, which is often non \\u2010 linear in nature, time varying, multi \\u2010 objective, and subject to interruptions. Ergo, the machine learning \\u2010 based scheduling, has become a research hotspot and attracted the attention of many scholars. In the literature, the research methods employed in solving scheduling problems are based on various perspectives, such as mathematical programming, combinatorial optimization, and heuristic rules. However, due to the inherent complexity of the problem, many issues remain to be addressed. In particular, with the availability of production data, the progress of computing power, and the breakthrough in intelligent algorithms, a novel branch of data driven algorithms present great potential, for example, the deep learning and reinforcement learning \\u2010 based algorithms. To reveal the value of machine learning \\u2010 based scheduling methods, bibliometric analysis was conducted to analyse the relevant articles and documents from the year 1980 to 2019. Finally, the future research trend in the domain of machine learning \\u2010 based scheduling is considered and tips are provided for researchers as well as practitioners to find leading scientists for collaborations.\"}, {\"paperId\": \"95beb6c2a030929486c91d101f05caf8622c1527\", \"abstract\": \"With recent technological advances, large-scale experimental facilities generate huge datasets, into the petabyte range, every year, thereby creating the Big Data deluge effect. Data management, including the collection, management, and curation of these large datasets, is a significantly intensive precursor step in relation to the data analysis that underpins scientific investigations. The rise of artificial intelligence (AI), machine learning (ML), and robotic automation has changed the landscape for experimental facilities, producing a paradigm shift in how different datasets are leveraged for improved intelligence, operation, and data analysis. Therefore, such facilities, known as superfacilities, which fully enable user science while addressing the challenges of the Big Data deluge, are critical for the scientific community. In this work, we discuss the process of setting up the Big Data Science Center within the Shanghai Synchrotron Radiation Facility (SSRF), China\\u2019s first superfacility. We provide details of our initiatives for enabling user science at SSRF, with particular consideration given to recent developments in AI, ML, and robotic automation.\"}, {\"paperId\": \"a08a070203f2650cf400a6c4e4ab041cbc03f2eb\", \"abstract\": \"This systematic review adopts a formal and structured approach to review the intersection of data science and smart tourism destinations in terms of components found in previous research. The study period corresponds to 1995\\u20132021 focusing the analysis mainly on the last years (2015\\u20132021), identifying and characterizing the current trends on this research topic. The review comprises documentary research based on bibliometric and conceptual analysis, using the VOSviewer and SciMAT software to analyze articles from the Web of Science database. There is growing interest in this research topic, with more than 300 articles published annually. Data science technologies on which current smart destinations research is based include big data, smart data, data analytics, social media, cloud computing, the internet of things (IoT), smart card data, geographic information system (GIS) technologies, open data, artificial intelligence, and machine learning. Critical research areas for data science techniques and technologies in smart destinations are public tourism marketing, mobility-accessibility, and sustainability. Data analysis techniques and technologies face unprecedented challenges and opportunities post-coronavirus disease-2019 (COVID-19) to build on the huge amount of data and a new tourism model that is more sustainable, smarter, and safer than those previously implemented.\"}, {\"paperId\": \"1af69a9c9be7d46fcbb7e72d834f11edd7d55e42\", \"abstract\": \"Data industry has grown exceedingly, now finding its applications in the fields no one would have imagined a few decades ago. Data science, Machine learning and Artificial intelligence have major impacts in the national economy through a million ways. Today, we will explore one of their untapped uses: Traffic! This research document will provide methods and information regarding the possible integration of Radio-Frequency Identification (RFID) and Machine Learning (ML).<br>\"}, {\"paperId\": \"d7cdcca624836e1fcdf05066820e3c8f6497ffe2\", \"abstract\": \"Machine learning algorithms, and artificial intelligence in general, have a wide range of applications in the field of pharmaceutical technology. Starting from the formulation development, through a great potential for integration within the Quality by design framework, these data science tools provide a better understanding of the pharmaceutical formulations and respective processing. Machine learning algorithms can be especially helpful with the analysis of the large volume of data generated by the Process analytical technologies. This paper provides a brief explanation of the artificial neural networks, as one of the most frequently used machine learning algorithms. The process of the network training and testing is described and accompanied with illustrative examples of machine learning tools applied in the context of pharmaceutical formulation development and related technologies, as well as an overview of the future trends. Recently published studies on more sophisticated methods, such as deep neural networks and light gradient boosting machine algorithm, have been described. The interested reader is also referred to several official documents (guidelines) that pave the way for a more structured representation of the machine learning models in their prospective submissions to the regulatory bodies.\"}, {\"paperId\": \"2a906056df5286e8cac44e2a86651c3460aafc83\", \"abstract\": \"The advancement of computing and technology has invaded all the dimensions of science. Artificial intelligence (AI) is one core branch of Computer Science, which has percolated to all the arenas of science and technology, from core engineering to medicines. Thus, AI has found its way for application in the field of medicinal chemistry and heath care. The conventional methods of drug design have been replaced by computer-aided designs of drugs in recent times. AI is being used extensively to improve the design techniques and required time of the drugs. Additionally, the target proteins can be conveniently identified using AI, which enhances the success rate of the designed drug. The AI technology is used in each step of the drug designing procedure, which decreases the health hazards related to preclinical trials and also reduces the cost substantially. The AI is an effective tool for data mining based on the huge pharmacological data and machine learning process. Hence, AI has been used in de novo drug design, activity scoring, virtual screening and in silico evaluation in the properties (absorption, distribution, metabolism, excretion and toxicity) of a drug molecule. Various pharmaceutical companies have teamed up with AI companies for faster progress in the field of drug development, along with the healthcare system. The review covers various aspects of AI (Machine learning, Deep learning, Artificial neural networks) in drug design. It also provides a brief overview of the recent progress by the pharmaceutical companies in drug discovery by associating with different AI companies.\"}, {\"paperId\": \"1ddf99ec22555352e07aea8a783313a4bd2f8637\", \"abstract\": \"Background This paper explores machine learning algorithms and approaches for predicting alum income to obtain insights on the strongest predictors and a \\u2018high\\u2019 earners\\u2019 class. Methods It examines the alum sample data obtained from a survey from a multicampus Mexican private university. Survey results include 17,898 and 12,275 observations before and after cleaning and pre-processing, respectively. The dataset comprises income values and a large set of independent demographical attributes of former students. We conduct an in-depth analysis to determine whether the accuracy of traditional algorithms can be improved with a data science approach. Furthermore, we present insights on patterns obtained using explainable artificial intelligence techniques. Results Results show that the machine learning models outperformed the parametric models of linear and logistic regression, in predicting alum\\u2019s current income with statistically significant results (p < 0.05) in three different tasks. Moreover, the later methods were found to be the most accurate in predicting the alum\\u2019s first income after graduation. Conclusion We identified that age, gender, working hours per week, first income and variables related to the alum\\u2019s job position and firm contributed to explaining their current income. Findings indicated a gender wage gap, suggesting that further work is needed to enable equality.\"}, {\"paperId\": \"7eb9c4db229010b3a31c7d2fabb4f30f03e890d8\", \"abstract\": \"Increasing clinical contributions and novel techniques have been made by artificial intelligence (AI) during the last decade. The role of AI is increasingly recognized in cancer research and clinical application. Cancers like gastric cancer, or stomach cancer, are ideal testing grounds to see if early undertakings of applying AI to medicine can yield valuable results. There are numerous concepts derived from AI, including machine learning (ML) and deep learning (DL). ML is defined as the ability to learn data features without being explicitly programmed. It arises at the intersection of data science and computer science and aims at the efficiency of computing algorithms. In cancer research, ML has been increasingly used in predictive prognostic models. DL is defined as a subset of ML targeting multilayer computation processes. DL is less dependent on the understanding of data features than ML. Therefore, the algorithms of DL are much more difficult to interpret than ML, even potentially impossible. This review discussed the role of AI in the diagnostic, therapeutic and prognostic advances of gastric cancer. Models like convolutional neural networks (CNNs) or artificial neural networks (ANNs) achieved significant praise in their application. There is much more to be fully covered across the clinical administration of gastric cancer. Despite growing efforts, adapting AI to improving diagnoses for gastric cancer is a worthwhile venture. The information yield can revolutionize how we approach gastric cancer problems. Though integration might be slow and labored, it can be given the ability to enhance diagnosing through visual modalities and augment treatment strategies. It can grow to become an invaluable tool for physicians. AI not only benefits diagnostic and therapeutic outcomes, but also reshapes perspectives over future medical trajectory.\"}, {\"paperId\": \"a94a6f30db2600e1ba943a27e6e10c6126beabdb\", \"abstract\": null}, {\"paperId\": \"332c998f46a10101db40c0d1673b9b84749724b2\", \"abstract\": null}, {\"paperId\": \"267cd81d0a0dbfd4c78bfcf1ba11795c4a51f344\", \"abstract\": \"Archaeologists, like other scientists, are experiencing a dataflood in their discipline, fueled by a surge in computing power and devices that enable the creation, collection, storage and transfer of an increasingly complex (and large) amount of data, such as remotely sensed imagery from a multitude of sources. In this paper, we pose the preliminary question if this increasing availability of information actually needs new computerized techniques, and Artificial Intelligence methods, to make new and deeper understanding into archaeological problems. Simply said, while it is a fact that Deep Learning (DL) has become prevalent as a type of machine learning design inspired by the way humans learn, and utilized to perform automatic actions people might describe as intelligent, we want to anticipate, here, a discussion around the subject whether machines, trained following this procedure, can extrapolate, from archaeological data, concepts and meaning in the same way that humans would do. Even prior to getting to technical results, we will start our reflection with a very basic concept: Is a collection of satellite images with notable archaeological sites informative enough to instruct a DL machine to discover new archaeological sites, as well as other potential locations of interest? Further, what if similar results could be reached with less intelligent machines that learn by having people manually program them with rules? Finally: If with barrier of meaning we refer to the extent to which humanlike understanding can be achieved by a machine, where should be posed that barrier in the archaeological data science?\"}, {\"paperId\": \"e0325646aa37abcad0bf163dbdb055b2d9609cb4\", \"abstract\": \"Subjective paper evaluation is a tricky and tiresome task to do by manual labor. Insufficient understanding and acceptance of data are crucial challenges while analyzing subjective papers using Artificial Intelligence (AI). Several attempts have been made to score students\\u2019 answers using computer science. However, most of the work uses traditional counts or specific words to achieve this task. Furthermore, there is a lack of curated data sets as well. This paper proposes a novel approach that utilizes various machine learning, natural language processing techniques, and tools such as Wordnet, Word2vec, word mover\\u2019s distance (WMD), cosine similarity, multinomial naive bayes (MNB), and term frequency-inverse document frequency (TF-IDF) to evaluate descriptive answers automatically. Solution statements and keywords are used to evaluate answers, and a machine learning model is trained to predict the grades of answers. Results show that WMD performs better than cosine similarity overall. With enough training, the machine learning model could be used as a standalone as well. Experimentation produces an accuracy of 88% without the MNB model. The error rate is further reduced by 1.3% using MNB.\"}, {\"paperId\": \"4a0a86fe5314d4404520b494f91d04889e615976\", \"abstract\": \"The case focusses on Rho AI, a data science firm, and its attempt to leverage artificial intelligence to encourage environmental, social and governance investments to limit the impact of climate change. Rho AI\\u2019s proposed open-source artificial intelligence tool integrates automated web scraping technology and machine learning with natural language processing. The aim of the tool is to enable investors to evaluate the climate impact of companies and to use this evaluation as a basis for making investments in companies. The case study allows for students to gain an insight into some of the strategic choices that need to be considered when developing an artificial intelligence\\u2013based tool. Students will be able to explore the role of ethics in decision-making related to artificial intelligence, while familiarising themselves with key technical terminology and possible business models. The case encourages students to see beyond the technical granularities and to consider the multi-faceted, wider corporate and societal issues and priorities. This case contributes to students recognising that business is not conducted in a vacuum and enhances students\\u2019 understanding of the role of business in society during new developments triggered by digital technology.\"}, {\"paperId\": \"cff011e8974c903f7de4e88c604c4c9453e6d4f6\", \"abstract\": null}, {\"paperId\": \"e06e68cf27c9ab60676b2eafe35479bfbe37c94c\", \"abstract\": null}, {\"paperId\": \"9d709d1005b07c71b319ba0da4b130992aa257e5\", \"abstract\": \"Plant phenomics has been rapidly advancing over the past few years. This advancement is attributed to the increased innovation and availability of new technologies which can enable the high-throughput phenotyping of complex plant traits. The application of artificial intelligence in various domains of science has also grown exponentially in recent years. Notably, the computer vision, machine learning, and deep learning aspects of artificial intelligence have been successfully integrated into non-invasive imaging techniques. This integration is gradually improving the efficiency of data collection and analysis through the application of machine and deep learning for robust image analysis. In addition, artificial intelligence has fostered the development of software and tools applied in field phenotyping for data collection and management. These include open-source devices and tools which are enabling community driven research and data-sharing, thereby availing the large amounts of data required for the accurate study of phenotypes. This paper reviews more than one hundred current state-of-the-art papers concerning AI-applied plant phenotyping published between 2010 and 2020. It provides an overview of current phenotyping technologies and the ongoing integration of artificial intelligence into plant phenotyping. Lastly, the limitations of the current approaches/methods and future directions are discussed.\"}, {\"paperId\": \"4e5ed3ccac14e25ef0cc970de1b300a88439014c\", \"abstract\": \"Skin diseases are a major and worrying problem in societies due to their physical and psychological effects on patients. Detecting skin diseases at an early stage has an important role in treatment. The process of diagnosing and treating skin injury is related to the skill and experience of the specialist doctor. The diagnostic process must be accurate and timely. Recently, artificial intelligence science has been used in the field of diagnosing skin diseases through the use of machine learning algorithms and the exploitation of the vast amount of data available in health centers and hospitals. In this paper, quite many previous studies related to methods of classification of skin diseases based on the principle of machine learning were collected. In a group of previous studies, the researchers used some systems, mechanisms, and algorithms. Several systems have been successful in classifying skin diseases and achieving varying diagnostic accuracy. Various systems have relied on methods of image processing and feature extraction that help predict and detect disease type. There are other systems designed to identify specific types of skin disease through clinical features and features obtained from tissue analyzes after a skin biopsy of the affected area. This survey shows that the diagnostic accuracy in image processing methods was relatively uneven, ranged between (50% to 100%). As for the methods of treating tissue features, the accuracy was of an excellent level of 94% or more. The results provide an overview of the actual relevant studies found in the literature and highlight most of which research gaps have emerged.\"}, {\"paperId\": \"f4ec66a40b1bcb4598370879b78729131df0996d\", \"abstract\": \"Uncertainty quantification in artificial intelligence (AI)-based predictions of material properties is of immense importance for the success and reliability of AI applications in materials science. While confidence intervals are commonly reported for machine learning (ML) models, prediction intervals, i.e., the evaluation of the uncertainty on each prediction, are not as frequently available. In this work, we compare three different approaches to obtain such individual uncertainty, testing them on 12 ML-physical properties. Specifically, we investigated using the quantile loss function, machine learning the prediction intervals directly, and using Gaussian processes. We identify each approach\\u2019s advantages and disadvantages and end up slightly favoring the modeling of the individual uncertainties directly, as it is the easiest to fit and, in most of the cases, minimizes over- and underestimation of the predicted errors. All data for training and testing were taken from the publicly available JARVIS-DFT database, and the codes developed for computing the prediction intervals are available through the JARVIS-tools package.\"}, {\"paperId\": \"98c75c5c3c7290b0a086403bc48e317cd6513cc4\", \"abstract\": \"Technical and methodological enhancement of hazards and disaster research is identified as a critical question in disaster management. Artificial intelligence (AI) applications, such as tracking and mapping, geospatial analysis, remote sensing techniques, robotics, drone technology, machine learning, telecom and network services, accident and hot spot analysis, smart city urban planning, transportation planning, and environmental impact analysis, are the technological components of societal change, having significant implications for research on the societal response to hazards and disasters. Social science researchers have used various technologies and methods to examine hazards and disasters through disciplinary, multidisciplinary, and interdisciplinary lenses. They have employed both quantitative and qualitative data collection and data analysis strategies. This study provides an overview of the current applications of AI in disaster management during its four phases and how AI is vital to all disaster management phases, leading to a faster, more concise, equipped response. Integrating a geographic information system (GIS) and remote sensing (RS) into disaster management enables higher planning, analysis, situational awareness, and recovery operations. GIS and RS are commonly recognized as key support tools for disaster management. Visualization capabilities, satellite images, and artificial intelligence analysis can assist governments in making quick decisions after natural disasters.\"}, {\"paperId\": \"b9dc83d35167b156429201f1f2afa493dc7c1ab8\", \"abstract\": \"Despite recent improvements in therapeutic interventions, hepatocellular carcinoma is still associated with a poor prognosis in patients with an advanced disease at diagnosis. Recently, significant progress has been made in image recognition through advances in the field of artificial intelligence (AI) (or machine learning), especially deep learning. AI is a multidisciplinary field that draws on the fields of computer science and mathematics for developing and implementing computer algorithms capable of maximizing the predictive accuracy from static or dynamic data sources using analytic or probabilistic models. Because of the multifactorial and complex nature of liver diseases, the machine learning approach to integrate multiple factors would appear to be an advantageous approach to improve the likelihood of making a precise diagnosis and predicting the response of treatment and prognosis of liver diseases. In this review, we attempted to summarize the potential use of AI in the diagnosis and management of liver diseases, especially hepatocellular carcinoma.\"}, {\"paperId\": \"d287f81ff05551d65ba422afce4b253d3021b319\", \"abstract\": null}, {\"paperId\": \"b5b98051b65da6b1b3b579862b0407d48c5bef48\", \"abstract\": \"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods\\u2014machine learning (ML) and pattern recognition models in particular\\u2014so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.\"}, {\"paperId\": \"3d991ee05dac750949fc5001c8d3c6c0c9fa47f9\", \"abstract\": \"Nowadays, the construction industry is on a fast track to adopting digital processes under the Industrial Revolution (IR) 4.0. The desire to automate maximum construction processes with less human interference has led the industry and research community to inclined towards artificial intelligence. This chapter has been themed on automated construction monitoring practices by adopting material classification via machine learning (ML) techniques. The study has been conducted by following the structure review approach to gain an understanding of the applications of ML techniques for construction progress assessment. Data were collected from the Web of Science (WoS) and Scopus databases, concluding 14 relevant studies. The literature review depicted the support vector machine (SVM) and artificial neural network (ANN) techniques as more effective than other ML techniques for material classification. The last section of this chapter includes a python-based ANN model for material classification. This ANN model has been tested for construction items (brick, wood, concrete block, and asphalt) for training and prediction. Moreover, the predictive ANN model results have been shared for the readers, along with the resources and open-source web links.\"}, {\"paperId\": \"b2e0bba3afa75fd67f012f7a9b031d51c3633292\", \"abstract\": \"Artificial intelligence is a subject that studies all kinds of human intelligent activities and their laws. It is developed on the basis of the cohesion of many disciplines such as computer science, politics, information system, neurophysiology, psychology, philosophy, and language. This paper aims to study how to build a computer or intelligent machine, including hardware and software, imitate and expand the human brain to perform thinking functions such as thinking, programming, arithmetic, and learning, solve complex problems that need to be handled by professionals, and better apply the artificial intelligence assistance system to the teaching of piano performance. In this paper, Prolog language and music-assisted learning system based on the ARM and SA algorithm are proposed, the principle and operation process of music automatic recording technology are deeply studied, and the system data of artificial intelligence are summarized and analyzed by using internal database, so as to find out the implementation principle and law of piano automatic recording system. So that the artificial intelligence assistant system can be better applied to music teaching. The experimental results show that, in the teaching system of piano performance and music automatic notation algorithm, the utilization rate of artificial intelligence auxiliary technology has reached 56.81 and is growing rapidly. Therefore, we can find that the artificial intelligence assistant system plays an important role in the teaching system of piano performance and automatic music notation.\"}, {\"paperId\": \"0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24\", \"abstract\": \"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools\\u2014including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization\\u2014that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.\"}, {\"paperId\": \"eb2e6b9640248fe5d327e680ecce59057e8fea96\", \"abstract\": \"Spatial metabolomics is an emerging field of omics research that has enabled localizing metabolites, lipids, and drugs in tissue sections, a feat considered impossible just two decades ago. Spatial metabolomics and its enabling technology-imaging mass spectrometry-generate big hyper-spectral imaging data that have motivated the development of tailored computational methods at the intersection of computational metabolomics and image analysis. Experimental and computational developments have recently opened doors to applications of spatial metabolomics in life sciences and biomedicine. At the same time, these advances have coincided with a rapid evolution in machine learning, deep learning, and artificial intelligence, which are transforming our everyday life and promise to revolutionize biology and healthcare. Here, we introduce spatial metabolomics through the eyes of a computational scientist, review the outstanding challenges, provide a look into the future, and discuss opportunities granted by the ongoing convergence of human and artificial intelligence.\"}, {\"paperId\": \"38b9ef1248a7770ddf14c0218cb461957c7e8bf8\", \"abstract\": null}, {\"paperId\": \"298941fdc7991949af5f34dcab1138d6d3a037de\", \"abstract\": \"With the continuous development of the Internet of Things, artificial intelligence, big data technology, and intelligent agriculture have become hot topics in agricultural science and technology research. Machine learning is one of the core topics in artificial intelligence, and its application has penetrated every aspect of human social life. In modern agricultural intelligent management and decision making, machine learning plays an important role in crop classification, crop disease and insect pest prediction, agricultural product price prediction, and other aspects of management and decision-making processes in agriculture. To detect and recognize the latest research developing features in a quantitative and visual way, and based on machine learning methods in agricultural management, the authors of this paper used CiteSpace bibliometric methods to analyze relevant studies on the development process and hot spots. High-value references, productive authors, country and institution distributions, journal visualizations, research topics, and emerging trends were reviewed and analyzed. According to the keyword visualization and high-value references, machine learning approaches focus on sustainable agriculture, water resources, remote sensing, and machine learning methods. The research mainly focuses on six topics: learning technology, land environment, reference evapotranspiration, decision support systems for river geography, soil management, and winter wheat, while learning technology has been the most popular in recent years.\"}, {\"paperId\": \"2ff8ef80f664e39cdb5bfd4e14a095919d904a47\", \"abstract\": \"Artificial intelligence represents the science which will probably change the future of medicine by solving actually challenging issues. In this special article, the general features of machine learning are discussed. First, a background explanation regarding the division of artificial intelligence, machine learning and deep learning is given and a focus on the structure of machine learning subgroups is shown. The traditional process of a machine learning analysis is described, starting from the collection of data, across features engineering, modelling and till the validation and deployment phase. Due to the several applications of machine learning performed in literature in the last decades and the lack of some guidelines, the need of a standardization for reporting machine learning analysis results emerged. Some possible standards for reporting machine learning results are identified and discussed deeply; these are related to study population (number of subjects), repeatability of the analysis, validation, results, comparison with current practice. The way to the use of machine learning in clinical practice is open and the hope is that, with emerging technology and advanced digital and computational tools, available from hospitalization and subsequently after discharge, it will also be possible, with the help of increasingly powerful hardware, to build assistance strategies useful in clinical practice.\"}, {\"paperId\": \"948ac22bb840414bf82017efd93e075a20d5e201\", \"abstract\": \"Machine learning (ML) is the subfield of artificial intelligence (AI), born from the marriage between statistics and computer science, with the unique purpose of building prediction algorithms able to improve their performances by automatically learning from massive data sets. The availability of ever-growing computational power and highly evolved pattern recognition software has led to the spread of ML-based systems able to perform complex tasks in bioinformatics, medical imaging, and diagnostics. These intelligent tools could be the answer to the unmet need for non-invasive and patient-tailored instruments for the diagnosis and management of bladder cancer (BC), which are still based on old technologies and unchanged nomograms. We reviewed the most significant evidence on ML in the diagnosis, prognosis, and management of bladder cancer, to find out if these intelligent technologies are ready to be introduced into the daily clinical practice of the urologist.\"}, {\"paperId\": \"f5223ae7749f650f46db9fb66809374421b4e8ee\", \"abstract\": \"Supplemental Digital Content is available in the text. Machine learning (ML) represents a collection of advanced data modeling techniques beyond the traditional statistical models and tests with which most clinicians are familiar. While a subset of artificial intelligence, ML is far from the science fiction impression frequently associated with AI. At its most basic, ML is about pattern finding, sometimes with complex algorithms. The advanced mathematical modeling of ML is seeing expanding use throughout healthcare and increasingly in the day-to-day practice of surgeons. As with any new technique or technology, a basic understanding of principles, applications, and limitations are essential for appropriate implementation. This primer is intended to provide the surgical reader an accelerated introduction to applied ML and considerations in potential research applications or the review of publications, including ML techniques.\"}, {\"paperId\": \"3f67393a3775852280e14873f55c8d59087ecb57\", \"abstract\": \"Rapid growth in data, computational methods, and computing power is driving a remarkable revolution in what variously is termed machine learning (ML), statistical learning, computational learning, and artificial intelligence. In addition to highly visible successes in machine-based natural language translation, playing the game Go, and self-driving cars, these new technologies also have profound implications for computational and experimental science and engineering, as well as for the exascale computing systems that the Department of Energy (DOE) is developing to support those disciplines. Not only do these learning technologies open up exciting opportunities for scientific discovery on exascale systems, they also appear poised to have important implications for the design and use of exascale computers themselves, including high-performance computing (HPC) for ML and ML for HPC. The overarching goal of the ExaLearn co-design project is to provide exascale ML software for use by Exascale Computing Project (ECP) applications, other ECP co-design centers, and DOE experimental facilities and leadership class computing facilities.\"}, {\"paperId\": \"d296c8349e46a4805e233190e673e52f3d818e8a\", \"abstract\": \"1 Master of Engineering student, Department of Computer Science and Technology, V.V.P.I.E.T. Solapur, Maharashtra, India. ---------------------------------------------------------------------***--------------------------------------------------------------------Abstract Breast cancer is one of the most widely spreading diseases and the second leading cause of cancer death among women. [3].The survival rate increases on detecting breast cancer early as better treatment can be provided. Data classification using machine learning has been widely used in the diagnosis of breast cancer and for early detection of breast cancer. The aim of this literature review is to focus on the use of machine learning in classification of available data in breast cancer early detection and diagnosis. On reviewing several papers of artificial intelligence it is apparent that there are different techniques available for cancer detection. The objective of this study is to summarize various review and technical articles on diagnosis and prognosis of breast cancer. It gives an overview of the current research being carried out on various breast cancer datasets using the data mining techniques to enhance the breast cancer diagnosis and prognosis.\"}, {\"paperId\": \"08f7d942fee118de257acba94e1233ec65f4175c\", \"abstract\": \"Machine learning (ML) is a branch of artificial intelligence (AI) that enables computers to learn without being explicitly programmed, through a combination of statistics and computer science. It encompasses a variety of techniques used to analyse and interpret extremely large amounts of data, which can then be applied to create predictive models. Such applications of this technology are now ubiquitous in our day-to-day lives: predictive text, spam filtering, and recommendation systems in social media, streaming video and e-commerce to name a few examples. It is only more recently that ML has started to be implemented against the vast amount of data generated in healthcare. The emerging role of AI in refining healthcare delivery was recently highlighted in the \\u2018National Health Service Long Term Plan 2019\\u2019. In paediatrics, workforce challenges, rising healthcare attendance and increased patient complexity and comorbidity mean that demands on paediatric services are also growing. As healthcare moves into this digital age, this review considers the potential impact ML can have across all aspects of paediatric care from improving workforce efficiency and aiding clinical decision-making to precision medicine and drug development. Summary of the potential role of Artificial intelligence in management of childhood disease.\"}, {\"paperId\": \"046a661509397201dbe10229dc1a1a1103561710\", \"abstract\": \"Kim A. Nicoli, Christopher J. Anders, Lena Funcke, Tobias Hartung, 5 Karl Jansen, Pan Kessel, Shinichi Nakajima and Paolo Stornati Technische Universit\\u00e4t Berlin, Machine Learning Group, Marchstrasse 23, Berlin,10587, Germany Berlin Institute for the Foundation of Learning and Data (BIFOLD), Technische Universit\\u00e4t Berlin, Berlin, Germany Center for Theoretical Physics, Co-Design Center for Quantum Advantage, and NSF AI Institute for Artificial Intelligence and Fundamental Interactions, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA 02139, USA Perimeter Institute for Theoretical Physics, 31 Caroline Street North, Waterloo, ON N2L 2Y5, Canada Computation-Based Science and Technology Research Center, The Cyprus Institute, 20 Kavafi Street, 2121 Nicosia, Cyprus 5 Department of Mathematical Sciences University of Bath, Bath, United Kingdom Deutsches Elektronen-Synchrotron DESY, Platanenallee 6, 15738 Zeuthen, Germany RIKEN Center for AIP, 1-4-1 Nihonbashi, Chuo-ku, Tokyo, Japan ICFO, The Barcelona Institute of Science and Technology, Av. Carl Friedrich Gauss 3, 08860 Castelldefels (Barcelona), Spain E-mail: kim.a.nicoli@tu-berlin.de, anders@tu-berlin.de, lfuncke@mit.edu,\"}, {\"paperId\": \"ebe6dae4c3fc0465426b9f8b0ae4ba1280822982\", \"abstract\": \"With the advent of the big data era, artificial intelligence technology has penetrated and deeply affected our daily life. In addition, data\\u2010based machine learning algorithms have been applied to physics, chemistry, material science, and other basic science fields. However, the scarcity of data sets is known as the main obstacle to its development. Mining effective information from the limited data samples and building an appropriate machine learning algorithms framework are the major breakthroughs. For solid materials, the intrinsic properties are closely related to their atomic composition and relative positions, namely crystal structures. Here, inspired by the emerging of graph convolution neural network and material crystal graph, we proposed an integrated algorithms framework embedded crystal graph to train and predict the lattice thermal conductivities of crystal materials. This machine learning algorithms framework showed superior learning and generalization ability. In addition, not only in predicting thermal conductivities, but our framework also has great performance in predicting other phonon or electron\\u2010related properties. This strategy provided a new approach in the design of machine learning framework, which indicated the great potential for the application of machine learning in material science.\"}, {\"paperId\": \"6aa153c1a9ede7510bbfaf02a71cf04a9d2610c1\", \"abstract\": \"With the rapid development of information technology, English education has attracted the interest of scholars for its ability to rely on computers to analyze and understand human language. Taking machine learning (ML) as the core, this paper tries to develop an evaluation system for ML-based English education. Specifically, a basic ML model was established with four stages: environment, knowledge base, learning process, and implementation process. The entire ML system was divided from top to bottom into a user layer, a business layer, and a data layer. Application results show that, during the ML, even users with similar personal data and the same goal have vastly different suitable learning materials, due to their gap in personal capabilities. The research provides an effective way to evaluate English education in the context of computer science and artificial intelligence.\"}, {\"paperId\": \"dd69dc41f6ac8455d8b5a45d0b97c304ae922e3b\", \"abstract\": \"On the road toward artificial general intelligence (AGI), two solution paths have been explored: neuroscience-driven neuromorphic computing such as spiking neural networks (SNNs) and computer-science-driven machine learning such as artificial neural networks (ANNs). Owing to availability of data, high-performance processors, effective learning algorithms, and easy-to-use programming tools, ANNs have achieved tremendous breakthroughs in many intelligent applications. Recently, SNNs also attracted a lot of attention due to its biological plausibility and the possibility of achieving energy-efficiency (Roy et al., 2019). However, they suffer from ongoing debates and skepticisms due to worse accuracy compared to \\u201cstandard\\u201d ANNs. The performance gap comes from a variety of factors, including learning techniques, benchmarks, programming tools and execution hardware, all of which in SNNs are not as developed as those in the ANN domain. To this end, we propose a Research Topic, named \\u201cUnderstanding and Bridging the Gap between Neuromorphic Computing and Machine Learning,\\u201d in Frontiers in Neuroscience and Frontiers in Computational Neuroscience to collect recent researches on neuromorphic computing and machine learning to help understand and bridge the aforementioned gap. We received 18 submissions in total and accepted 14 of them in the end. The scope of these accepted papers covers learning algorithms, applications, and efficient hardware.\"}, {\"paperId\": \"b0c2e2723b7f74bc6125431f8b0bd8b48b41d400\", \"abstract\": \"Artificial intelligence is a spectacular part of computer engineering that has earned a compelling diversion in the field of medical data classification due to its state-of-art algorithmic strength and learning capabilities. Machine Learning is a major sub-domain of artificial intelligence, where it has become one of the most promising fields in computer science. In recent years, there is a large spectrum of healthcare and biomedical data that has been growing intensely. Due to the huge labeled or unlabeled data, it is important to have a compact and robust machine learning solution for classification. Several optimizers have been deployed to improve the inclusive performance of machine learning models. The classification of machine learning models depends on several factors. This comprehensive review paper aims to insight into the current stage of optimized machine learning success on medical data classification. An increasing number of unstructured medical data has been utilizing in machine learning algorithms to predict intuitions. But it is difficult to inherent immense intuition from those data. So machine learning researchers have utilized state-of-art optimizers and novel feature selection techniques to overcome and emend the performance accuracy. We have highlighted some recent literature, which exhibits the robust impact of optimizers and feature selection on machine learning techniques on medical data characterization. On the other hand, a clean-cut introduction on machine learning and theoretical outlook of widely utilized optimization techniques like genetic algorithm, gray wolf optimization, and particle swarm optimization are discussed for initial understanding of the optimization techniques.\"}, {\"paperId\": \"85b7f45d2798371d11811fbe4a5e844405188f1b\", \"abstract\": \"Machine leaning is a ground of recent research that officially focuses on the theory, performance, and properties of learning systems and algorithms. It is a extremely interdisciplinary field building upon ideas from many different kinds of fields such as artificial intelligence, optimization theory, information theory, statistics, cognitive science, optimal control, and many other disciplines of science, engineering, and mathematics. Because of its implementation in a wide range of applications, machine learning has covered almost every scientific domain, which has brought great impact on the science and society. It has been used on a variety of problems, including recommendation engines, recognition systems, informatics and data mining, and autonomous control systems. This research paper compared different machine algorithms for classification. Classification is used when the desired output is a discrete label.\"}, {\"paperId\": \"8702e8a17eed574c314d77c19a0ff2c2f26fdbb3\", \"abstract\": \"Machine learning (ML) has evolved rapidly over recent years with the promise to substantially alter and enhance the role of data science in a variety of disciplines. Compared with traditional approaches, ML offers advantages to handle complex problems, provide computational efficiency, propagate and treat uncertainties, and facilitate decision making. Also, the maturing of ML has led to significant advances in not only the main-stream artificial intelligence (AI) research but also other science and engineering fields, such as material science, bioengineering, construction management, and transportation engineering. This study conducts a comprehensive review of the progress and challenges of implementing ML in the earthquake engineering domain. A hierarchical attribute matrix is adopted to categorize the existing literature based on four traits identified in the field, such as ML method, topic area, data resource, and scale of analysis. The state-of-the-art review indicates to what extent ML has been applied in four topic areas of earthquake engineering, including seismic hazard analysis, system identification and damage detection, seismic fragility assessment, and structural control for earthquake mitigation. Moreover, research challenges and the associated future research needs are discussed, which include embracing the next generation of data sharing and sensor technologies, implementing more advanced ML techniques, and developing physics-guided ML models.\"}, {\"paperId\": \"a203cec35c167efc87f2ef83e088198e9d90e749\", \"abstract\": \"ABSTRACT How do we teach and learn with our students about data literacy, at the same time as Biesta (2015) calls for an emphasis on \\u2018subjectification\\u2019 i.e. \\u2018the coming into presence of unique individual beings\\u2019? (Good Education in an Age of Measurement: Ethics, Politics, Democracy. Routledge) Our response to these challenges and the datafication of higher education, is a hands-on approach to building an open, collaborative pedagogy of data literacy, based on Bayesian Networks (BNs) (Pearl, J. 1985. Bayesian Networks: A Model of Self\\u2013Activated Memory for Evidential Reasoning. Los Angeles: University of California (Computer Science Department)). BNs can be used to merge subjective views of the learning process with objective data analysis from the learning environment; BNs are visual data constructs and, unlike other Machine Learning approaches that obfuscate and complexify, BNs can be developed to reveal relationships from observations. In this paper, we share ways in which teachers and students can work together in a praxis approach to use data to \\u2018read the world\\u2019 around them (Freire, P. 1970. Pedagogy of the Oppressed. New York: Continuum. 125).\"}, {\"paperId\": \"af26348693321847450d78f862f5ddd8d137af3e\", \"abstract\": \"Artificial intelligence (AI) can be defined as the intelligence exhibited by machines and computers in accomplishing desired tasks in a similar way to how normal human beings think and act. Hence AI is also termed machine intelligence. For a computational system to be artificially intelligent, the system should possess the ability to understand the surrounding environment, make proper assumptions, and based on the circumstances make judicious decisions that maximize the possibilities of accomplishing goals most of the time. These AI\\u2010enabled devices are also called Intelligent Agents. These intelligent agents use some mapping functions also termed cognitive functions, which take these environmental parameters and contextual information as inputs along with the goal to be accomplished and manipulate the right means to accomplish the targeted goal. AI can also be considered inter\\u2010 disciplinary as it involves several other disciplines such as Machine Learning, Computer Vision, Cognitive Science, Neural Networks, Data Mining, Natural Language Processing (NLP), robotics, and mathematics. All these disciplines are related, and thereby intelligent agents are trained to understand and adapt to the surrounding environment according to the context. The use of AI spans across several applications such as Human\\u2013Computer Interaction (HCI) based smart agent development, devising smart surveillance solutions using computer vision, creating robust and stable decision making systems that can understand, evaluate, manipulate, analyze, and predict several novel patterns by processing large volumes of application data, development of multilingual systems that uses NLP to understand the language features used across the context and aid decision making and so on. Also, since its inception as an academic discipline in the 1950s, AI has grown leaps and bounds as a discipline, and its applications have stretched across several domains such as Retail and Business solutions, Manufacturing and Logistics, Automobiles, Business Analytics and Market predictions, Healthcare, Security Systems, and Education. One of the key emerging areas where extensive efforts are spent towards developing smart applications and agents is the educational domain. Gone are the days where the educational system was completely driven by humans, and the growth of AI\\u2010enabled intelligent agents has set the tone by replacing most human work with that of smart agents. Educational systems use AI\\u2010based agents to study the behavior of students and suggest suitable courses for them. Smart agents are nowadays deployed in classrooms for complete classroom monitoring that includes tracking attendance, monitoring classroom activities, student and staff behavior monitoring, and so on. Similarly, smart agents are deployed to scan through the contents available online and suggest suitable content to students according to the course and also according to the different levels of understanding of student fraternity. Also, computer vision\\u2010based smart agents are deployed to study the state of mind of students when they undergo different courses and provide insightful information about their likeness towards a subject or course. This agent\\u2010based information serves as useful information in deciding the teaching methodology and also framing of course contents. Also, smart systems play a vital role in analyzing student results and providing insightful information about student performance. Thus, it is imperative that AI has become an indispensable force to reckon with in the future forward across the educational domain. However, the major drawback in these artificially intelligent systems is that they are not always accurate with decision making and at times predict otherwise. Also, training the AI\\u2010based agent to understand the contextual paradigm and surrounding environment is a challenge. This special issue on \\u201cArtificial Intelligence In Education\\u201d is focused on drawing original studies related to the development and refinement of smart agents that can be applied across the educational domain.\"}, {\"paperId\": \"2d891ff211a0e9a7509613124e00290a7c2ba520\", \"abstract\": \"The application of artificial intelligence technology in higher education has study value and study space. The study mainly used core data collection of the Web of Science database based on SSCI and SCI journals and based on \\u201cartificial intelligence\\u201d and \\u201chigher education\\u201d these two subjects retrieval for 2009 to 2019. In addition, It used keyword co-occurrence function and cluster analysis function of CiteSpace Knowledge Graph software for visualization analysis and detailed analysis of study hotspots. The study elaborated on the application of artificial intelligence technology in higher education, and gained study hotspots such as machine learning, augmented reality, and learning management systems, and briefly discussed the impact of artificial intelligence on higher education.\"}, {\"paperId\": \"39e007fe8274adb38da55c11a492ae1be0121c7f\", \"abstract\": \"Machine learning (ML) utilises data and algorithms to simulate the way people learn and improve their accuracy over time and it\\u2019s also a subdivision of artificial intelligence (AI) and computer science. In AI, ML is a relatively recent domain that involves studying computational methods for discovering new knowledge and managing existing knowledge. Methods of machine learning have been applied to a diversity of application domains. . However, in recent years, as a result of various technological advancements and research efforts, new data has become available, resulting in new domains in which machine learning can be applied. This paper introduces the definition of machine learning and its basic structure. These algorithms are used for various purposes, including data mining, image processing, predictive analytics, and so on. The primary benefit of using machine learning is that once an algorithm learns what to do with data, it can do so automatically. This survey replenishes a brief outline and outlook on numerous machine learning applications.\"}, {\"paperId\": \"9b51193fc3239d6fa374c5029e323b6b21249ccb\", \"abstract\": \"Background The scope and productivity of artificial intelligence applications in health science and medicine, particularly in medical imaging, are rapidly progressing, with relatively recent developments in big data and deep learning and increasingly powerful computer algorithms. Accordingly, there are a number of opportunities and challenges for the radiological community. Purpose To provide review on the challenges and barriers experienced in diagnostic radiology on the basis of the key clinical applications of machine learning techniques. Material and Methods Studies published in 2010\\u20132019 were selected that report on the efficacy of machine learning models. A single contingency table was selected for each study to report the highest accuracy of radiology professionals and machine learning algorithms, and a meta-analysis of studies was conducted based on contingency tables. Results The specificity for all the deep learning models ranged from 39% to 100%, whereas sensitivity ranged from 85% to 100%. The pooled sensitivity and specificity were 89% and 85% for the deep learning algorithms for detecting abnormalities compared to 75% and 91% for radiology experts, respectively. The pooled specificity and sensitivity for comparison between radiology professionals and deep learning algorithms were 91% and 81% for deep learning models and 85% and 73% for radiology professionals (p\\u2009<\\u20090.000), respectively. The pooled sensitivity detection was 82% for health-care professionals and 83% for deep learning algorithms (p\\u2009<\\u20090.005). Conclusion Radiomic information extracted through machine learning programs form images that may not be discernible through visual examination, thus may improve the prognostic and diagnostic value of data sets.\"}, {\"paperId\": \"f176c0004601aecbead05aacfc233319bb350334\", \"abstract\": \"With the search for a smarter, faster, and technological ways of getting things accomplished, Artificial Intelligence (AI) is developing at a faster pace. The technology has become a part of daily life, where the blend of human intelligence and machine learning has reached heights in various fields of science and technology. The machine simulates the human intelligence and improves their abilities with the help of self-adapting algorithms. Artificial intelligence has provided many benefits in various fields, particularly in medicine, where it plays a major role in the advancement of the medical field, ranging from virtual assistants to creating a better diagnosis and treatment using accumulated patient data. In orthodontics, the treatment focuses on altering the occlusion, controlling the development of dentoalveolar components and growth abnormalities. An effective assessment of these problems enables in determining the need for treatment and to prioritize it. Precise diagnosis, offering relevant and complete information is a key to a successful practice in orthodontics. Of late artificial intelligence is applied in orthodontics in decision making and planning effective treatment outcomes. Artificial intelligence is useful in simulation of various clinical scenarios in the three-essential sequence - diagnosis, treatment planning and treatment, which is efficient enough in reducing the workload, time and also increases the accuracy and monitoring. In no ways artificial intelligence can replace the dentist because clinical practice is not just about the diagnosis and treatment plan. So, humans should have a basic understanding on artificial intelligence models to assist in clinical judgement and not to replace the knowledge and expertise of humans. KEY WORDS Artificial intelligence, Machine Learning, Artificial Neural Network, Orthodontics, Review\"}, {\"paperId\": \"628cffa778fc88c306844c2464c740e53b38c70c\", \"abstract\": \"The rapid growth of healthcare data in recent years calls for more advanced and efficient analytic techniques. Artificial intelligence facilitates finding insightful patterns in massive high-dimensional data. Considering the latest movements towards using machine learning and deep learning techniques in the medical domain, in this study, we focused on the publications in which researchers employed artificial intelligence techniques for cancer diagnosis and treatment. Using dynamic topic modeling and natural language processing techniques, we analyzed the contents and trends of more than 12,000 scientific publications within the period of 2000 to 2018, extracted from two different sources, i.e., Elsevier\\u2019s Scopus and PubMed. While drawing the landscape of cancer research, our results also shed light on the evolution of artificial intelligence techniques and algorithms used for cancer diagnosis and treatment. Our findings confirm that modern computer science algorithms are being widely applied to extract patterns from large-scale medical images to cure different types of cancer with a special focus on deep learning techniques in recent years.\"}, {\"paperId\": \"40612d5f49e1785f97eb7e6c3618aa324e292a9e\", \"abstract\": \"Artificial Intelligence (AI) is deemed to be the commanding point of science and technology in the next era. In recent years, with the enhancement of computer computing power, the improvement of the quantity and quality of big data, and the important breakthroughs in many research fields such as machine learning and speech recognition, AI technology has developed rapidly and has been widely used in all walks of life. In the financial industry, the application of AI technology in risk control, marketing, customer service, transaction, operation, and product optimization of financial institutions is becoming increasingly mature, and some new business models have been created. Starting from the application status and significance of AI in the international financial field, this paper expounds on the application, status quo, and development trend of AI in the financial industry. Then, in view of the risks and practical challenges existing in the development process of AI, based on the reality of international financial development, this paper summarizes the measures to promote the in-depth, healthy, and sustainable development of AI in the financial market. This paper aims to let readers understand the development status of AI in the financial field, and also provide theoretical reference for scholars in this field.\"}, {\"paperId\": \"00ab9f3b05d49833fabc7a3d1f371ed2a4361ce8\", \"abstract\": null}, {\"paperId\": \"f95b46ded837a8f8a007e411dc07aff1e05f00fc\", \"abstract\": \"Department of Computer Science, Capital University of Science and Technology (CUST) Islamabad Expressway, Kahuta Road Zone-V, Islamabad, Pakistan School of Quantitative Sciences, UUM College of Arts and Sciences, Universiti Utara Malaysia, 06010 UUM Sintok, Kedah, Malaysia Institute for Artificial Intelligence and Big Data(AIBIG), Universiti Malaysia Kelantan, City Campus, 16100 Kota Bharu, Kelantan, Malaysia Corresponding author: *nooraini.y@umk.edu.my\"}, {\"paperId\": \"68f691a444ddf3914f785f03765790f95cce7ab0\", \"abstract\": null}, {\"paperId\": \"2333016ded3dd7ff4f06ad0d7b0139e34559c4b0\", \"abstract\": \"Artificial intelligence can optimize cancer drug discovery, development, and administration Artificial intelligence (AI) approaches have the potential to affect several facets of cancer therapy. These include drug discovery and development and how these drugs are clinically validated and ultimately administered at the point of care, among others. Currently, these processes are expensive and time-consuming. Moreover, therapies often result in variable treatment outcomes between patients. The convergence of AI and cancer therapy has resulted in multiple solutions to address these challenges. AI platforms ranging from machine learning to neural networks can accelerate drug discovery, harness biomarkers to accurately match patients to clinical trials, and truly personalize cancer therapy using only a patient's own data. These advances are indicators that practice-changing cancer therapy empowered by AI may be on the horizon.\"}, {\"paperId\": \"aebae3ecc62aad144a39f6315251c5b688242077\", \"abstract\": \"Innovations in Machine Learning and Data Analytics can possibly affect numerous aspects of Environmental Science (ES). Data Analytics refers to a collection of data resources indicated in terms of variety, velocity, veracity and volume. Big data contributes to the ES arena in applications such as weather forecasting, energy sustainability and disaster management with the advent of techniques such as Remote Sensing, Information and Communication technologies. Though big data is used to accomplish data analysis and interpretation for ES, there are still requirements for efficient ways of data storage, processing and retrieval. Machine Learning and Deep Learning are the sub fields of artificial intelligence which deals with training the models to learn from data without being explicitly programmed. When Machine Learning and Deep Learning are combined together it is possible to unleash the supremacy of data analytics. These techniques show high prospective for process optimization, information-centric decision making and scientific discovery. Scientific developments like these will assist ES to make real time autonomous decisions by extracting useful insights from huge data. These advancements also aid in bridging the gap between the theoretical backgrounds on ES to practical implementation. The primary objective of this survey is to figure out the basic concepts of Machine Learning, Deep Learning, and Data Analytics and find the state-of-the-art applications in ES, and observe the impending benefits of information-centric investigation on ES.\"}, {\"paperId\": \"f86f1748d1b6d22870f4347fd5d65314ba800583\", \"abstract\": \"Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias\\u2013variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias\\u2013variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This \\u201cdouble-descent\\u201d curve subsumes the textbook U-shaped bias\\u2013variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.\"}, {\"paperId\": \"e09c5bafc369baae8089a1d49db173b7c6a1273f\", \"abstract\": \"Individually and collectively, copyrighted works have the potential to generate information that goes far beyond what their individual authors expressed or intended. Various methods of computational and statistical analysis of text \\u2014 usually referred to as text data mining (\\u201cTDM\\u201d) or just text mining \\u2014 can unlock that information. However, because almost every use of TDM involves making copies of the text to be mined, the legality of that copying has become a fraught issue in copyright law in United States and around the world. One of the most fundamental questions for copyright law in the Internet age is whether the protection of the author\\u2019s original expression should stand as an obstacle to the generation of insights about that expression. How this question is answered will have a profound influence on the future of research across the sciences and the humanities, and for the development of the next generation of information technology: machine learning and artificial intelligence. \\n \\nThis Article consolidates a theory of copyright law should that I have advanced in a series of articles and amicus briefs over the past decade. It explains why applying copyright\\u2019s fundamental principles in the context of new technologies necessarily implies that copying expressive works for non-expressive purposes should not be counted as infringement and must be recognized as fair use. The Article shows how that theory was adopted and applied in the recent high-profile test cases, Authors Guild v. HathiTrust and Authors Guild v. Google, and takes stock of the legal context for TDM research in the United States in the aftermath of those decisions. \\n \\nThe Article makes important contributions to copyright theory, but is also integrates that theory with a practical assessment various interrelated legal issues that text mining researchers and their supporting institutions must confront if they are to realize the full potential of these technologies. These issues range from the enforceability of website terms of service, the effect of laws prohibiting computer hacking and the circumvention of technological protection measures (i.e., encryption and other digital locks), and cross-border copyright issues.\"}, {\"paperId\": \"0e4a356b5114d7fc4303495002c9b52a631b608e\", \"abstract\": null}, {\"paperId\": \"6e23398447a022fb9495c44fa80e9de593a574bc\", \"abstract\": \"Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.\"}, {\"paperId\": \"15964b16c48834df9de0ffda56561c8755c67fed\", \"abstract\": null}, {\"paperId\": \"928564b5756095b02563826dc5579c582ec3d5f6\", \"abstract\": \"Abstract\\u2014 Machine learning is a branch of artificial intelligence science i.e. the systems that can learn data. For example, a machine learning system can learn e-mail receiving and distinguish the difference between spam and non-spam message from each other. After training, the system can put new messages in their folders using classification. Currently, we do not know how to program computers in order to human learn more efficient. Although the methods that have been discovered operate very effectively for certain purposes, not suitable for all purposes. For example, machine learning algorithms are commonly used in data mining. Even in areas where data are concerned, these algorithms operate and result much better than other methods. For example, in issues such as speech recognition, algorithms based on machine learning resulted much better than the other methods. Apparently, it seems that our knowledge of computers will improve gradually. Certainly, it can be said that the topic of machine learning play a highly significant role in the field of computer science and game technology. This paper describes machine learning algorithms, feature selection methods, dimensions reduction, and deleting of useless data.\"}, {\"paperId\": \"bc386debfedf3b16101b6c3274485cea78ad6bb7\", \"abstract\": \"Precision medicine is an emerging approach to clinical research and patient care that focuses on understanding and treating disease by integrating multimodal or 'multi-omics' data from an individual to make patient-tailored decisions. With the large and complex datasets generated using precision medicine diagnostic approaches, novel techniques to process and understand these complex data were needed. At the same time, computer science has progressed rapidly to develop techniques that enable the storage, processing, and analysis of these complex datasets, a feat that traditional statistics and early computing technologies could not accomplish. Machine learning, a branch of artificial intelligence, is a computer science methodology that aims to identify complex patterns in data that can be used to make predictions or classifications on new unseen data or for advanced exploratory data analysis. Machine learning analysis of precision medicine's multimodal data allows for broad analysis of large datasets and ultimately a greater understanding of human health and disease. This review focuses on machine learning utilization for precision medicine's \\\"big data\\\", in the context of genetics, genomics, and beyond.\"}, {\"paperId\": \"1f133b018af44652578b5ba498e722f66974503b\", \"abstract\": \"\\n Machine learning, a subfield of artificial intelligence, offers various methods that can be applied in marine science. It supports data-driven learning, which can result in automated decision making of de novo data. It has significant advantages compared with manual analyses that are labour intensive and require considerable time. Machine learning approaches have great potential to improve the quality and extent of marine research by identifying latent patterns and hidden trends, particularly in large datasets that are intractable using other approaches. New sensor technology supports collection of large amounts of data from the marine environment. The rapidly developing machine learning subfield known as deep learning\\u2014which applies algorithms (artificial neural networks) inspired by the structure and function of the brain\\u2014is able to solve very complex problems by processing big datasets in a short time, sometimes achieving better performance than human experts. Given the opportunities that machine learning can provide, its integration into marine science and marine resource management is inevitable. The purpose of this themed set of articles is to provide as wide a selection as possible of case studies that demonstrate the applications, utility, and promise of machine learning in marine science. We also provide a forward-look by envisioning a marine science of the future into which machine learning has been fully incorporated.\"}, {\"paperId\": \"8b7a419ba0ba9ac37b037d6e73ce6b1b69e7c174\", \"abstract\": \"As an interdisciplinary research approach, traditional cognitive science adopts mainly the experiment, induction, modeling, and validation paradigm. Such models are sometimes not applicable in cyber-physical-social-systems (CPSSs), where the large number of human users involves severe heterogeneity and dynamics. To reduce the decision-making conflicts between people and machines in human-centered systems, we propose a new research paradigm called parallel cognition that uses the system of intelligent techniques to investigate cognitive activities and functionals in three stages: descriptive cognition based on artificial cognitive systems (ACSs), predictive cognition with computational deliberation experiments, and prescriptive cognition via parallel behavioral prescription. To make iteration of these stages constantly on-line, a hybrid learning method based on both a psychological model and user behavioral data is further proposed to adaptively learn an individual\\u2019s cognitive knowledge. Preliminary experiments on two representative scenarios, urban travel behavioral prescription and cognitive visual reasoning, indicate that our parallel cognition learning is effective and feasible for human behavioral prescription, and can thus facilitate human-machine cooperation in both complex engineering and social systems.\"}, {\"paperId\": \"2f78e27d8979060fb0c8fb63959b891775f8054e\", \"abstract\": null}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 200, \"next\": 300, \"data\": [{\"paperId\": \"bbe4f6f523a3d1875298e669846727f61ed1a466\", \"abstract\": null}, {\"paperId\": \"f39c96f4f5d473247c3ac8a8a20bdb7073989236\", \"abstract\": \"Machine learning is a branch of artificial intelligence, and it has been widely used in many science and engineering areas, such as data mining, natural language processing, computer vision, biological analysis and so on. Quantum computer is considered as one of the most promising technologies of human beings in the near future. With the development of machine learning and quantum computing, researchers consider to combine these two aspects to gain more benefits. As a result, a novel interdisciplinary subject has emerged\\u2014quantum machine learning. This paper reviews the state-of-the-art research of algorithms of quantummachine learning and shows a path of the research from the basic quantum information to quantum machine learning algorithms from the perspective of people in the field of computer science.\"}, {\"paperId\": \"da02c0a44591b07c0dcae88249e396b30961f21a\", \"abstract\": null}, {\"paperId\": \"79417cd08b28ec97c11e760598226fadc938c534\", \"abstract\": \"Fraud exists in all walks of life and detecting and preventing fraud represents an important research question relevant to many stakeholders in society. With the rise of big data and artificial intelligence, new opportunities have arisen in using advanced machine learning models to detect fraud. This chapter provides a comprehensive overview of the challenges in detecting fraud using machine learning. We use a framework (data, method, and evaluation criterion) to review some of the practical considerations that may affect the implementation of ma-chine-learning models to predict fraud. Then, we review select papers in the academic literature across different disciplines that can help address some of the fraud detection challenges. Finally, we suggest promising future directions for this line of research. As accounting fraud constitutes an important class of fraud, we will discuss all of these issues within the context of accounting fraud detection.\"}, {\"paperId\": \"47a148e508c43db0c1faeb7513f9ae07eee3f4d2\", \"abstract\": \"Machine learning (i.e., data mining, artificial intelligence, big data) has been increasingly applied in psychological science. Although some areas of research have benefited tremendously from a new set of statistical tools, most often in the use of biological or genetic variables, the hype has not been substantiated in more traditional areas of research. We argue that this phenomenon results from measurement errors that prevent machine-learning algorithms from accurately modeling nonlinear relationships, if indeed they exist. This shortcoming is showcased across a set of simulated examples, demonstrating that model selection between a machine-learning algorithm and regression depends on the measurement quality, regardless of sample size. We conclude with a set of recommendations and a discussion of ways to better integrate machine learning with statistics as traditionally practiced in psychological science.\"}, {\"paperId\": \"1276155f1fb8c7ce76e921042d19f3a0ffa353d0\", \"abstract\": \"Background: the COVID-19 outbreak has created a great challenge for the healthcare system worldwide. One of the most critical points of this challenge is the management of COVID-19 patients needing acute and/or critical respiratory care. This study was performed to discover an AI based model to improve the critical care of the COVID-19 patients.Material and methods: in a descriptive study, all the published research available in PubMed, Web of Science, Google scholar and other databases were retrieved. Based on these studies, a three stage model of input, process and output was created.Results: the three stage model of AI application in ICU was completed. Input included Clinical, Paraclinical, Personalized Medicine (OMICS) and Epidemiologic data. The process included Artificial Intelligence (i.e. Artificial Neural Network, Machine Learning, Deep Learning and Expert Systems). The output which was ICU Decision Making included Diagnosis, Treatment, Risk Stratification, Prognosis and Management.Conclusion: the efforts of the healthcare system to defeat COVID-19 could be supported by an AI-based decision-making system which would double them up and help manage these patients much more efficiently, especially those in COVID-19 ICU\"}, {\"paperId\": \"ba1fa8faac6ff937f1dd1d8e45b50257b06ac90b\", \"abstract\": \"Artificial intelligence and Big Data are articulated to be able to deal with different problems related to the analysis of big data, in particular, information from the COVID-19 In this sense, this article shows some research projects related to deep learning, machine learning, Big Data and data science, aimed to provide plausible solutions in monitoring, detection, diagnosis and treatment of diseases associated with the virus The correspondence between disruptive technologies and critical information is shown, creating synergies that allow the development of more advanced systems of study and analysis, facilitating the obtaining of relevant data for health decision-making Copyright \\u00a9 2020 Jairo Marquez Diaz\"}, {\"paperId\": \"55b9abf5a22ad4c5206121a9b2a5c5e63a7de0e6\", \"abstract\": null}, {\"paperId\": \"831ac51b66fbc8273e30a5300bbed2a97504cbc6\", \"abstract\": \"Abstract Background Artificial Intelligence (AI) is transforming the process of scientific research. AI, coupled with availability of large datasets and increasing computational power, is accelerating progress in areas such as genetics, climate change and astronomy [NeurIPS 2019 Workshop Tackling Climate Change with Machine Learning, Vancouver, Canada; Hausen R, Robertson BE. Morpheus: A deep learning framework for the pixel-level analysis of astronomical image data. Astrophys J Suppl Ser. 2020;248:20; Dias R, Torkamani A. AI in clinical and genomic diagnostics. Genome Med. 2019;11:70.]. The application of AI in behavioral science is still in its infancy and realizing the promise of AI requires adapting current practices. Purposes By using AI to synthesize and interpret behavior change intervention evaluation report findings at a scale beyond human capability, the HBCP seeks to improve the efficiency and effectiveness of research activities. We explore challenges facing AI adoption in behavioral science through the lens of lessons learned during the Human Behaviour-Change Project (HBCP). Methods The project used an iterative cycle of development and testing of AI algorithms. Using a corpus of published research reports of randomized controlled trials of behavioral interventions, behavioral science experts annotated occurrences of interventions and outcomes. AI algorithms were trained to recognize natural language patterns associated with interventions and outcomes from the expert human annotations. Once trained, the AI algorithms were used to predict outcomes for interventions that were checked by behavioral scientists. Results Intervention reports contain many items of information needing to be extracted and these are expressed in hugely variable and idiosyncratic language used in research reports to convey information makes developing algorithms to extract all the information with near perfect accuracy impractical. However, statistical matching algorithms combined with advanced machine learning approaches created reasonably accurate outcome predictions from incomplete data. Conclusions AI holds promise for achieving the goal of predicting outcomes of behavior change interventions, based on information that is automatically extracted from intervention evaluation reports. This information can be used to train knowledge systems using machine learning and reasoning algorithms.\"}, {\"paperId\": \"9a0268a0b0f8f043b19282aacc095abd466c8b12\", \"abstract\": \"Abstract The way to analyze data in spectroscopy has changed substantially. At the same time, data science has evolved to the point where spectroscopy can find space to be housed, adapted and be functional. The integration of the two sciences has introduced a knowledge gap between data scientists who know about advanced machine learning techniques and spectroscopists who have a solid background in chemometrics. To reach a symbiosis, the knowledge gap requires bridging. This review article focuses on introducing data science subjects to non-specialist spectroscopists, or those unfamiliar with the subject. The article will explain concepts that are covered in machine learning, such as supervised learning, unsupervised learning, deep learning, and most importantly, the difference between machine learning and artificial intelligence. This article also includes examples of published spectroscopy research, in which some of the concepts explained here are applied. Machine learning together with spectroscopy can provide a useful, fast, and efficient tool to analyze samples of interest both for industrial and research purposes.\"}, {\"paperId\": \"34a54359ac89635dcbe9503147f6a6bf097f4732\", \"abstract\": \"To solve many problems in data science, Machine Learning (ML) techniques implicates artificial intelligence which are commonly used. The major utilization of ML is to predict the conclusion established on the extant data. Using an established dataset machine determine emulate and spread them to an unfamiliar data sets to anticipate the conclusion. A few classification algorithm\\u2019s accuracy prediction is satisfactory, although other perform limited accuracy. Different ML and Deep Learning (DL) networks established on ANN have been extensively recommended for the disclosure of heart disease in antecedent researches. In this paper, we used UCI Heart Disease dataset to test ML techniques along with conventional methods (i.e. random forest, support vector machine, K-nearest neighbor), as well as deep learning models (i.e. long short-term-memory and gated-recurrent unit neural networks). To improve the accuracy of weak algorithms we explore voting based model by combining multiple classifiers. A provisional cogent approach was used to regulate how the ensemble technique can be enforced to improve an accuracy in the heart disease prediction. The strength of the proposed ensemble approach such as voting based model is compelling in improving the prognosis accuracy of anemic classifiers and established adequate achievement in analyze risk of heart disease. A superlative increase of 2.1% accuracy for anemic classifiers was attained with the help of an ensemble voting based model.\"}, {\"paperId\": \"824eff68781e780f52677c974a54388c74985088\", \"abstract\": null}, {\"paperId\": \"4828d37ed444649435676b2503d174d7545d031a\", \"abstract\": \"Remote sensing data proved to be a valuable resource in a variety of earth science applications. Using high-dimensional data with advanced methods such as machine learning algorithms (MLAs), a sub-domain of artificial intelligence, enhances lithological mapping by spectral classification. Support vector machines (SVM) are one of the most popular MLAs with the ability to define non-linear decision boundaries in high-dimensional feature space by solving a quadratic optimization problem. This paper describes a supervised classification method considering SVM for lithological mapping in the region of Souk Arbaa Sahel belonging to the Sidi Ifni inlier, located in southern Morocco (Western Anti-Atlas). The aims of this study were (1) to refine the existing lithological map of this region, and (2) to evaluate and study the performance of the SVM approach by using combined spectral features of Landsat 8 OLI with digital elevation model (DEM) geomorphometric attributes of ALOS/PALSAR data. We performed an SVM classification method to allow the joint use of geomorphometric features and multispectral data of Landsat 8 OLI. The results indicated an overall classification accuracy of 85%. From the results obtained, we can conclude that the classification approach produced an image containing lithological units which easily identified formations such as silt, alluvium, limestone, dolomite, conglomerate, sandstone, rhyolite, andesite, granodiorite, quartzite, lutite, and ignimbrite, coinciding with those already existing on the published geological map. This result confirms the ability of SVM as a supervised learning algorithm for lithological mapping purposes.\"}, {\"paperId\": \"4fbeb6a4e22fc150c50c68a10bd2af11507a470a\", \"abstract\": \"Machine learning and deep learning techniques are contributing much to the advancement of science. Their powerful predictive capabilities appear in numerous disciplines, including chaotic dynamics, but they miss understanding. The main thesis here is that prediction and understanding are two very different and important ideas that should guide us to follow the progress of science. Furthermore, the important role played by nonlinear dynamical systems is emphasized for the process of understanding. The path of the future of science will be marked by a constructive dialogue between big data and big theory, without which we cannot understand.\"}, {\"paperId\": \"60dc19eeb94b8da364508705e1c68a3583a815f0\", \"abstract\": null}, {\"paperId\": \"64d125b83777cf181a731b3667833847063338e5\", \"abstract\": \"Nanoindentation was utilized as a non-destructive technique to identify Portland Cement hydration phases. Artificial Intelligence (AI) and semi-supervised Machine Learning (ML) were used for knowledge gain on the effect of carbon nanotubes to nanomechanics in novel cement formulations. Data labelling is performed with unsupervised ML with k-means clustering. Supervised ML classification is used in order to predict the hydration products composition and 97.6% accuracy was achieved. Analysis included multiple nanoindentation raw data variables, and required less time to execute than conventional single component probability density analysis (PDA). Also, PDA was less informative than ML regarding information exchange and re-usability of input in design predictions. In principle, ML is the appropriate science for predictive modeling, such as cement phase identification and facilitates the acquisition of precise results. This study introduces unbiased structure-property relations with ML to monitor cement durability based on cement phases nanomechanics compared to PDA, which offers a solution based on local optima of a multidimensional space solution. Evaluation of nanomaterials inclusion in composite reinforcement using semi-supervised ML was proved feasible. This methodology is expected to contribute to design informatics due to the high prediction metrics, which holds promise for the transfer learning potential of these models for studying other novel cement formulations.\"}, {\"paperId\": \"6cb9d0a79c00aa1f053c267eec7a9df70047f03e\", \"abstract\": \"The developed framework apportions model error to inputs, computes predictive guarantees, and enables model correctability. Data science has primarily focused on big data, but for many physics, chemistry, and engineering applications, data are often small, correlated and, thus, low dimensional, and sourced from both computations and experiments with various levels of noise. Typical statistics and machine learning methods do not work for these cases. Expert knowledge is essential, but a systematic framework for incorporating it into physics-based models under uncertainty is lacking. Here, we develop a mathematical and computational framework for probabilistic artificial intelligence (AI)\\u2013based predictive modeling combining data, expert knowledge, multiscale models, and information theory through uncertainty quantification and probabilistic graphical models (PGMs). We apply PGMs to chemistry specifically and develop predictive guarantees for PGMs generally. Our proposed framework, combining AI and uncertainty quantification, provides explainable results leading to correctable and, eventually, trustworthy models. The proposed framework is demonstrated on a microkinetic model of the oxygen reduction reaction.\"}, {\"paperId\": \"9a54565f6da16e1e5cc2c63d29e165d43da9f5f8\", \"abstract\": \"Artificial intelligence (AI) is a relatively new branch of computer science involving many disciplines and technologies, including robotics, speech recognition, natural language and image recognition or processing, and machine learning. Recently, AI has been widely applied in the medical field. The effective combination of AI and big data can provide convenient and efficient medical services for patients. Colorectal cancer (CRC) is a common type of gastrointestinal cancer. The early diagnosis and treatment of CRC are key factors affecting its prognosis. This review summarizes the research progress and clinical application value of AI in the investigation, early diagnosis, treatment, and prognosis of CRC, to provide a comprehensive theoretical basis for AI as a promising diagnostic and treatment tool for CRC.\"}, {\"paperId\": \"6d7434f9d76bc38cabefcd888fd7b847a8afe426\", \"abstract\": \"The development in science and technical intelligence has incited to represent an extensive amount ofdata from various fields of agriculture. Therefore an objective rises up for the examination of the available data and integrating with processes like crop enhancement, yield prediction, examination of plant infections etc. Machine learning has up surged with tremendous processing techniques to perceive new contingencies in the multi-disciplinary agrarian advancements. In this pa- per a novel hybrid regression algorithm, reinforced extreme gradient boosting is proposed which displays essentially improved execution over traditional machine learning algorithms like artificial neural networks, deep Q-Network, gradient boosting, ran- dom forest and decision tree. Extreme gradient boosting constructs new models, which are essentially, decision trees learning from the mistakes of their predecessors by optimizing the gradient descent loss function. The proposed hybrid model performs reinforcement learning at every node during the node splitting process of the decision tree construction. This leads to effective utilizationofthesamplesbyselectingtheappropriatesplitattributeforenhancedperformance. Model\\u2019sperformanceisevaluated by means of Mean Square Error, Root Mean Square Error, Mean Absolute Error, and Coefficient of Determination. To assure a fair assessment of the results, the model assessment is performed on both training and test dataset. The regression diagnostic plots from residuals and the results obtained evidently delineates the fact that proposed hybrid approach performs better with reduced error measure and improved accuracy of 94.15% over the other machine learning algorithms. Also the performance of probability density function for the proposed model delineates that, it can preserve the actual distributional characteristics of the original crop yield data more approximately when compared to the other experimented machine learning models.\"}, {\"paperId\": \"0337ba3cb773560817c7b2d459368bcf4dfa334c\", \"abstract\": \"Machine learning techniques are widely used nowadays in the healthcare domain for the diagnosis, prognosis, and treatment of diseases. These techniques have applications in the field of hematopoietic cell transplantation (HCT), which is a potentially curative therapy for hematological malignancies. Herein, a systematic review of the application of machine learning (ML) techniques in the HCT setting was conducted. We examined the type of data streams included, specific ML techniques used, and type of clinical outcomes measured. A systematic review of English articles using PubMed, Scopus, Web of Science, and IEEE Xplore databases was performed. Search terms included \\u201chematopoietic cell transplantation (HCT),\\u201d \\u201cautologous HCT,\\u201d \\u201callogeneic HCT,\\u201d \\u201cmachine learning,\\u201d and \\u201cartificial intelligence.\\u201d Only full-text studies reported between January 2015 and July 2020 were included. Data were extracted by two authors using predefined data fields. Following PRISMA guidelines, a total of 242 studies were identified, of which 27 studies met the inclusion criteria. These studies were sub-categorized into three broad topics and the type of ML techniques used included ensemble learning (63%), regression (44%), Bayesian learning (30%), and support vector machine (30%). The majority of studies examined models to predict HCT outcomes (e.g., survival, relapse, graft-versus-host disease). Clinical and genetic data were the most commonly used predictors in the modeling process. Overall, this review provided a systematic review of ML techniques applied in the context of HCT. The evidence is not sufficiently robust to determine the optimal ML technique to use in the HCT setting and/or what minimal data variables are required.\"}, {\"paperId\": \"b74d80faa4e4c95e44523d49f238dc40514d8b7c\", \"abstract\": \"Artificial intelligence (AI) is a branch of computer science and a technology aimed at developing the theories, methods, algorithms, and applications for simulating and extending human intelligence. Modern AI enables going from an old world-where people give computers rules to solve problems-to a new world-where people give computers problems directly and the machines learn how to solve them on their own using a set of algorithms. An algorithm is a self-contained sequence of instructions and actions to be performed by a computational machine. Starting from an initial state and initial input, the instructions describe computational steps, which, when executed, proceed through a finite number of well-defined successive states, eventually producing an output and terminating at a final ending state. AI algorithms are a rich set of algorithms used to perform AI tasks, notably those pertaining to perception and cognition that involve learning from data and experiences simulating human intelligence.\"}, {\"paperId\": \"7af22e4586bbaf8c036c445ce921aa096045103e\", \"abstract\": \"We live in an era with unprecedented availability of clinical and biological data that include electronic health records, wearable sensors, biomedical imaging and multiomics. The scale, complexity and rate at which such data are collected require innovative approaches to statistics and computer science that draw on the rapid advances in artificial intelligence (AI) for efficiently identifying actionable insights into disease processes. A basic understanding of AI\\u2019s strengths, applications and limitations is now essential for researchers and clinical cardiologists.\\n\\nIn this context, AI refers to a collection of computational concepts that can be summarised as a machine\\u2019s ability to generalise learning in order to efficiently achieve complex tasks autonomously. Machine learning (ML) achieves this by using algorithms to improve task performance without needing to be explicitly programmed and can be broadly divided into supervised and unsupervised approaches. In supervised learning, the mapping between paired input and output variables is iteratively optimised for use in regression and classification tasks. In unsupervised learning, only input data are available and algorithms are used to find inherent clusters or associations. In recent years, ML has become dominated by deep learning (DL), which is a methodology using multilayer neural networks to progressively obtain more abstract representations of complex data. Figure 1 provides a high-level schematic of the field of AI.\\n\\n\\n\\nFigure 1 \\nArtificial Intelligence through time.\\n\\n\\n\\nA DL algorithm consists of three types of layer: an input layer, hidden layers \\u2026\"}, {\"paperId\": \"b05de573c8b334c561cf4a4cb853422a4c532568\", \"abstract\": null}, {\"paperId\": \"ce83c09b90af8ef994418122772680539b1d7ca4\", \"abstract\": \"Artificial intelligence is a broad branch of computer science that has garnered significant interest in the field of medicine because of its problem solving, decision making and pattern recognition abilities. Machine learning, a subset of artificial intelligence, hones in on the ability of computers to receive data and learn for themselves, manipulating algorithms as they organize the information they are processing. Dermatology is at a particular advantage in the implementation of machine learning due to the availability of large clinical image databases that can be used for machine training and interpretation. While numerous studies have implemented machine learning in the diagnostic aspect of dermatology, less research has been conducted on the use of machine learning in predicting long-term outcomes in skin disease, with only a few studies published to date. Such an approach would assist physicians in selecting the best treatment methods, save patients' time, reduce treatment costs and improve the quality of treatment overall by reducing the amount of trial-and-error in the treatment process. In this review, we aim to provide a brief and relevant introduction to basic artificial intelligence processes, and to consolidate and examine the published literature on the use of machine learning in predicting clinical outcomes in dermatology.\"}, {\"paperId\": \"f61bf902ff747c1c47a4444648c1f567093d837f\", \"abstract\": \"Basic ideas and formal concepts from fuzzy sets and fuzzy logic have been used successfully in various branches of science and engineering. This paper elaborates on the use of fuzzy sets in the broad field of data analysis and statistical sciences, including modern manifestations such as data mining and machine learning. In the fuzzy logic community, this branch of research has recently gained in importance, especially due to the emergence of data science as a new scientific discipline, and the increasing relevance of machine learning as a key methodology of modern artificial intelligence. This development has been accompanied by an internal shift from largely knowledge-based to strongly data-driven fuzzy modeling and systems design. Reflecting on the historical dimension and evolution of the area, we discuss the role of fuzzy logic in data analysis and related fields, highlight existing contributions of fuzzy sets in these fields, and outline interesting directions for future work.\"}, {\"paperId\": \"53a70551a754b6a5018c40dc971cdf5cc58e159b\", \"abstract\": \"Data-driven science and its corollaries in machine learning and the wider field of artificial intelligence have the potential to drive important changes in medicine. However, medicine is not a science like any other: It is deeply and tightly bound with a large and wide network of legal, ethical, regulatory, economical, and societal dependencies. As a consequence, the scientific and technological progresses in handling information and its further processing and cross-linking for decision support and predictive systems must be accompanied by parallel changes in the global environment, with numerous stakeholders, including citizen and society. What can be seen at the first glance as a barrier and a mechanism slowing down the progression of data science must, however, be considered an important asset. Only global adoption can transform the potential of big data and artificial intelligence into an effective breakthroughs in handling health and medicine. This requires science and society, scientists and citizens, to progress together.\"}, {\"paperId\": \"0d8ac07f1f376796ed1f142909971c6ef1c293d6\", \"abstract\": null}, {\"paperId\": \"01c69ac4f5cc933b501ad647c8817653c1ab5677\", \"abstract\": \"Through the exploitation of scientific knowledge and technology, man has made wonderous strides in the field of automation, the focus of which lies in 'Robotics and Machine Learning'. All around us we see machines taking over work with accuracy and ease. Seen as a subset of artificial intelligence, machine learning relies on data, patterns in data and inference to aid technology in thinking for itself. This paper aims to apply the science of machine learning in the field of agriculture, by carrying soil fertility analysis using most accurate algorithm. The fertility of soil plays a principal role in determining the suitability of cultivating a particular crop on a given soil type. Analysis is carried out by the examination of various properties of the soil like the pH value, Electrical Conductivity, Moisture content, Temperature and (N)Nitrogen (P)Phosphorous (K) Potassium levels, followed up by soil type classification. Finally, a recommendation for the most suitable crop is provided in real time.\"}, {\"paperId\": \"a663794cad13b2d57aef4d7e07f30a28789ad80e\", \"abstract\": \"Machine learning, data science and artificial intelligence (AI) technology in healthcare (herein collectively referred to as machine learning for healthcare (MLHC)) is positioned to have substantial positive impacts on healthcare, enhancing progress in both the acquisition of healthcare knowledge\"}, {\"paperId\": \"15e0914a9a4e3687fa74936bf9ae02b32103f92c\", \"abstract\": \"Biomedical research is increasingly a high-dimensional science. In the pharmaceutical industry, data supporting the drug discovery and development process have become cheaper to generate and are increasing in complexity, diversity, and volume at a fast pace. This is the result of the introduction and development of novel technologies that enable molecular profiling, imaging, and other types of high-throughput readouts at an unprecedented scale. Similarly, clinical data created by companies or compiled by biobanks are growing at an exponential pace, not only in terms of sample size but also in the breadth of (increasingly digital) endpoints being measured and captured. In parallel to the increase in data, methodological advances are driving renewed development in statistical modeling, machine learning, and artificial intelligence (AI). The combination of data, computing power, and advanced analytics is positioning data science as a critical core discipline in pharmaceutical research, alongside the more traditional disciplines of biology, chemistry, and medicine. Realizing the full potential of data science requires adapting both the structure and culture of an organization. Signs of this transformation can already be seen across the pharmaceutical industry, with the creation of large data science teams and executive roles responsible for the implementation of a company-wide data science strategy. At a time when discovering transformative medicines is more challenging and requires more scientific creativity than ever before, we offer strategic recommendations to those who aspire to propel a digital culture shift and data science transformation in their organizations (Fig 1).\"}, {\"paperId\": \"f7dfdffe373428468e1cb9ef65233682aa65c6c5\", \"abstract\": null}, {\"paperId\": \"63a87ece269d298d87ebc6d59b25119814e5a63c\", \"abstract\": \"In the last two decades, we have witnessed the intensive development of artificial intelligence in the field of agriculture. In this period, the transition from the application of simpler machine learning algorithms to the application of deep learning algorithms can be observed. This paper provides a quantitative overview of papers published in the past two decades, thematically related to machine learning, neural networks, and deep learning. Also, a review of the contribution of individual countries was given. The second part of the paper analyses trends in the first half of the current year, with an emphasis on areas of application, selected deep learning methods, input data, crop mentioned in the paper and applied frameworks. Scopus and Web of Science citation databases were used.\"}, {\"paperId\": \"da3bbf1927d42a8b3152500cd1195033f45bb95c\", \"abstract\": \"Now-a-days artificial intelligence has become an asset for engineering and experimental studies, just like statistics and calculus. Data science is a growing field for researchers and artificial intelligence, machine learning and deep learning are roots of it. This paper describes the relation between these roots of data science. There is a need of machine learning if any kind of analysis is to be performed. This study describes machine learning from the scratch. It also focuses on Deep Learning. Deep learning can also be known as new trend of machine learning. This paper gives a light on basic architecture of Deep learning. A comparative study of machine learning and deep learning is also given in the paper and allows researcher to have a broad view on these techniques so that they can understand which one will be preferable solution for a particular problem.\"}, {\"paperId\": \"a89e70b01904d174aab42d2577d59dd42c09e66e\", \"abstract\": \"Personalized medicine is being realized by our ability to measure biological and environmental information about patients. Much of these data are\\u00a0being stored in electronic health records yielding big data that presents challenges for its management and analysis. Here, we review several areas of knowledge that are necessary for next-generation scientists to fully realize the potential of biomedical big data. We begin with an overview of big data and its storage and management. We then review statistics and data science as foundational topics followed by a core curriculum of artificial intelligence, machine learning and natural language processing that are needed to develop predictive models for clinical decision making. We end with some specific training recommendations for preparing next-generation scientists for biomedical big data.\"}, {\"paperId\": \"c2fb23f4767ee61717ee87cfd864c05eaa69bead\", \"abstract\": \"Credit cards are very commonly used in making online payments. In recent years\\u2019 frauds are reported which are accomplished using credit cards. It is very difficult to detect and prevent the fraud which is accomplished using credit card. Machine Learning(ML) is an Artificial Intelligence (AI) technique which is used to solve many problems in science and engineering. In this paper, machine learning algorithms are applied on a data set of credit cards frauds and the power of three machine learning algorithms is compared to detect the frauds accomplished using credit cards. The accuracy of Random Forest machine learning algorithm is best as compared to Decision Tree and XGBOOST algorithms.\"}, {\"paperId\": \"d92c390fcb0ea1cb9da8ee4a2e85bebdfc65c96f\", \"abstract\": \"Objective: The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments. Summary Background Data: AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers. Methods: A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed. Results: Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed. Conclusions: Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care.\"}, {\"paperId\": \"ff507fb61704e6ba733c2bd7b9229e32a3ef3d8c\", \"abstract\": \"The continued advances in artificial intelligence and automation through machine learning applications, under the heading of data science, gives reason for pause within the educator community as we consider how to position future human factors engineers to contribute meaningfully in these projects. Do the lessons we learned and now teach regarding automation based on previous generations of technology still apply? What level of DS and ML expertise is needed for a human factors engineer to have a relevant role in the design of future automation? How do we integrate these topics into a field that often has not emphasized quantitative skills? This panel discussion brings together human factors engineers and educators at different stages of their careers to consider how curricula are being adapted to include data science and machine learning, and what the future of human factors education may look like in the coming years.\"}, {\"paperId\": \"3342a3a20cefe42c592e51c4f8d0be378ed0387b\", \"abstract\": \"ABSTRACT High performance computing is required for fast geoprocessing of geospatial big data. Using spatial domains to represent computational intensity (CIT) and domain decomposition for parallelism are prominent strategies when designing parallel geoprocessing applications. Traditional domain decomposition is limited in evaluating the computational intensity, which often results in load imbalance and poor parallel performance. From the data science perspective, machine learning from Artificial Intelligence (AI) shows promise for better CIT evaluation. This paper proposes a machine learning approach for predicting computational intensity, followed by an optimized domain decomposition, which divides the spatial domain into balanced subdivisions based on the predicted CIT to achieve better parallel performance. The approach provides a reference framework on how various machine learning methods including feature selection and model training can be used in predicting computational intensity and optimizing parallel geoprocessing against different cases. Some comparative experiments between the approach and traditional methods were performed using the two cases, DEM generation from point clouds and spatial intersection on vector data. The results not only demonstrate the advantage of the approach, but also provide hints on how traditional GIS computation can be improved by the AI machine learning.\"}, {\"paperId\": \"16c324d2375102ad35db8b395651c1716b950428\", \"abstract\": null}, {\"paperId\": \"08923cf18d934563ff3b5661a06bee3896c6beb9\", \"abstract\": \"The pandemic caused by COVID-19 in 2020 triggered a devastating effect on the economy and health of the world population, whose social implications for the next few years are still uncertain. Two types of standard tests are used to detect COVID-19: the viral test that indicates whether the patient is infected and the antibody test that allows us to observe if the patient has previously had an infection. These tests employ techniques such as reverse transcription and polymerase chain reaction (RT-PCR), immunochromatographic lateral flow or rapid test, and ELISA-type immunoassay In this paper we have designed and implemented a system whose main purpose is to detect the rise of Covid-19 cases using disruptive technologies such as artificial intelligence and intelligent computing, manifested through machine learning (Machine Learning) and deep learning (Deep Learning). Combined with data science, Big Data and advanced data analytics, among others that present various research and development options, it can help the early detection of COVID-19 through the search for relevant characteristics that allow the scientific community identify biochemical, molecular and cellular factors that facilitate the early detection of the virus in its different states of infection, incubation, propagation and treatments to be used\"}, {\"paperId\": \"e81f44ff2f93ae8b669982796475e448ec95557a\", \"abstract\": \"Industrial processes are ripe with data and offer countless opportunities for applied data science, machine learning and artificial intelligence. While process automation and control are providing more guidance in normal operating states, the need for data analytics is abundant when dealing with deviations from defined states, aiming at consistent transitions, or exploring new operating states to optimize production. This paper provides a brief overview of some examples, and introduces a real-life case study available to educators to challenge engineering students in preparation for roles in the\"}, {\"paperId\": \"bfeef986b82c9cb958e647b742eb6a86951b368d\", \"abstract\": \"The synthesis of CdSe/CdS core/shell nanoparticles was revisited with the help of a causal inference machine learning framework. The tadpole morphology with 1-2 tails was experimentally discovered. The causal inference model revealed the causality between the oleic acid (OA), octadecyl-phosphonic acid (ODPA) ligands and the detailed tail shape of the tadpole morphology. Further, with the identified causality, a neural network was provided to predict and directly lead to the original experimental discovery of new tadpole-shaped structures. An entropy-driven nucleation theory was developed to understand both the ligand & temperature dependent experimental data and the causal inference from the machine learning framework. This work provided a vivid example of how the artificial intelligence technology, including machine learning, could benefit the materials science research for the discovery.\"}, {\"paperId\": \"97e3181e0cc8c797c0cf599b91a38690b2bc3ca8\", \"abstract\": \"Amidst the global pandemic, the methodical acquisition, analysis, and evaluation of health-related data is crucial to learn from experience and strengthen preparedness for future challenges of similar nature. The goal of this paper is to survey the recent approaches to disease surveillance or outbreak monitoring in the context of artificial intelligence or machine learning. Utilizing Elsevier's Scopus database, the keywords, \\u201cDisease Surveillance\\u201d or \\u201cOutbreak Monitoring\\u201d yielded 12,648 document results (with year duration starting 2016). Then, the documents were reduced to 367 after conjunction with the terms, namely, \\u201cArtificial Intelligence\\u201d or \\u201cMachine Learning (ML)\\u201d or \\u201cData Science\\u201d and limiting the document type to article and review only. The documents were examined one-by-one leaving only the most recent and those papers which applied machine learning methods. The survey showed four major ML tasks in the recent literature, namely, topic modeling, time series forecasting & regression, classification, and clustering. It was also observed that many disease surveillance systems are still utilizing traditional (shallow) machine learning techniques. However, deep learning-based techniques was also demonstrated, in the literature, to perform better than the traditional models.\"}, {\"paperId\": \"64dbf2bbc265f7e91b6e90cfc1177363c5d8c2f0\", \"abstract\": null}, {\"paperId\": \"56a10536beadb78e87b8dcb88012d772cf60e892\", \"abstract\": \"This paper identifies and measures developments in science, algorithms and technologies related to artificial intelligence (AI). Using information from scientific publications, open source software (OSS) and patents, it finds a marked increase in AI-related developments over recent years. Since 2015, AI-related publications have increased by 23% per year; from 2014 to 2018, AI-related OSS contributions grew at a rate three times greater than other OSS contributions; and AI-related inventions comprised, on average, more than 2.3% of IP5 patent families in 2017. China\\u2019s growing role in the AI space also emerges. The analysis relies on a three-pronged approach based on established bibliometric and patent-based methods, and machine learning (ML) implemented on purposely collected OSS data.\"}, {\"paperId\": \"bce43a15423c7b1b049cfec2d923cb2352f646df\", \"abstract\": null}, {\"paperId\": \"39b9bf46ed2f9f03cd82e7faa84b8ff5d190187d\", \"abstract\": \"Genomics creates large databases for the discovery, study and production of new therapeutics worldwide. It would not be impossible to imagine that 3 billion base pairs comprising the humanoid genetic makeup may now be studied to find genetic differences within the population by artificial intelligence. Large pharmaceutical firms such as Astra Zeneca are aiming to research up to 2 million genomes by 2026 and review vast quantities of patient data points from their clinical drug trials. AI will be used in genomics for multiple omics experiments, such as transcriptomics, as we introduce more instruments. AI is increasingly being used by healthcare firms in accordance with HEOR (Health Economics Outcome Research), i.e. In order to help classify possible clinically important genes, AI is used to combine data produced from genomic studies with analysis from science literature. Machine learning today plays an integral role in the development of the genomics industry. In this paper, we set out to explore the uses of genomics machine learning to help market leaders consider existing and evolving developments in the field. We have discussed history terms and distilled perspectives from various study. Current applications of machine learning in gene technology boost up future applications of genomics machine learning.\"}, {\"paperId\": \"eb39aea48593194cd61ef7f4f1765e3da8667287\", \"abstract\": \"Artificial intelligence is a branch of computer science that, in broad terms, deals with either decision making or classification. The aim of artificial intelligence is to surpass human cognitive functioning such that automated decisions can be made. Machine learning \\u2014 an application of artificial intelligence \\u2014 is commonly used in image recognition. In general, the machine, or algorithm, learns from exposure to a large dataset. Once learning has taken place, the algorithm can be applied to unseen data. The potential advantages of this approach in health care are clear: machines can learn from very large datasets in relatively short time frames and can apply themselves to new data without fatigue or intraobserver replication error.\"}, {\"paperId\": \"52260b5317f9ae76127f4301969be2ff3aad847d\", \"abstract\": null}, {\"paperId\": \"838b570090485c987688da9669b47971d7663f4b\", \"abstract\": \"Over the past few years, pharmaceutical R&D has become aware of the potential benefits of leveraging artificial intelligence and its collective subfields including machine learning, deep learning, data science and advanced analytics. These technologies are being embraced across industries to provide enhanced automation, gain insights into data, and improve data-driven decision making. The evangelization from lower level technical experts has now been echoed by the top levels of many organizations, as exemplified by Vas Narasimhan\\u2019s (Novartis CEO) goal to evolve AI to place it at the \\u201cheart of the company\\u201d [1] and Alex Bourla\\u2019s (Pfizer CEO) aim to win the digital race in pharma using machine learning and AI to expedite R&D [2]. Although its value compared to pure science continues to be questioned, machine learning and particularly deep learning have introduced many compelling use cases.\"}, {\"paperId\": \"a01cb109d2488a9820c52ba43fbf07a2893c51fa\", \"abstract\": \"Causal inference has numerous real-world applications in many domains such as health care, marketing, political science and online advertising. Treatment effect estimation, a fundamental problem in causal inference, has been extensively studied in statistics for decades. However, traditional treatment effect estimation methods may not well handle large-scale and high-dimensional heterogeneous data. In recent years, an emerging research direction has attracted increasing attention in the broad artificial intelligence field, which combines the advantages of traditional treatment effect estimation approaches (e.g., matching estimators) and advanced representation learning approaches (e.g., deep neural networks). In this tutorial, we will introduce both traditional and state-of-the-art representation learning algorithms for treatment effect estimation. Background about causal inference, counterfactuals and matching estimators will be covered as well. We will also showcase promising applications of these methods in different application domains.\"}, {\"paperId\": \"45b6fc61b1db8c7b551416cb6cae92e72255a9e5\", \"abstract\": null}, {\"paperId\": \"6e82e84260d8d50c210cb3196eaf3155cefbc320\", \"abstract\": \"For half a century, bioinformatics and computational biology have provided tools and data analysis approaches, so the beginning of the omics era represented a novel challenge for researchers, that converged to the area of bioinformatics from the fields of informatics, mathematics, and statistics. In most cases, the solutions offered appeared difficult to use for researchers working in biomedical areas. This occurred in particular when sophisticated approaches from the field of data science and artificial intelligence (AI), were applied to biomedical data (Lisboa et al., 2000). Machine learning, statistical learning, and soft-computing approaches, such as deep neural networks or genetic algorithms, have also become terms used in the bio world, with an incomplete comprehension however, of their potential (Pavel et al., 2016; Lin and Lane, 2017; Zeng and Lumley, 2018). In recent years, omics, multi-omics, and inter-omics experiments have presented a further step toward the investigation in biology, opening the window on personalized medicine, for example for diagnostics (Riemenschneider et al., 2016). The era of big data in medicine is imminent and represents yet a further step forward. Considering this, our Research Topic presents articles on novel developments in the field of artificial intelligence in biology and medicine, and their applications in the analysis of high-throughput data from omics and inter-omics approaches (Facchiano et al.).\"}, {\"paperId\": \"ff9fb4feefc7a7a196e8cb4e5ebf7fa75a050859\", \"abstract\": \". Artificial intelligence is rapidly expanding its presence and is now being drawn into the construction industry. Building Information Modeling (BIM) is considered one of the key elements in the field of architecture, engineering and construction (NPPs) with a market volume of $8 billion by 2020 in various segments such as commercial, educational, residential, medical, industrial, entertaining and sports. BIM is a relatively new technology in the industry that is usually slow to change. However, many early followers believed that BIM would grow exponentially through the development of digital technologies such as mobile communications, IoT, Big Data, Data Science, machine learning and artificial intelligence. Direct integration of machine learning technology in BIM can facilitate in various areas, such as \\u2013 identification of the default object from previous similar projects; excessive removal of information in the process of learning without a teacher. Semi\\u2013supervised or supervised learning can upgrade earlier non\\u2013BIM data to complex BIM-supported projects. Advanced training agents can help with online management and maintenance. Even ready-made machine learning, such as language recognition, object detection, object identification, or natural language processing using machine learning technology, can be used to directly model recovery instead of prone to manual typing errors. Like any new process, change takes time, especially in an industry as distinct as construction. However, the simplified and cost\\u2013effective BIM design process offers proven cost reduction and quality improvement. What can be easily achieved with artificial intelligence and machine learning.\"}, {\"paperId\": \"d836a8d6a0ec72fe1564c30a5fabce2463e79188\", \"abstract\": \"One of the areas where Artificial Intelligence is having more impact is machine learning, which develops algorithms able to learn patterns and decision rules from data. Machine learning algorithms have been embedded into data mining pipelines, which can combine them with classical statistical strategies, to extract knowledge from data. Within the EU-funded MOSAIC project, a data mining pipeline has been used to derive a set of predictive models of type 2 diabetes mellitus (T2DM) complications based on electronic health record data of nearly one thousand patients. Such pipeline comprises clinical center profiling, predictive model targeting, predictive model construction and model validation. After having dealt with missing data by means of random forest (RF) and having applied suitable strategies to handle class imbalance, we have used Logistic Regression with stepwise feature selection to predict the onset of retinopathy, neuropathy, or nephropathy, at different time scenarios, at 3, 5, and 7 years from the first visit at the Hospital Center for Diabetes (not from the diagnosis). Considered variables are gender, age, time from diagnosis, body mass index (BMI), glycated hemoglobin (HbA1c), hypertension, and smoking habit. Final models, tailored in accordance with the complications, provided an accuracy up to 0.838. Different variables were selected for each complication and time scenario, leading to specialized models easy to translate to the clinical practice.\"}, {\"paperId\": \"f9e0e85732f0736c0d5a6f0c63df5c7f1f245dcd\", \"abstract\": null}, {\"paperId\": \"a7c3e325b5c42fb9cdeab93be33a26d17c54db2c\", \"abstract\": \"Big data and artificial intelligence has revolutionized science in almost every field \\u2013 from economics to physics. In the area of materials science and computational heterogeneous catalysis, this revolution has led to the development of scientific data repositories, as well as data mining and machine learning tools to investigate the vast materials space. The goal of using these tools is to establish a deeper understanding of the relations between materials properties and activity, selectivity and stability \\u2013 the important figures of merit in catalysis. Based on these insights, catalyst design principles can be established, which hopefully lead us to discover highly efficient catalysts to solve pressing issues for a sustainable future and the synthesis of highly functional materials, chemicals and pharmaceuticals. The inherent complexity of catalytic reactions quests for machine learning methods to efficiently navigate through the high\\u2010dimensional hyper\\u2010surfaces in structure optimization problems to determine relevant chemical structures and transition states. In this review, we show how cutting edge data infrastructures and machine learning methods are being used to address problems in computational heterogeneous catalysis.\"}, {\"paperId\": \"a87d4b0e40b2dc6138f0bd2286a969d69326a262\", \"abstract\": \"It is common practice amongst coaches and analysts to search for key performance indicators related to attacking play in football. Match analysis in professional football has predominately utilised notational analysis, a statistical summary of events based on video footage, to study the sport and prepare teams for competition. Recent increases in technology have facilitated the dynamic analysis of more complex process variables, giving practitioners the potential to quickly evaluate a match with consideration to contextual parameters. One field of research, known as machine learning, is a form of artificial intelligence that uses algorithms to detect meaningful patterns based on positional data. Machine learning is a relatively new concept in football, and little is known about its usefulness in identifying performance metrics that determine match outcome. Few studies and no reviews have focused on the use of machine learning to improve tactical knowledge and performance, instead focusing on the models used, or as a prediction method. Accordingly, this article provides a critical appraisal of the application of machine learning in football related to attacking play, discussing current challenges and future directions that may provide deeper insight to practitioners.\"}, {\"paperId\": \"94e8c8744ebd5f8752b8519c7ce7f279dfd12038\", \"abstract\": \"In recent years, the growth of Internet of Things (IoT) as an emerging technology has been unbelievable. The number of networkenabled devices in IoT domains is increasing dramatically, leading to the massive production of electronic data. These data contain valuable information which can be used in various areas, such as science, industry, business and even social life. To extract and analyze this information and make IoT systems smart, the only choice is entering artificial intelligence (AI) world and leveraging the power of machine learning and deep learning techniques. This paper evaluates the performance of 11 popular machine and deep learning algorithms for classification task using six IoT-related datasets. These algorithms are compared according to several performance evaluation metrics including precision, recall, f1-score, accuracy, execution time, ROC-AUC score and confusion matrix. A specific experiment is also conducted to assess the convergence speed of developed models. The comprehensive experiments indicated that, considering all performance metrics, Random Forests performed better than other machine learning models, while among deep learning models, ANN and CNN achieved more interesting results.\"}, {\"paperId\": \"bbe128f8797996036c3b9860c7648d8e9b339fc5\", \"abstract\": \"Every day, farms produce thousands of information points on temperature, soil, usage of water, atmospheric phenomenon, etc. With the assistance of computer science and machine learning models, this data is leveraged in real-time for obtaining useful insights like choosing the correct time to plant seeds, determining the crop choices, hybrid seed choices etc. Keywords: Artificial intelligence, agriculture robots, agriculture, intelligent spraying, temperature, soil, water, machine learning Cite this Article G. Ramachandran, T. Sheela, S. Kannan, A. Malarvizhi, G. Sureshkumar, P.M. Murali, G. Murali. Applications of Artificial Intelligence in Agriculture. Journal of Computer Technology & Applications. 2020; 11(1): 1\\u20133p.\"}, {\"paperId\": \"d532ce319b4ecbfdc5751ac2955bf5b9015d0034\", \"abstract\": null}, {\"paperId\": \"50cadaf9a0c43cf1a2fcea1f3c22eaa556057a2f\", \"abstract\": \"The rapid development of computer technologies brings us great changes in daily life and work. Artificial intelligence is a branch of computer science, which is to allow computers to exercise activities that are normally confined to intelligent life. The broad sense of artificial intelligence includes machine learning and robots. This article mainly focuses on machine learning and related medical fields, and deep learning is an artificial neural network in machine learning. Convolutional neural network (CNN) is a type of deep neural network, that is developed on the basis of deep neural network, further imitating the structure of the visual cortex of the brain and the principle of visual activity. The current machine learning method used in medical big data analysis is mainly CNN. In the next few years, it is the developing trend that artificial intelligence as a conventional tool will enter the relevant departments of medical image interpretation. In addition, this article also shares the progress of the integration of artificial intelligence and biomedicine combined with actual cases, and mainly introduces the current status of CNN application research in pathological diagnosis, imaging diagnosis and endoscopic diagnosis for gastrointestinal diseases.\"}, {\"paperId\": \"ed96a834f2ba3d605bfc450048c457c9e178f33a\", \"abstract\": \"Artificial intelligence (AI) has recently become an object of interest for specialists from various fields of science and technology, including healthcare professionals. Significantly increased funding for the development of AI models confirms this fact. Advances in machine learning (ML), availability of large data sets, and increasing processing power of computers promote the implementation of AI in many areas of human activity. Being a type of AI, machine learning allows automatic development of mathematical models using large data sets. These models can be used to address multiple problems, such as prediction of various events in obstetrics and neonatology. Further integration of artificial intelligence in perinatology will facilitate the development of this important area in the future. This review covers the main aspects of artificial intelligence and machine learning, their possible application in healthcare, potential limitations and problems, as well as outlooks in the context of AI integration into perinatal medicine. Key words: artificial intelligence, cardiotocography, neonatal asphyxia, fetal congenital abnormalities, fetal hypoxia, machine learning, neural networks, prediction, prognosis, perinatal risk, prenatal diagnosis\"}, {\"paperId\": \"a5b76dcf7580e1052300715936eb0ae5e9c3c18b\", \"abstract\": \"Objective Research in artificial intelligence area appears to have been taken up by different specialties with varying enthusiasm. We compared the contribution of different medical specialties to machine learning, deep learning, and artificial intelligence research over 30 years. Methods The Web of Science database was searched retrospectively for the terms \\u201cartificial intelligence\\u201d, \\u201cmachine learning\\u201d and \\u201cdeep learning\\u201d. Results were limited to articles, proceedings papers, or reviews published in Web of Science categories mapped to the OECD 3.02 Clinical Medicine schema between the year 1988 and 2018. A list of international medical specialties was assembled, a search tool was created to query author affiliation data, and analysis performed to assess specialty publication over time and inter-specialty collaboration. Publication differences between specialties were evaluated for significance with two-tail unpaired t-test. Results Initial database search returned 3937 unique results once duplicates were removed. Medical specialty analysis returned 2381 papers from 789 different journals. Radiology published significantly more papers than other top specialties (p < 0.001), being involved in 783 papers (33% of returned total), followed by psychiatry (406 papers) and neurology (287 papers). Conclusion There has been an exponentially increase in yearly publications involving artificial intelligence, machine learning, and deep learning over the last 30 years. Radiology is the leading medical speciality in machine learning, deep learning, and artificial intelligence research in terms of volume of yearly publications and overall citations, followed by psychiatry and neurology. Tasman Medical Journal 2020; 2: 20-27 Introduction Interest in machine learning and artificial intelligence (AI) has increased over recent years, and new techniques such as deep learning (DL) have achieved impressive results in multiple diverse fields. Modern image recognition techniques have proven to be very capable when applied to the increasingly available medical datasets, with algorithms performing at or above the level of physicians at a number of specific tasks such as skin cancer classification, pneumonia detection on chest x-ray, and arrhythmia detection on electrocardiogram. In addition to the advances in\"}, {\"paperId\": \"d688bb34a3e6ed59c326dc30d43fce8083d69baf\", \"abstract\": \"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human\\u2013AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.\"}, {\"paperId\": \"26c842995920157dd3997c27ff8175416b4604d9\", \"abstract\": null}, {\"paperId\": \"b22ddde457fb9439d02b16761af572830b25c985\", \"abstract\": \"Artificial Intelligence (AI) can now automate the algorithm selection, feature engineering, and hyperparameter tuning steps in a machine learning workflow. Commonly known as AutoML or AutoAI, these technologies aim to relieve data scientists from the tedious manual work. However, today's AutoAI systems often present only limited to no information about the process of how they select and generate model results. Thus, users often do not understand the process, neither do they trust the outputs. In this short paper, we provide a first user evaluation by 10 data scientists of an experimental system, AutoAIViz, that aims to visualize AutoAI's model generation process. We find that the proposed system helps users to complete the data science tasks, and increases their understanding, toward the goal of increasing trust in the AutoAI system.\"}, {\"paperId\": \"5ea1a7929f5cf850b9e4a7ffbc483ec32a8f2130\", \"abstract\": null}, {\"paperId\": \"ee9995f5bf81578b0f9622e26f777cc16c10dc48\", \"abstract\": null}, {\"paperId\": \"35a0e5a047dea64b5f4b248faba9b6df5fb25e65\", \"abstract\": \"Abstract \\u2013 Data are the prominent elements in scientific researches and approaches. Data Science methodology is used to select and to prepare enormous numbers of data for further processing and analysing. Big Data technology collects vast amount of data from many sources in order to exploit the information and to visualise trend or to discover a certain phenomenon in the past, present, or in the future at high speed processing capability. Predictive analytics provides in-depth analytical insights and the emerging of machine learning brings the data analytics to a higher level by processing raw data with artificial intelligence technology. Predictive analytics and machine learning produce visual reports for decision makers and stake-holders. Regarding cyberspace security, big data promises the opportunities in order to prevent and to detect any advanced cyber-attacks by using internal and external security data. Keywords : Big Data, Cyber Security, Data Science, Intelligence, Predictive Analytics Abstrak \\u2013 Data merupakan unsur terpenting dalam setiap penelitian dan pendekatan ilmiah. Metodologi sains data digunakan untuk memilah, memilih dan mempersiapkan sejumlah data untuk diproses dan dianalisis. Teknologi big data mampu mengumpulkan data dengan sangat banyak dari berbagai sumber dengan tujuan untuk mendapatkan informasi dengan visualisasi tren atau menyingkapkan pengetahuan dari suatu peristiwa yang terjadi baik dimasa lalu, sekarang, maupun akan datang dengan kecepatan pemrosesan data sangat tinggi. Analisis prediktif memberikan wawasan analisis lebih dalam dan kemunculan machine learning membawa analisis data ke tingkat yang lebih tinggi dengan bantuan teknologi kecerdasan buatan dalam tahap pemrosesan data mentah. Analisis prediktif dan machine learning menghasilkan laporan berbentuk visual untuk pengambil keputusan dan pemangku kepentingan. Berkenaan dengan keamanan siber, big data menjanjikan kesempatan dalam rangka untuk mencegah dan mendeteksi setiap serangan canggih siber dengan memanfaatkan data keamanan internal dan eksternal. Kata Kunci : Analisis Prediktif, Big Data, Intelijen, Keamanan Siber, Sains Data\"}, {\"paperId\": \"31bd19a1f6a4557840f84af253ac5395ae498433\", \"abstract\": null}, {\"paperId\": \"8d41cf63e1455b27fad5b9299c49aaa658c0cd85\", \"abstract\": \"A dvances in machine learning (ML) are making a large impact in many fields, including: artificial intelligence, materials science, and chemical engineering. Generally, ML tools learn from data to find insights or make fast predictions of target properties. Recently, ML is also greatly influencing heterogeneous catalysis research due to the availability of ML (e.g., Python Scikit-learn, TensorFlow) and workflow management tools (e.g., ASE, Atomate), the growing amount of data in materials databases (e.g., Novel Materials Discovery Laboratory, Citrination, Materials Project, CatApp), and algorithmic improvements. New catalysts are needed for sustainable chemical production, alternative energy, and pollution mitigation applications to meet the demands of our world\\u2019s rising population. It is a challenging endeavor, however, to make novel heterogeneous catalysts with good performance (i.e., stable, active, selective) because their performance depends on many properties: composition, support, surface termination, particle size, particle morphology, and atomic coordination environment. Additionally, the properties of heterogeneous catalysts can change under reaction conditions through various phenomena such as Ostwald ripening, particle disintegration, surface oxidation, and surface reconstruction. Many heterogeneous catalyst structures are disordered or amorphous in their active state, which further complicates their atomic-level characterization by modeling and experiment. Computational modeling using quantum mechanical (QM) methods such as density functional theory (DFT) can accelerate catalyst screening by enabling rapid prototyping and revealing active sites and structure-activity relations. The high computational cost of QM methods, however, limits the range of catalyst spaces that can be examined. Recent progress in merging ML with QM modeling and experiments promises to drive forward rational catalyst design. Therefore, it is timely to highlight the ability of ML tools to accelerate heterogeneous catalyst research. A key question we aim to address in this perspective is how machine learning can aid heterogeneous catalyst design and discovery. ML has been used in catalysis research since at least the 1990s. Early studies used neural networks to correlate catalyst physicochemical properties and reaction conditions with measured catalytic performance, but these studies were limited in the number of systems considered. Recently, ML has been applied to the high-throughput screening of heterogeneous catalysts and found to be predictive and applicable across a broad space of catalysts. ML algorithms such as decision trees, kernel ridge regression, neural networks, support vector machines, principal component analysis, and compressed sensing can help create predictive models of catalyst target properties, which are typically figures of merit corresponding to stability, activity, selectivity. In this perspective, we discuss various areas where ML is making an impact on heterogeneous catalysis research. ML is also aiding homogeneous catalysis research and shares many similarities (and differences) with ML for heterogeneous catalysis, but this discussion is beyond the perspective\\u2019s scope (for interested readers, see Ref. 26\\u201328). Here, we emphasize the ability of ML combined with QM calculations to speed-up the search for optimal catalysts in combinatorial large spaces, such as alloys. ML-derived interatomic potentials for accurate and fast catalyst simulations will also be assessed, as well as the opportunity for ML to help find descriptors of catalyst performance in large datasets. The use of ML to aid transition state search algorithms (to compute reaction mechanisms) will Correspondence concerning this article should be addressed to B. R. Goldsmith at bgoldsm@umich.edu.\"}, {\"paperId\": \"23ce205be40bb18d13e2af1f6bd93d0c02f645f5\", \"abstract\": null}, {\"paperId\": \"4cfcdf865d51f1578f0bd968a480c2cf81747802\", \"abstract\": \"In this thesis I examine how Artificial Intelligence (AI) techniques can help Computer Science students learn programming and mathematics skills more efficiently using algorithms and concepts such as Predictive Modelling, Machine Learning, Deep Learning, Representational Learning, Recommender Systems and Graph Theory. For that, I use Learning Analytics (LA) and Educational Data Mining (EDM) principles. In Learning Analytics one collects and analyses data about students and their contexts for purposes of understanding and improving their learning and the environments students interact with. Educational Data Mining applies Data Mining, Machine Learning and statistics to data captured during these learning processes. \\nMy central research question is how we can optimise the learning by students, of subjects like computer programming and mathematics in blended and online classrooms by mining and analysing data generated in these environments by the students. To validate the research question I have implemented several examples of monitoring student behaviour while learning, I have gathered various forms of student interaction data and combined it with demographics and student performance data (e.g. exam results) in order to test out different predictive models developed using a variety of AI and machine learning techniques. In these example environments I have used these models not only to predict outcome and exam performance but also to automatically generate feedback to students in a variety of ways, including recommending better programming techniques. My research question is explored by examining the performance of the AI techniques in helping to improve student learning.\"}, {\"paperId\": \"ad4f070836743c195fc7f30d4226b425a5400321\", \"abstract\": \"Machine learning is an inevitable outcome of the development of artificial intelligence research to a certain stage. It has become the most popular technology in the fields of computer vision and natural language processing. Learning techniques such as inductive logic programming, neural network-based connectionist learning technology, and statistical learning theory are constantly evolving. The shortcomings of various learning techniques in data representation and result processing have become a concern of many scholars. As one of the cutting-edge science and technology in the 21st century, artificial intelligence has a profound impact on education. This research is based on the artificial intelligence in Visualization Application for nearly 12 years. It has carried out visual analysis of research hotspots and frontiers. In addition, this study also discusses the research content through cluster analysis, discusses the influence of artificial intelligence on Chinese education field, and reflects on it.\"}, {\"paperId\": \"5c861e98893682e9e183ffa47041de802de81def\", \"abstract\": null}, {\"paperId\": \"3aef4583b8d4c07aebb4c84af4145711a400d752\", \"abstract\": null}, {\"paperId\": \"6db5d0ced9fe413bfd6c75d0bfc5b869a30412f8\", \"abstract\": \"Despite the application of advanced statistical and pharmacometric approaches to pediatric trial data, a large pediatric evidence gap still remains. Here, we discuss how to collect more data from children by using real\\u2010world data from electronic health records, mobile applications, wearables, and social media. The large datasets collected with these approaches enable and may demand the use of artificial intelligence and machine learning to allow the data to be analyzed for decision making. Applications of this approach are presented, which include the prediction of future clinical complications, medical image analysis, identification of new pediatric end points and biomarkers, the prediction of treatment nonresponders, and the prediction of placebo\\u2010responders for trial enrichment. Finally, we discuss how to bring machine learning from science to pediatric clinical practice. We conclude that advantage should be taken of the current opportunities offered by innovations in data science and machine learning to close the pediatric evidence gap.\"}, {\"paperId\": \"b0ebda50db352d956aa3e5bc66f6437a2d528ea5\", \"abstract\": \"Nowadays, data is being generated by so many devices, therefore the term big data. This paper attempts to offer a broader definition of big data that captures its defining characteristics. This paper also reinforces the need to devise new tools for predictive analytics using machine learning which is a subset of artificial intelligence in the field of computer science that often uses statistical techniques to give computers the ability to \\\"learn\\\" (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed. With the abundance of data, comes the prediction models along with the machine learning that has been trained, the executives will become better at their decision-making process.\"}, {\"paperId\": \"0475222d7c2d410d62677dd5f5e92fe27792c1b4\", \"abstract\": \"This article offers a brief overview of some of the ethical challenges raised by artificial intelligence (AI), in particular machine learning and data science, and summarizes and discusses a number of challenges for near-future regulation in this area. This includes the difficulties of moving from principles to more concrete measures and problems with implementing ethics by design and responsible innovation.\"}, {\"paperId\": \"83e51a0499279bce498873d1a5ffd3b009a53859\", \"abstract\": \"Recent advances in computer hardware and algorithms are spawning an explosive growth in the use of computer-based systems aimed at analyzing and ultimately correlating large amounts of experimental and synthetic data. As these machine learning tools become more widespread, it is becoming imperative that scientists and researchers become familiar with them, both in terms of understanding the tools and the current limitations of artificial intelligence, and more importantly being able to critically separate the hype from the real potential. This article presents a classroom exercise aimed at first-year science and engineering college students, where a task is set to produce a correlation to predict the normal boiling point of organic compounds from an unabridged data set of >6000 compounds. The exercise, which is fully documented in terms of the problem statement and the solution, guides the students to initially perform a linear correlation of the boiling point data with a plausible relevant variable (the ...\"}, {\"paperId\": \"fb566f2001e44a65433fb7cc2eb7bcf6513a7db8\", \"abstract\": \"Scientific rigor and critical thinking skills are indispensable in this age of big data because machine learning and artificial intelligence are often led astray by meaningless patterns. The 9 Pitfalls of Data Science is loaded with entertaining real-world examples of both successful and misguided approaches to interpreting data, both grand successes and epic failures. Anyone can learn to distinguish between good data science and nonsense. We are confident that readers will learn how to avoid being duped by data, and make better, more informed decisions. Whether they want to be effective creators, interpreters, or users of data, they need to know the nine pitfalls of data science.\"}, {\"paperId\": \"17d167c5cafb9121f262597f69b65eded6171f08\", \"abstract\": null}, {\"paperId\": \"2bc30fe676b33bb023c3b12031d163484921f03f\", \"abstract\": \"Machine-Learning and other Artificial Intelligence techniques have nowadays many practical applications in engineering, science or everyday life. In the water industry, there is also a broad scope of potential applications. In this paper, it will be presented a system developed by Canal de Isabel II to identify residential use of water in its different appliances, based on records from precision water meters equipped with pulse emitters. Developed models are based on Support Vector Machines, and Artificial Neural Network paradigms. Training data sets for the models have been extracted from a sample of about 300 residential users in the Region of Madrid (Spain), monitored since 2008. In this time, more than 35 million of water use events have been registered and about 15 million hours of water consumption monitored. Machine-Learning techniques have proved to be an accurate and suitable method for automate this task that otherwise should require a huge number of man-hours of processing by operators.\"}, {\"paperId\": \"08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02\", \"abstract\": \"The Data Science for Local Government project was about understanding how the growth of \\u2018data science\\u2019 is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.\"}, {\"paperId\": \"8df5c0d8fdb7b21c60982b77c4c601ac47e8439f\", \"abstract\": \"In this context, we understand \\u201cartificial intelligence\\u201d as a collective term for technologies and their applications which process potentially very large and heterogeneous data sets using complex methods modelled on human intelligence to arrive at a result which may be used in automated applications. The most important building blocks of AI as part of computer science are sub-symbolic pattern recognition, machine learning, computerized knowledge representation and knowledge processing, which encompasses heuristic search, inference and planning.\"}, {\"paperId\": \"55631a0ef4ec9fc9752011169fcf2af0f724e7c2\", \"abstract\": \"Artificial intelligence (AI), and machine learning in particular, promises lawmakers greater specificity and fewer errors. Algorithmic lawmaking and judging will leverage models built from large stores of data that permit the creation and application of finely tuned rules. AI is therefore regarded as something that will bring about a movement from standards to rules. Drawing on contemporary data science, this Article shows that machine learning is less impressive when the past is unlike the future, as it is whenever new variables appear over time. In the absence of regularities, machine learning loses its advantage and, as a result, looser standards can become superior to rules. We apply this insight to bail and sentencing decisions, as well as familiar corporate and contract law rules. More generally, we show that a Human-AI combination can be superior to AI acting alone. Just as today\\u2019s judges overrule errors and outmoded precedent, tomorrow\\u2019s lawmakers will sensibly overrule AI in legal domains where the challenges of measurement are present. When measurement is straightforward and prediction is accurate, rules will prevail. When empirical limitations such as overfit, Simpson\\u2019s Paradox, and omitted variables make measurement difficult, AI should be trusted less and law should give way to standards.\"}, {\"paperId\": \"a1f77a243862217261aa380d1580d7e75b339bc0\", \"abstract\": \"Crop breeding is as ancient as the invention of cultivation.\\u00a0 In essence, the objective of crop breeding is to improve plant fitness under human cultivation conditions, making crops more productive while maintaining consistency in life cycle and quality. The applications of predictive breeding has been gaining momentum in agricultural industry and public breeding programs for the last decade, in the aftermath of genomic selection being recognized and widely applied for accelerating genetic gain in breeding programs. The massive amounts of data that has been generated by industry and farmers year after year through several decades has finally been recognized as an asset. A wide range of analytical methods such as machine learning, deep learning and artificial intelligence that were initially developed for diverse quantitative disciplines are now being adopted to crop breeding decision making processes. New technologies are currently being developed that would enable integration of data from various domains such as geospatial variables and a multitude of phenotypic responses as well as genetic information, in order to identify, develop and improve crop faster via partial or full automation of the decisions that pertain to variety development. Here we will discuss and summarize efforts from public and private domains for predictive analytics, and its applications to crop breeding and agricultural product development, and provide suggestions for future research.\"}, {\"paperId\": \"cb2840b7b6e0fdb8a964dbe47ae5b56bfa732219\", \"abstract\": \"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human\\u2013AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.\"}, {\"paperId\": \"146a6daa620833c0d7c94e1fb6cb44fb639c440b\", \"abstract\": \"Additive Manufacturing (AM) which is also known as 3D printing technology; is recognized as a new paradigm for manufacturing industry. Additive manufacturing is rapidly expanding across different sectors such as healthcare, electronics, automotive, science and engineering, education, dental, etc. Machine Learning and Big Data are both emerging technologies which are becoming popular and gaining more attention from the industries and academic. Machine Learning is a growing field of Artificial Intelligence (AI) that allows systems to learn from data, identify patterns and make decisions with very little human involvement. On the other hand, Big Data is referred to as datasets whose size is more than the capacity of what a conventional database software tools can capture, store, manage and analyze. Lately, Machine Learning techniques and Big Data Analytics are being applied to various applications of additive manufacturing to monitor building process and enhance decisions making using data generated through different sensors or cameras. This paper explores recent applications of Machine Learning with Big Data in the field of additive manufacturing, for instance, application of machine learning in detecting defect or anomaly during build process in additive manufacturing/3D printing machine.\"}, {\"paperId\": \"0fd8e04c9717409fa23e0d2fc995920a23c933be\", \"abstract\": null}, {\"paperId\": \"a1994d7eb6207aaae93eadb850ad0eafab64077b\", \"abstract\": \"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together \\u2013 each targeting a straightforward prediction task \\u2013 to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.\"}, {\"paperId\": \"a9da560ead28d476733a21f1bf3e7297087565d6\", \"abstract\": \"The computer science technology trend called artificial intelligence (AI) is not new. Both machine learning and deep learning AI applications have recently begun to impact cardiovascular medicine. Scientists working in the AI domain have long recognized the importance of data quality and provenance to AI algorithm efficiency and accuracy. A diverse array of cardiovascular raw data sources of variable quality-electronic medical records, radiological picture archiving and communication systems, laboratory results, omics, etc.-are available to train AI algorithms for predictive modeling of clinical outcomes (in-hospital mortality, acute coronary syndrome risk stratification, etc.), accelerated image interpretation (edge detection, tissue characterization, etc.) and enhanced phenotyping of heterogeneous conditions (heart failure with preserved ejection fraction, hypertension, etc.). A number of software as medical device narrow AI products for cardiac arrhythmia characterization and advanced image deconvolution are now Food and Drug Administration approved, and many others are in the pipeline. Present and future health professionals using AI-infused analytics and wearable devices have 3 critical roles to play in their informed development and ethical application in practice: (1) medical domain experts providing clinical context to computer and data scientists, (2) data stewards assuring the quality, relevance and provenance of data inputs, and (3) real-time and post-hoc interpreters of AI black box solutions and recommendations to patients. The next wave of so-called contextual adaption AI technologies will more closely approximate human decision-making, potentially augmenting cardiologists' real-time performance in emergency rooms, catheterization laboratories, imaging suites, and clinics. However, before such higher order AI technologies are adopted in the clinical setting and by healthcare systems, regulatory agencies, and industry must jointly develop robust AI standards of practice and transparent technology insertion rule sets.\"}, {\"paperId\": \"9cc1f18a6d4e1d44f5fdb09b45db21b1c763b02e\", \"abstract\": \"A vast majority of the archaeological record, globally, is understudied and increasingly threatened by climate change, economic and political instability, and violent conflict. Archaeological data are crucial for understanding the past, and as such, documentation of this information is imperative. The development of machine intelligence approaches (including machine learning, artificial intelligence, and other automated processes) has resulted in massive gains in archaeological knowledge, as such computational methods have expedited the rate of archaeological survey and discovery via remote sensing instruments. Nevertheless, the progression of automated computational approaches is limited by distinct geographic imbalances in where these techniques are developed and applied. Here, I investigate the degree of this disparity and some potential reasons for this imbalance. Analyses from Web of Science and Microsoft Academic searches reveal that there is a substantial difference between the Global North and South in the output of machine intelligence remote sensing archaeology literature. There are also regional imbalances. I argue that one solution is to increase collaborations between research institutions in addition to data sharing efforts.\"}, {\"paperId\": \"0ab98050a152a48f8de5200cc82e773c9b186fb2\", \"abstract\": null}, {\"paperId\": \"a85fc20c760206bbfc1a92842b043b4a85d48627\", \"abstract\": null}, {\"paperId\": \"026730ead96211ee8e954e9df5c220191fa2d329\", \"abstract\": \"Machine Learning is part of Artificial Intelligence that has the ability to make future forecastings based on the previous experience. Methods has been proposed to construct models including machine learning algorithms such as Neural Networks (NN), Support Vector Machines (SVM) and Deep Learning. This paper presents a comparative performance of Machine Learning algorithms for cryptocurrency forecasting. Specifically, this paper concentrates on forecasting of time series data. SVM has several advantages over the other models in forecasting, and previous research revealed that SVM provides a result that is almost or close to actual result yet also improve the accuracy of the result itself. However, recent research has showed that due to small range of samples and data manipulation by inadequate evidence and professional analyzers, overall status and accuracy rate of the forecasting needs to be improved in further studies. Thus, advanced research on the accuracy rate of the forecasted price has to be done.\"}, {\"paperId\": \"e95983d89e2731df95ae67d85a59c68d0eafb4e3\", \"abstract\": \"Abstract The analysis of large amounts of personal data with artificial neural networks for deep learning is the driving technology behind new artificial intelligence (AI) systems for all areas in science and technology. These AI methods have evolved from applications in computer vision, the automated analysis of images, and now include frameworks and methods for analyzing multimodal datasets that combine data from many different source, including biomedical devices, smartphones and common user behavior in cyberspace. For neuroscience, these widening streams of personal data and machine learning methods provide many opportunities for basic data-driven research as well as for developing new tools for diagnostic, predictive and therapeutic applications for disorders of the nervous system. The increasing automation and autonomy of AI systems, however, also creates substantial ethical challenges for basic research and medical applications. Here, scientific and medical opportunities as well ethical challenges are summarized and discussed.\"}, {\"paperId\": \"fdfef52dd3dd09f55d24e6e8e8be3e09fa21bbc0\", \"abstract\": \"Machine learning is an emerging field of artificial intelligence which can be applied to the agriculture sector. It refers to the automated detection of meaningful patterns in a given data. \\u00a0Modern agriculture seeks ways to conserve water, use nutrients and energy more efficiently, and adapt to climate change. \\u00a0Machine learning in agriculture allows for more accurate disease diagnosis and crop disease prediction. This paper briefly introduces what machine learning can do in the agriculture sector.\"}, {\"paperId\": \"596b02e4ee4b414e521564f45fdc0ee39ce7b5a6\", \"abstract\": \"AbstractHigh-impact weather events, such as severe thunderstorms, tornadoes, and hurricanes, cause significant disruptions to infrastructure, property loss, and even fatalities. High-impact events can also positively impact society, such as the impact on savings through renewable energy. Prediction of these events has improved substantially with greater observational capabilities, increased computing power, and better model physics, but there is still significant room for improvement. Artificial intelligence (AI) and data science technologies, specifically machine learning and data mining, bridge the gap between numerical model prediction and real-time guidance by improving accuracy. AI techniques also extract otherwise unavailable information from forecast models by fusing model output with observations to provide additional decision support for forecasters and users. In this work, we demonstrate that applying AI techniques along with a physical understanding of the environment can significantly improve ...\"}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 300, \"next\": 400, \"data\": [{\"paperId\": \"fdfef52dd3dd09f55d24e6e8e8be3e09fa21bbc0\", \"abstract\": \"Machine learning is an emerging field of artificial intelligence which can be applied to the agriculture sector. It refers to the automated detection of meaningful patterns in a given data. \\u00a0Modern agriculture seeks ways to conserve water, use nutrients and energy more efficiently, and adapt to climate change. \\u00a0Machine learning in agriculture allows for more accurate disease diagnosis and crop disease prediction. This paper briefly introduces what machine learning can do in the agriculture sector.\"}, {\"paperId\": \"596b02e4ee4b414e521564f45fdc0ee39ce7b5a6\", \"abstract\": \"AbstractHigh-impact weather events, such as severe thunderstorms, tornadoes, and hurricanes, cause significant disruptions to infrastructure, property loss, and even fatalities. High-impact events can also positively impact society, such as the impact on savings through renewable energy. Prediction of these events has improved substantially with greater observational capabilities, increased computing power, and better model physics, but there is still significant room for improvement. Artificial intelligence (AI) and data science technologies, specifically machine learning and data mining, bridge the gap between numerical model prediction and real-time guidance by improving accuracy. AI techniques also extract otherwise unavailable information from forecast models by fusing model output with observations to provide additional decision support for forecasters and users. In this work, we demonstrate that applying AI techniques along with a physical understanding of the environment can significantly improve ...\"}, {\"paperId\": \"d08f428e16d29e92aa7f0373a05bbc24abe70f64\", \"abstract\": \"Machine learning\\u201d (ML) is a subfield of artificial intelligence. The term applies broadly to a collection of computational algorithms and techniques that train systems from raw data rather than a priori models. ML techniques are now technologically mature enough to be applied to particle accelerators, and we expect that ML will become an increasingly valuable tool to meet new demands for beam energy, brightness, and stability. The intent of this white paper is to provide a high-level introduction to problems in accelerator science and operation where incorporating ML-based approaches may provide significant benefit. We review ML techniques currently being investigated at particle accelerator facilities, and we place specific emphasis on active research efforts and promising exploratory results. We also identify new applications and discuss their feasibility, along with the required data and infrastructure strategies. We conclude with a set of guidelines and recommendations for laboratory managers and administrators, emphasizing the logistical and technological requirements for successfully adopting this technology. This white paper also serves as a summary of the discussion from a recent workshop held at SLAC on ML for particle accelerators [1].\"}, {\"paperId\": \"8dd6676d6e90821c155b403187fd62b33f1f9bf8\", \"abstract\": \"An increasing number of consequential decisions are made automatically by software that employs machine learning, data analytics, and artificial intelligence to discover decision rules using data. The shift to data driven systems exacerbates gaps between traditional governance and oversight processes and the realities of software-driven decision-making. And with more and more software-mediated systems turning to machine learning, data analytics, and artificial intelligence to discover decision rules using data instead of having humans code those rules by hand, this gap can exist even for the software engineers, data scientists, and system operators who design, build, deploy, and manage the machines that mediate our modern lives. Whether algorithms are approving credit applications, selecting travelers for security screening, driving a car, granting and denying visas, or determining the risk profile of an accused or convicted criminal, there is a broad societal interest in ensuring the good governance of these technologies and building accountable algorithms.\"}, {\"paperId\": \"376fc77bb2f4e655e75e137085372f4edc7733fd\", \"abstract\": \"Machine learning has emerged as an important and distinct area of research closely related to and often overlaps with various domains within computer science, computational statistics, artificial intelligence, cognitive science. One can observe connections with these fields at the cognitive level (in terms of theoretical framework), and on methodological levels (drawing from tools and techniques of these fields). The evolution of the field has taken a very directed and operational approach with basic tenet of machine learning being \\u2018teaching computers how to learn from data to make decisions or predictions\\u2019. As we move into systems that increasingly need to exploit data, we find the research in this area getting more application oriented, expansive in scope with loci of research and innovation dispersed across academia, research institutions and industry. It is thus becoming a challenging as well as useful exercise to know the structure and dynamics of this field. The paper is centered on this issue; it tries to capture the intellectual structure of this field and research trends from quantitative and statistical analysis of research publications. Conceptual connections are constructed from linkages among keywords using tools and techniques of Social network Analysis. It also acts as a conceptual framework for the study. Some indications from patent statistics are also drawn to provide some insights of the technological trends.\"}, {\"paperId\": \"4162ade724bf2bb73a1a7511ce908aff78a8b8aa\", \"abstract\": \"Behavioral economics changed the way we think about market participants and revolutionized policy-making by introducing the concept of choice architecture. However, even though effective on the level of a population, interventions from behavioral economics, nudges, are often characterized by weak generalisation as they struggle on the level of individuals. Recent developments in data science, artificial intelligence (AI) and machine learning (ML) have shown ability to alleviate some of the problems of weak generalisation by providing tools and methods that result in models with stronger predictive power. This paper aims to describe how ML and AI can work with behavioral economics to support and augment decision-making and inform policy decisions by designing personalized interventions, assuming that enough personalized traits and psychological variables can be sampled.\"}, {\"paperId\": \"26868bddeda914c7b820bcab008da69871bbc71d\", \"abstract\": \"Machine learning has become an important tool set for artificial intelligence and data science across many fields. A modern machine learning method can be often reduced to a mathematical optimization problem. Among algorithms to solve the optimization problem, gradient descent and its variants like stochastic gradient descent and momentum methods are the most popular ones. The optimization problem induced from classical machine learning methods is often a convex and smooth one, for which gradient descent is guaranteed to solve it efficiently. On the other hand,modern machine learning methods, like deep neural networks, often require solving a non-smooth and non-convex problem. Theoretically, non-convex mathematicaloptimization problems cannot be solved efficiently. However, in practice, gradient descent and its variants can find a global optimum efficiently. These competing factsshow that often there are special structures in the optimization problems that can make gradient descent succeed in practice. This thesis presents technical contributions to fill the gap between theory andpractice on the gradient descent algorithm. The outline of the thesis is as follows. In the first part, we consider applying gradient descent to minimize the empiricalrisk of a neural network. We will show if a multi-layer neural network with smooth activation function is sufficiently wide, then randomly initialized gradient descent can efficiently find a global minimum of the empirical risk. We will also show the same result for the two-layer neural network with Rectified Linear Unit (ReLU) activation function. It is quite surprising that although the objective function of neural networks is non-convex, gradient descent can still find their global minimum. Lastly, we will study structural property of the trajectory induced by the gradient descent algorithm.? In the second part, we assume the label is generated from a two-layer teacher convolutional neural network and we consider using gradient descent to recover the teacher convolutional neural network. We will show that if the input distribution is Gaussian, then gradient descent can recovered a one-hidden-layer convolutional neural network in which both the convolutional weights and the output wights are unknown parameters to be recovered. We will also show that the Gaussian input assumption can be relaxed to a general structural assumption if we only need to recover a single convolutional filter. In the third part, we study conditions under which gradient descent fails. Wewill show gradient descent can take exponential time to optimize a smooth function with the strict saddle point property for which the noise-injected gradient can optimize in polynomial time. While our focus is theoretical, whenever possible, we also present experiments that illustrate our theoretical findings.\"}, {\"paperId\": \"bdefcfdcc33440ef992dbe192c0c0350ba9bbb1a\", \"abstract\": \"1 August Pi i Sunyer Biomedical Research Institute (IDIBAPS), Barcelona, Spain 2 University Pompeu Fabra, Department of Information and Communication Technologies, Barcelona, Spain 3 University of Zagreb School of Medicine, Department of Cardiovascular Diseases, Zagreb, Croatia 4 Joint Research Centre, European Commission, Brussels, Belgium 5 Computer Science Department, Intelligent Data Science and Artificial Intelligence (IDEAI) Research Center, Universitat Politecnica de Catalunya, Barcelona, Spain 6 Wales Heart Research Institute, School of Medicine, Cardiff University, Cardiff, UK 7 ICREA, Barcelona, Spain\"}, {\"paperId\": \"29d1d2adf899a53f1c5a6602f3b756422c710001\", \"abstract\": \"Industry 4.0 is the new industrial stage that is committed to greater automation, connectivity and globalization. The interrelation between the different areas has penetrated the industrial world thanks to the Internet of things and the world of Big Data. This amount of information available in plants is growing increasingly, also aided by the network computing services offered by cloud computing or edge computing. That is why it\\u2019s necessary to carry out complex fusion methods and data analysis using Machine Learning techniques to address specific industrial requirements and needs. The central challenge of industry 4.0 from the perspective of data science is to predict the history within the monitored processes, providing as much information as possible, avoiding them and stave off severe economic losses. This article will show a review of the application of Artificial Intelligence (AI) techniques such as Machine Learning (ML) immersed in multi-agent systems (MAS) in Industry 4.0. For this, a bibliographic search has been carried out in databases recognized as Science Direct, Google Scholar, Scopus or Springer, filtering the investigations from 2018 to the actually. The article concludes by pointing out the possible future lines and the importance of the transition towards the implementation of new technologies for the competitiveness of factories.\"}, {\"paperId\": \"aca335d882f10e6ecacc39ecffe0347a7a383129\", \"abstract\": \"Artificial Intelligence, perhaps better known by its two-letter acronym \\u201cAI,\\u201d has been the fodder of science fiction writing for decades. But the technology underlying AI research has recently found applications in the financial sector \\u2013 in a movement that falls under the banner of \\u201cFintech.\\u201d And the same underlying technology (machine learning and AI) is fueling the spinoff field of \\u201cRegTech,\\u201d to make compliance and regulatory-related activities easier, faster, and more efficient. Like many financial institutions and other market participants, the Commission has made recent and rapid advancements with analytic programs that harness the power of big data (a.k.a \\\"SupTech\\\"). They are driving SEC surveillance programs and allowing innovations in many market risk assessment initiatives. My remarks are intended to highlight many of the promises \\u2013 but also the limitations \\u2013 of machine learning, big data, and AI in market regulation.\"}, {\"paperId\": \"1850cf6e34125184ba447e88df8d6f15fcacf5a3\", \"abstract\": null}, {\"paperId\": \"e75fb552f391037a1a2072804a784db7397df6c6\", \"abstract\": \"Machine Learning and Artificial Intelligence has gained much attention from researchers in healthcare and medical sciences. Today volume, velocity, and variety of healthcare data rapidly increased, therefore there is a need of an efficient machine learning tool which will enhance prediction accuracy in healthcare. The main purpose of this paper is to find the best and most suitable algorithm for prediction and diagnosis of diseases and application of machine learning for heathcare systems. This paper also provides an overview of the data science concepts from data mining technique to machine learning classification algorithms.abstract summarizes, in one paragraph (usually), the major aspects of the entire paper in the following prescribed sequence.\"}, {\"paperId\": \"d5e83274c1b59fd464b9e1a74a436fb061b79ae2\", \"abstract\": \"Conversion of raw data into insights and knowledge requires substantial amounts of effort from data scientists. Despite breathtaking advances in Machine Learning (ML) and Artificial Intelligence (AI), data scientists still spend the majority of their effort in understanding and then preparing the raw data for ML/AI. The effort is often manual and ad hoc, and requires some level of domain knowledge. The complexity of the effort increases dramatically when data diversity, both in form and context, increases. In this paper, we introduce our solution, Augmented Data Science (ADS), towards addressing this \\\"human bottleneck\\\" in creating value from diverse datasets. ADS is a data-driven approach and relies on statistics and ML to extract insights from any data set in a domain-agnostic way to facilitate the data science process. Key features of ADS are the replacement of rudimentary data exploration and processing steps with automation and the augmentation of data scientist judgment with automatically-generated insights. We present building blocks of our end-to-end solution and provide a case study to exemplify its capabilities.\"}, {\"paperId\": \"a66d5be5ab4e05b5197dbca45a9bdcf964ec342b\", \"abstract\": \"This study has the primary goal to analyze the growth of data science through the main search trends. This study was conducted by defining in high level the concept of data science as well as its main components. Supported in those elements, we identified the main trends. We used mainly data from google trends to determine the evolution of search by topics., research area, or simple expressions. It allowed us to reckon that artificial intelligence (AI)suffered a lack of interest until 2012. Then it became an increasingly popular field since 2014. This is due to the progression of machine learning and data science. Results show a cumulative search of data science since 2012.\"}, {\"paperId\": \"1fc228642851ba6c2cb6cc7f8050d2a016145862\", \"abstract\": \"BACKGROUND\\nRapid advances in science challenge the timely adoption of evidence-based care in community settings. To bridge the gap between what is possible and what is practiced, we researched approaches to developing an artificial intelligence (AI) application that can provide real-time patient-specific decision support.\\n\\n\\nMATERIALS AND METHODS\\nThe Oncology Expert Advisor (OEA) was designed to simulate peer-to-peer consultation with three core functions: patient history summarization, treatment options recommendation, and management advisory. Machine-learning algorithms were trained to construct a dynamic summary of patients cancer history and to suggest approved therapy or investigative trial options. All patient data used were retrospectively accrued. Ground truth was established for approximately 1,000 unique patients. The full Medline database of more than 23 million published abstracts was used as the literature corpus.\\n\\n\\nRESULTS\\nOEA's accuracies of searching disparate sources within electronic medical records to extract complex clinical concepts from unstructured text documents varied, with F1 scores of 90%-96% for non-time-dependent concepts (e.g., diagnosis) and F1 scores of 63%-65% for time-dependent concepts (e.g., therapy history timeline). Based on constructed patient profiles, OEA suggests approved therapy options linked to supporting evidence (99.9% recall; 88% precision), and screens for eligible clinical trials on ClinicalTrials.gov (97.9% recall; 96.9% precision).\\n\\n\\nCONCLUSION\\nOur results demonstrated technical feasibility of an AI-powered application to construct longitudinal patient profiles in context and to suggest evidence-based treatment and trial options. Our experience highlighted the necessity of collaboration across clinical and AI domains, and the requirement of clinical expertise throughout the process, from design to training to testing.\\n\\n\\nIMPLICATIONS FOR PRACTICE\\nArtificial intelligence (AI)-powered digital advisors such as the Oncology Expert Advisor have the potential to augment the capacity and update the knowledge base of practicing oncologists. By constructing dynamic patient profiles from disparate data sources and organizing and vetting vast literature for relevance to a specific patient, such AI applications could empower oncologists to consider all therapy options based on the latest scientific evidence for their patients, and help them spend less time on information \\\"hunting and gathering\\\" and more time with the patients. However, realization of this will require not only AI technology maturation but also active participation and leadership by clincial experts.\"}, {\"paperId\": \"1528e92304c8f7a68df6584c389f412ddbd66966\", \"abstract\": \"Machine learning is a science which was found and developed as a subfield of artificial intelligence in the 1950s. The first steps of machine learning goes back to the 1950s but there were no significant researches and developments on this science. However, in the 1990s, the researches on this field restarted, developed and have reached to this day. It is a science that will improve more in the future. The reason behind this development is the difficulty of analysing and processing the rapidly increasing data. Machine learning is based on the principle of finding the best model for the new data among the previous data thanks to this increasing data. Therefore, machine learning researches will go on in parallel with the increasing data. This research includes the history of machine learning, the methods used in machine learning, its application fields, and the researches on this field. The aim of this study is to transmit the knowledge on machine learning, which has become very popular nowadays, and its applications to the researchers.\"}, {\"paperId\": \"49593e6129a150d4a28a783d0ef5e7ac3f55408c\", \"abstract\": \"Future intelligent machines will be more human-friendly and human-like, while offering much higher throughput and automation, thus augmenting our (human) capabilities. Anthropomorphic machine learning is an emerging direction for future development in artificial intelligence (AI) and data science. This revolutionary shift offers human-like abilities to the next generation of machine learning with greater potential for underpinning breakthroughs in technology development as well as in various aspects of everyday life.\"}, {\"paperId\": \"f195ee24de7351421c93faa75b6019198a36ad7e\", \"abstract\": \"Artificial intelligence (AI) already represents a factor for increasing efficiency and productivity in many sectors, and there is a need for expanding its implementation in animal science. There is a growing demand for the development and use of smart devices at the farm level, which would generate enough data, which increases the potential for AI using machine learning algorithms and real-time analysis. Machine learning (ML) is a category of algorithm that allows software to become accurate in predicting outcomes without being explicitly programmed. The essential principle of machine learning is to construct algorithms that can receive input data and use statistical analysis to predict an output. Exploitation of machine learning approaches, by using different training inputs, derived the prediction accuracy of growth and body weight in broiler chickens that ranged from 98 to 99%. Furthermore, a neural network with an accuracy of 100% identified the presence or absence of ascites in broiler chickens, while the support vector machine (SVM) model obtained an accuracy rate of 99.5% in combination with machine vision for the recognition of healthy and bird flu-challenged chickens. Consequently, machine learning algorithms, besides accurate growth prediction of broiler chickens, can successfully contribute to health disorders prediction. It is obvious that machine learning has a great potential for application in the future. This paper analyses machine learning applications in broiler growth and health prediction, and its ability to cope with high inputs of data and non-linearity can successfully replace common methodology.\"}, {\"paperId\": \"96c65928adb47a09744b05ff0df8fd90190b36fd\", \"abstract\": \"Research on the microbiome is an emerging and crucial science that finds many applications in healthcare, food safety, precision agriculture and environmental studies. Huge amounts of DNA from microbial communities are being sequenced and analyzed by scientists interested in extracting meaningful biological information from this big data. Analyzing massive microbiome sequencing datasets, which embed the functions and interactions of thousands of different bacterial, fungal and viral species, is a significant computational challenge. Artificial intelligence has the potential for building predictive models that can provide insights for specific cutting edge applications such as guiding diagnostics and developing personalised treatments, as well as maintaining soil health and fertility. Current machine learning workflows that predict traits of host organisms from their commensal microbiome do not take into account the whole genetic material constituting the microbiome, instead basing the analysis on specific marker genes. In this paper, to the best of our knowledge, we introduce the first machine learning workflow that efficiently performs host phenotype prediction from whole shotgun metagenomes by computing similaritypreserving compact representations of the genetic material. Our workflow enables prediction tasks, such as classification and regression, from Terabytes of raw sequencing data that do not necessitate any pre-prossessing through expensive bioinformatics pipelines. We compare the performance in terms of time, accuracy and uncertainty of predictions for four different classifiers. More precisely, we demonstrate that our ML workflow can efficiently classify real data with high accuracy, using examples from dog and human metagenomic studies, representing a step forward towards real time diagnostics and a potential for cloud applications.\"}, {\"paperId\": \"e38813b35027203e3debfb00a38554379484601e\", \"abstract\": \"Specialists of past generations have accumulated huge and valuable experience of analytics and forecasting in a variety of subject areas. Within the framework of the development of methods of processing scientific data, taking into account the accumulated experience of generations of specialists, it is possible to structure, specify, classify and rank them into flows of smart data for automation of research of various scientific problems by cognitive systems of artificial intelligence. Automated analytics is based on big data. A large amount of data is generated in real time by modeling a scientific experiment. When working with data, it must be processed as efficiently as possible to get the correct output. The main thing is to prepare the training sample correctly. If you select the training data sampling principle correctly, you can scale the task using a more complete set of data. It should be understood that rationing and data preparation is crucial for traditional machine learning. This process significantly affects the choice of the architecture of neural networks used, especially in so-called deep learning, when it is necessary to correctly determine the number of hidden layers in the neural network and the number of artificial neurons in them. One of the main advantages of multilayer neural networks is the simulation of the work of some complex mathematical dependence.\"}, {\"paperId\": \"6d2ddc19d660d956ec24505f3dfc89613066815b\", \"abstract\": \"Predictive Analytics, Soft Computing (SC) and\\u00a0Optimization, Data Mining and Data Science are rapidly becoming some of the most-discussed, perhaps utmost glorified\\u00a0topics in healthcare business. Artificial Intelligence, Machine\\u00a0Learning, Artificial Neural Networks, Fuzzy Logic, Expert\\u00a0 Systems, etc., is well-studied disciplines with a long history of\\u00a0success in many industries. Healthcare can acquire treasured sessions from this prior achievement to startup the efficacy of\\u00a0predictive analytics for refining patient care, chronic disease management, hospital administration and supply chain efficiencies. The prospects that presently occurs for healthcare systems is to state what \\u201cpredictive analytics\\u201d stands for to them and how can it be cast off furthermost excellently to cause\\u00a0further enhancements. In all industries including healthcare, prediction plays a best worthwhile role when that data is passed on as accomplishments. The inclinations to mediate the vital data is in harnessing the power of historical and real-time data with visions from forecasting those data based on the times ahead.\\u00a0Importantly, to best gauge efficacy and value, both the predictor and the intervention must be integrated within the same system and workflow where the trend occurs. A valuable report of the organized publicity and expectation of predictive analytics in healthcare through a blend of psychology, digital technology, and entrepreneurship is available for real-time implementation for the good of the public. Review and evaluation on these disciplines pave ways to\\u00a0open up new arenas envisaging the future trends of Predictive analytics, Data Mining and Science and Soft Computing (SC) in healthcare, stepping strongly into pervasive computing, ambient intelligence, ubiquitous computing and many more automated technical concepts and computing\\u2019s ahead.\"}, {\"paperId\": \"8da320bd6379806a805af60ef6a8b2f98a63bdc9\", \"abstract\": \"The consumer is ultimately the key determinant of success of an organization. The need for an in-depth and objective understanding of the consumers, therefore, in terms of what runs in their minds and hearts for the way they behave and act when they go about making complex purchase decisions cannot be over-emphasized. Borrowed from the disciplines of psychology, economics and sociology, the study of consumer behavior isan interdisciplinary and continually-evolving science since long. Other data-science related areas, such as, Big Data, Artificial Intelligence, Machine Learning, Neural Networks and Internet of Things are also being increasingly adopted to analyze and predict consumer behavior. This paper attempts to explore the significance of Big Data and Artificial Intelligence in empowering consumer research.\"}, {\"paperId\": \"648d66dc7123040161136cf732b9528a0249a4cd\", \"abstract\": \"Interest and awareness of Artificial Intelligence (AI) grows at such a rate that academia and higher education struggle keeping up with the accelerating demand of industry. It is forecast that 75% of enterprise applications will use AI, Machine Learning or Deep Learning technology by 2021, yet university programs commonly place the burden on students to obtain their data science educations through elective coursework spanning multiple disparate departments. Data science requires specific mathematics preparation especially for cybersecurity students whose programs have reduced the requirements for advance mathematics to a bare minimum. To facilitate curricula preparation, and hands-on usage of AI tools, a notebook (\\u201cA Trellis For Novice AI Practitioners\\u201d) was prepared in the R programming language as a first step in introducing computer science and cybersecurity students to the concepts and capabilities of AI. A focus on an intrusion detection data set to mitigate nine common cyber vulnerabilities is used in this analysis. Trellis bridges the theoretical and practical chasm for students by building an ANN network intrusion predictive model from scratch. It serves as a template but also encourages heavy contextual modification, and may be relied upon in the beginning stages of a cybersecurity practitioner's data science activities on a wide variety of data sets in all areas of the discipline.\"}, {\"paperId\": \"7cb612a2dfe05e56a0b5c6583c5dd10f09df2bff\", \"abstract\": \"Artificial intelligence (AI) is an area of computer science that emphasizes the creation of intelligent software or system based on big data information, machine learning and deep learning technologies. The rapid development of science and technology as well as internet communication has enabled AI and big data to gradually apply to many fields of health care. The modern imaging medicine is one of the first areas where AI can play an important role and applications. As cross-sectional imaging, ultrasound (US) is well suitable for AI technology to standardize imaging protocols and improve diagnostic accuracy. This article reviews current AI technology and related clinical applications in the fields of thyroid, breast and liver US.\"}, {\"paperId\": \"2de907dfec39dee53fc1a71aed0a46759679367d\", \"abstract\": null}, {\"paperId\": \"139b2afafdaccba02c983d2f40f257db64860320\", \"abstract\": \"Machine learning, the core of artificial intelligence and data science, is a very active field, with vast applications throughout science and technology. Recently, machine learning techniques have been adopted to tackle intricate quantum many-body problems and phase transitions. In this work, the authors construct exact mappings from exotic quantum states to machine learning network models. This work shows for the first time that the restricted Boltzmann machine can be used to study both symmetry-protected topological phases and intrinsic topological order. The exact results are expected to provide a substantial boost to the field of machine learning of phases of matter.\"}, {\"paperId\": \"6751c8928116514be5de2d1be4ef64f4b2fb3d66\", \"abstract\": null}, {\"paperId\": \"23e0f0e9d308856222dea648fb604c1bedd61f18\", \"abstract\": \"Pain and pain chronification are incompletely understood and unresolved medical problems that continue to have a high prevalence. It has been accepted that pain is a complex phenomenon. Contemporary methods of computational science can use complex clinical and experimental data to better understand the complexity of pain. Among data science techniques, machine learning is referred to as a set of methods (Fig. 1) that can automatically detect patterns in data and then use the uncovered patterns to predict or classify future data, to observe structures such as subgroups in the data, or to extract information from the data suitable to derive new knowledge. Together with (bio)statistics, artificial intelligence and machine learning aim at learning from data. Although statistics can be regarded as a branch of mathematics, artificial intelligence and machine learning have developed from computer science (Ref. 58; see also https://en.wikipedia. org/wiki/Artificial_intelligence). The initial definition of artificial intelligence originates from Alan Turing who proposed an experiment where 2 players, who can either be human or artificial, try to convince a human third player, that they are also humans. The test of artificial intelligence is passed if the third player cannot tell who is the machine. Important steps in the development of machine learning were the first creation of the computer learning program, which was a checker game, and the first neural network called the perceptron. Statistics uses mathematical equations to model probability relationships between data variables, whereas machine learning learns from data without the necessity of previous knowledge. It aims at optimization and performance of an algorithm rather than on the analysis of the probabilities of observations, given a known underlying data distribution. Nevertheless, both machine learning and statistics techniques are working in concert for pattern recognition, knowledge discovery, and data mining and share partly the same methods such as regression, which is used widely in statistics but is also considered as a classification method in machine learning (Fig. 1). In the present research context, when provided with painrelated data, machine-learned methods are able to learn a mapping of complex features to a known class, that is, to predict a pain phenotype class from a complex pattern of acquired parameters. After the machine has learned the prediction of a pain-related phenotype, the algorithm can subsequently be used on new data from which the class membership of a novel yet unclassified subject can be identified. However, machine learning methods can also be used for pattern recognition in complex pain-related data to reveal traces of an underlying molecular background or for knowledge discovery in big data in a drug discovery or repurposing context. The increasing use of contemporary methods of computational science is reflected in the rising number of reports using machine learning for pain research (Table 1). This review is focused on machine-learned technologies applied to general pain research that allow one to analyze and predict pain phenotypes and to obtain knowledge from experimental and clinical pain-related data.\"}, {\"paperId\": \"8c0da7dfefe023fe69730e37c74ebe5e4ed0e831\", \"abstract\": \"This paper outlines some of the possible advancements for the technosignatures searches using the new methods currently rapidly developing in computer science, such as machine learning and deep learning. It also showcases a couple of case studies of large research programs where such methods have been already successfully implemented with notable results. We consider that the availability of data from all sky, all the time observations paired with the latest developments in computational capabilities and algorithms currently used in artificial intelligence, including automation, will spur an unprecedented development of the technosignatures search efforts.\"}, {\"paperId\": \"b4999584fc04fb72f7af0efb87948565452fd816\", \"abstract\": \"The twenty-five papers in this special cluster issue focus on machine learning applications in electromagnetics. The terms \\u201cmachine learning\\u201d and \\u201cartificial intelligence\\u201d were coined in the mid-1950s, but their mathematical foundations were rooted many decades earlier. While the term \\u201cartificial intelligence\\u201d (AI for short) has a broader context encompassing many different domains from neuroscience to algorithm development in computer science, the term \\u201cmachine learning\\u201d (ML for short) focuses on the practical aspect of AI, i.e., applying mathematics to solve unique problems and to teach them to machines. ML methods can focus on creating suitable algorithms to solve novel problems or to automate existing solutions mainly by leveraging vast amounts of data. n this special cluster issue, we are exploring the ingenuity of the researchers for applications of ML methods in electromagnetics, antennas, and propagation\"}, {\"paperId\": \"a4a2df59b966f9eb8d14832637bf49b771843355\", \"abstract\": null}, {\"paperId\": \"3e50f104327c34f3ed5b50ef1b4dcc16d086f6e6\", \"abstract\": \"Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:\"}, {\"paperId\": \"18e67ce6a4eaf6cc95b575443aff886df263f635\", \"abstract\": null}, {\"paperId\": \"a4a6b848bd027fe2c601f9da9435906252eb889d\", \"abstract\": null}, {\"paperId\": \"f031fe645728d6fd36a9a36232cf19a846e9db73\", \"abstract\": \"Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.\"}, {\"paperId\": \"b6db7a5e02ef563d8c8cd010e6d6436be8618adf\", \"abstract\": \"Recent advances in artificial intelligence and computer science can be used by social scientists in their study of groups and teams. Here, we explain how developments in machine learning and simulations with artificially intelligent agents can help group and team scholars to overcome two major problems they face when studying group dynamics. First, because empirical research on groups relies on manual coding, it is hard to study groups in large numbers (the scaling problem). Second, conventional statistical methods in behavioral science often fail to capture the nonlinear interaction dynamics occurring in small groups (the dynamics problem). Machine learning helps to address the scaling problem, as massive computing power can be harnessed to multiply manual codings of group interactions. Computer simulations with artificially intelligent agents help to address the dynamics problem by implementing social psychological theory in data-generating algorithms that allow for sophisticated statements and tests of theory. We describe an ongoing research project aimed at computational analysis of virtual software development teams.\"}, {\"paperId\": \"a34c03c67f445db829fc60708a853479f9d63684\", \"abstract\": \"Motivated by examples of machine learning use in genomics, drug discovery, and materials science, we develop a multi-stage combinatorial model of artificial intelligence (AI) -aided innovation. The innovator can utilize AI as a tool for drawing on existing knowledge in the form of data on past successes and failures to produce a prediction model or map of the combinatorial search space. Modeling innovation as a multi-stage search process, we explore how improvements in AI could affect the productivity of the discovery pipeline in combinatorialtype research problems by allowing improved prioritization of the leads that flow through that pipeline. Furthermore, we show how enhanced prediction can increase or decrease the demand for downstream testing, depending on the type of innovation. Finally, we examine the role of data generation as an alternative source of spillovers in sustaining economic growth. \\u2217We gratefully acknowledge financial support from Science Foundation Ireland, Social Sciences Research Council of Canada, Centre for Innovation and Entrepreneurship at Rotman School of Management, and the Whitaker Institute for Innovation and Societal Development, NUI Galway. All errors are our own. c \\u00a9 2019 Ajay Agrawal, John McHale, and Alexander Oettl.\"}, {\"paperId\": \"b0b83f66bfcff01f78d3fb80df9e9e36714f8d19\", \"abstract\": \"Recent advances in computing power have enabled the application of machine learning (ML) across all areas of science. A step change from a data-rich landscape to one where new hypotheses, relationships, and knowledge is emerging as a result. While ML is related to artificial intelligence (AI), they are not the same. ML is a branch of AI involving the application of statistical algorithms to enable a system to learn. Learning can involve data interpretation, identification of patterns and decision making. However, application and acceptance of ML within environmental toxicology, and more specifically for our viewpoint, environmental risk assessment (ERA), remains low. ML is an example of a disruptive research technology, which is urgently needed to cope with the complexity and scale of work required.\"}, {\"paperId\": \"9e2db86bdf774974982ff1bcb92ce24444cfe9a0\", \"abstract\": \"n one of Dr. Diane Skiba\\u2019s last columns, she described the \\u201cinvisible health care provider\\u201d as a way to illustrate health care made possible by transforming big data collected from personal and medical devices into actionable information using artificial intelligence (AI; Skiba, 2018). This column presents basic information about AI and identifies opportunities with AI and associated risks. AI is the science of using computers to mimic human abilities (Nevala, 2017): speech, sensory abilities, movement, and so much more. Machine learning is a subset of AI; the computer uses a large training set of data to look for relationships and later applies the relationships to a new set of data to make predicted outcomes (Nevala, 2017). AI and machine learning were first discussed in research literature in the 1970s; however, because of limitations in computing power, few tangible advances in technology occurred until recently (Kulikowski, 2019). Now, AI is embedded in the human experience without our realizing it. For example, most email and texting applications suggest word choices, mimicking the speech patterns of their human users. Online shopping sites now suggest buying itemsbased on past purchases. These advanced technologies work in the background and have the potential to change human society in the future (Anderson, Rainie, & Luchsinger, 2018). But what does this mean for health care and nursing? In health care, the promise of AI includes the discovery of disease etiologies; the ability to select a treatment option for patients based on predicted outcomes; and calculation of disease risk based on genetic, environmental, and social factors (Galimova, Buzaev, Ramilevich, Yuldybaev, & Shaykhulova, 2019). The reality is more complicated. In order to use the vast amount of data created from personal and medical devices, the complexity of aligning of data must be addressed because data are different in terms of what the Vendome Group (2015) calls the 6Vs: volume (amount of data), variety (different structures of data), veracity (the trustworthiness of the data), velocity (frequency of data coming into the system), visualization (presentation of outcomes in an actionable manner), and value (usefulness in improving health care outcomes). In addition, when new data are quite different from the training set, the algorithm produced from\"}, {\"paperId\": \"1755653f969996f83ccbd8f3178dc0385c8007d1\", \"abstract\": null}, {\"paperId\": \"4b8c40fb52cfc6f86c10d5b1998c47704315fa9d\", \"abstract\": \"Artificial Intelligence is an interdisciplinary field, and formed about 60 years ago as an interaction between mathematical methods, computer science, psychology, and linguistics. Artificial Intelligence is an experimental science and today features a number of internally designed theoretical methods: knowledge representation, modeling of reasoning and behavior, textual analysis, and data mining. Within the framework of Artificial Intelligence, novel scientific domains have arisen: non-monotonic logic, description logic, heuristic programming, expert systems, and knowledge-based software engineering. Increasing interest in Artificial Intelligence in recent years is related to the development of promising new technologies based on specific methods like knowledge discovery (or machine learning), natural language processing, autonomous unmanned intelligent systems, and hybrid human-machine intelligence.\"}, {\"paperId\": \"2fb90f9cf39ade7e989c3af163ed2b50d33bbf9d\", \"abstract\": \"The field of artificial intelligence is driven by the goal to provide machines with human-like intelligence. However modern science is currently facing problems with high complexity that cannot be solved by humans in the same timescale as by machines. Therefore there is a demand on automation of complex tasks. To identify the category of tasks which can be performed by machines in the domain of optics measurements and correction on the Large Hadron Collider (LHC) is one of the central research subjects of this thesis. The application of machine learning methods and concepts of artificial intelligence can be found in various industry and scientific branches. In High Energy Physics these concepts are mostly used in offline analysis of experiments data and to perform regression tasks. In Accelerator Physics the machine learning approach has not found a wide application yet. Therefore potential tasks for machine learning solutions can be specified in this domain. The appropriate methods and their suitability for given requirements are to be investigated. The general question of this thesis is to identify the opportunities to apply machine learning methods to find and correct the errors in LHC optics and also to speed up beam measurements.\"}, {\"paperId\": \"aa6f2ee57d2e5b385f7993eccfb2af5347f26ea9\", \"abstract\": \"Machine learning, the core of artificial intelligence and big data science, is one of today's most rapidly growing interdisciplinary fields. Recently, machine learning tools and techniques have been adopted to tackle intricate quantum many-body problems. In this Letter, we introduce machine learning techniques to the detection of quantum nonlocality in many-body systems, with a focus on the restricted-Boltzmann-machine (RBM) architecture. Using reinforcement learning, we demonstrate that RBM is capable of finding the maximum quantum violations of multipartite Bell inequalities with given measurement settings. Our results build a novel bridge between computer-science-based machine learning and quantum many-body nonlocality, which will benefit future studies in both areas.\"}, {\"paperId\": \"2026362d245773134b01d5059120b858002d8861\", \"abstract\": \"Automatic text summarization has played a critical role in helping people obtain key information from increasing huge data with the advantaged development of technology. In the past, few literatures are related to solve the problem of generating titles (short summaries) by using artificial intelligence (AI). The purpose of this study is that we proposed an AI approach for automatic text summarization. We developed an AI text summarization system architecture with three models, namely, statistical model, machine learning model, and deep learning model as well as evaluating the performance of three models. Essay titles and essay abstracts are used to train artificial intelligence deep learning model to generate the candidate titles and evaluated by ROUGE for performance evaluation. The contribution of this paper is that we proposed an AI automatic text summarization system by applying deep learning to generate short summaries from the titles and abstracts of the Web of Science (WOS) database.\"}, {\"paperId\": \"9bef42e3aeee5317646c2442ebec659e2d25fe2c\", \"abstract\": \"Since 2010, machine learning based predictive techniques, and more specifically deep learning neural networks, have achieved spectacular performances in the fields of image recognition or automatic translation, under the umbrella term of \\u201cArtificial Intelligence\\u201d. But their filiation to this field of research is not straightforward. In the tumultuous history of AI, learning techniques using so-called \\\"connectionist\\\" neural networks have long been mocked and ostracized by the \\\"symbolic\\\" movement. This article retraces the history of artificial intelligence through the lens of the tension between symbolic and connectionist approaches. From a social history of science and technology perspective, it seeks to highlight how researchers, relying on the availability of massive data and the multiplication of computing power have undertaken to reformulate the symbolic AI project by reviving the spirit of adaptive and inductive machines dating back from the era of cybernetics.\"}, {\"paperId\": \"3f5b6c78d16970d9736aa199ad6840fd789507f4\", \"abstract\": \"Complex Machine Learning (ML) models can be effective at analyzing large amounts of data and driving business value. However, these models can be nonintuitive, their parameters meaningless, their potential biases difficult to detect and even harder to mitigate, and their predictions and decisions difficult to explain. Lenders, regulators, and customers need explainable models for automating credit decisions. Lack of algorithmic transparency is a broad concern beyond lending, which has led to much interest in \\u201cexplainable artificial intelligence\\u201d [1]. This paper discusses a model family which warrants explainability and transparency by design: the Transparent Generalized Additive Model Tree (TGAMT). Many credit risk models used in the US and internationally belong to this family. Today, these credit scores are developed painstakingly by teams of data scientists and credit risk experts in a tedious interplay of \\u201cart and science\\u201d in order to simultaneously achieve high predictive performance and intuitive explanations of how the scores are arrived at. The main contribution of this paper is to automate the learning of TGAMT models. We also report benchmark results indicating that TGAMT\\u2019s achieve strong predictive performance similar to complex ML models while being more explanation-friendly.\"}, {\"paperId\": \"0c8172b117efed6794c23d83d8190ab3f8a85240\", \"abstract\": null}, {\"paperId\": \"b51df739d8e675a6dfbeebdbd98ff48269290984\", \"abstract\": null}, {\"paperId\": \"335dfbd8f4c60e44ade648280bddb4407a7c90c8\", \"abstract\": null}, {\"paperId\": \"099c8600e4238af80c17f1478cb648a5750e2e5e\", \"abstract\": \"This paper addresses the rise of machine learning for big data analytics. First, machine learning and several terms related to machine learning are defined and explained in details and these terms include artificial intelligence, data mining, data science, data analytics and knowledge discovery, statistics and Business Intelligence. These definitions will show how these terms are inter-related to each other. Then, the definition of big data is outlined based on three terms: Volume, Velocity and Variety. Implementing a good big data strategy is very crucial in order to guarantee the success of applying machine learning for learning big data. As a result, the trending in Big Data is also illustrated and defined based on the landscape of big data; Infrastructure, Analytics, Applications, Cross-Infrastructures/Analytics, Open Sources, Data Sources and API, Incubators and Schools. This paper also addresses some of the open source facilities that are available for public in order to ensure that large scale of machine learning application can be realized. Finally, in conclusion, the big trend over the last few months in Big Data analytics has been the increasing focus on artificial intelligence to help analyze massive amounts of data and derive predictive insights. AI/machine learning is now precipitating a trend towards the emergence of the application layer of Big Data. The combination of Big Data and AI will drive incredible innovation across pretty much every industry. From that perspective, the Big Data opportunity is probably even bigger than people thought.\"}, {\"paperId\": \"ec3a65054206242a45f8a9f071a6d3671db8660b\", \"abstract\": \"The number of machine learning, artificial intelligence or data science related software engineering projects using Agile methodology is increasing. However, there are very few studies on how such projects work in practice. In this paper, we analyze project issues tracking data taken from Scrum (a popular tool for Agile) for several machine learning projects. We compare this data with corresponding data from non-machine learning projects, in an attempt to analyze how machine learning projects are executed differently from normal software engineering projects. On analysis, we find that machine learning project issues use different kinds of words to describe issues, have higher number of exploratory or research oriented tasks as compared to implementation tasks, and have a higher number of issues in the product backlog after each sprint, denoting that it is more difficult to estimate the duration of machine learning project related tasks in advance. After analyzing this data, we propose a few ways in which Agile machine learning projects can be better logged and executed, given their differences with normal software engineering projects.\"}, {\"paperId\": \"acf24e42a4253c89d047b4648838cbff1cf42fc2\", \"abstract\": \"Lately there has been an increase in the number of Machine Learning (ML) and Artificial Intelligence (AI) applications ranging from recommendation systems to face to speech recognition. At the helm of the advent of deep learning is the proliferation of data from diverse data sources ranging from Internet-of-Things (IoT) devices to self-driving automobiles. Tapping into this unlimited reservoir of information presents the problem of finding quality data out of a myriad of irrelevant ones, which to this day, has been a significant issue in data science with a direct ramification of this being the inability to generate quality ML models for useful predictive analysis. Edge computing has been deemed a solution to some of issues such as privacy, security, data silos and latency, as it ventures to bring cloud computing services closer to end-nodes. A new form of edge computing known as edge-AI attempts to bring ML, AI, and predictive analytics services closer to the data source (end devices). In this paper, we investigate an approach to bring edge-AI to end-nodes through a shared machine learning model powered by the blockchain technology and a federated learning framework called iFLBC edge. Our approach addresses the issue of the scarcity of relevant data by devising a mechanism known as the Proof of Common Interest (PoCI) to sieve out relevant data from irrelevant ones. The relevant data is trained on a model, which is then aggregated along with other models to generate a shared model that is stored on the blockchain. The aggregated model is downloaded by members of the network which they can utilize for the provision of edge intelligence to end-users. This way, AI can be more ubiquitous as members of the iFLBC network can provide intelligence services to end-users.\"}, {\"paperId\": \"219fadaa10b956b47d2012783f3b3428170f520c\", \"abstract\": \"Data Science and advanced data analytics are technologies that enable the development of low cost solutions with a high degree of customization. Based on that, this article presents a leakage detection system in a slurry pipeline using a combination of machine learning techniques. The techniques used to detect leakage were based on artificial intelligence, a machine learning model for the energy balance of the pipe combined with an anomaly detection technique approach. The system predicts energy at one point of the pipe based on another point, in order to infer if there is loss of energy (leakage) on it. Although the machine learning model used is a simple parametric linear regression and this technique is well-known in the artificial intelligence domain, its competitive differential is the use of an open source machine learning platform to implement them, which allows clients to have a customized model instead of using costly instrumentation with embedded systems. This work was fully implemented in a ore tailings pipeline of one of the biggest Brazilian iron ore companies. It has already detected several real leakage occurrences, greater than 70 m3/h with only three to five leak false alarms per month. Based on these results, this solution can be considered as an alternative solution for leak detection in short length pipelines, especially for the ones that transport iron ore tailings. Although it can detect leakage between a pipe sections, it cannot detect the exact point of the leak that motivates further development, such the use of wavelet package technique.\"}, {\"paperId\": \"7b543533a0092a5bf6c7c01c9d58ea338db5bfd2\", \"abstract\": \"Machine learning (ML) is a subfield of artificial intelligence. The term applies broadly to a collection of computational algorithms and techniques that train systems from raw data rather than a priori models. ML techniques are now technologically mature enough to be applied to particle accelerators, and we expect that ML will become an increasingly valuable tool to meet new demands for beam energy, brightness, and stability. The intent of this white paper is to provide a high-level introduction to problems in accelerator science and operation where incorporating ML-based approaches may provide significant benefit. We review ML techniques currently being investigated at particle accelerator facilities, and we place specific emphasis on active research efforts and promising exploratory results. We also identify new applications and discuss their feasibility, along with the required data and infrastructure strategies. We conclude with a set of guidelines and recommendations for laboratory managers and administrators, emphasizing the logistical and technological requirements for successfully adopting this technology. This white paper also serves as a summary of the discussion from a recent workshop held at SLAC on ML for particle accelerators.\"}, {\"paperId\": \"fbd31ec3b48765ce1f05265af08d5e85bb243c92\", \"abstract\": \"The name \\u2018machine learning\\u2019 was coined in 1959 [1], while the most widely quoted formal definition\\u2014\\u2018A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks inT, as measured by P, improves with experience E\\u2019\\u2014was given in the first textbook about machine learning by T. Mitchell in 1997 [2]. Roughly speaking, machine learning aims to enable computers to improve performance by experience. As experience usually appear as data examples, the main focus of machine learning is actually about the study and construction of learning algorithms that are able to build predictive or descriptive models from data. With the increasing demand of computerized data analysis, machine learning becomes more and more important, and stirs up the current artificial intelligence (AI) boom. To reflect the state-of-the-art research progress in the field of machine learning in China and beyond, this special section of the National Science Review presents several timely technical reviews and perspectives, along with a research highlight and an interview. In \\u2018Learning representations on graphs\\u2019, Zhu highlights recent effort about learning with network data that are typically represented as graphs. In the perspective \\u2018Model-driven deep learning\\u2019, Xu and Sun present their recent study about trying to design neural network topology with theoretical foundations, and make the network structure explainable and predictable. In the perspective \\u2018Deep learning for natural language processing\\u2019, Li summarizes the five major tasks in natural language processing, and discusses the advantages and challenges of deep learning. Causality plays an important role in explanation, prediction, decision making, etc., and it is desirable to learn causal knowledge from data. In the third perspective, Zhang et al. summarize their recent progress along this direction. Zhang and Yang offer an overview of multi-task learning, which aims to improve the performance of multiple related learning tasks by leveraging useful information among them. Due to the high cost of data-labeling process, in many tasks, it is desirable to do weakly supervised learning. Zhou provides an introduction to this direction. The special topic ends with an interview with T. Dietterich, the former president of the Association for the Advancement of Artificial Intelligence (AAAI) (the most prestigious association in thefieldofAI) and the foundingpresident of the International Machine Learning Society, about exciting recent advances and technical challenges ofmachine learning, aswell as its big impact on the world.\"}, {\"paperId\": \"4439df82a8a2e087d4456dfffca59c7577353be1\", \"abstract\": \"Unauthorized reproduction of this article is prohibited 1004 www.pccmjournal.org October 2018 \\u2022 Volume 19 \\u2022 Number 10 Our patients are data (1). Data science, derives information from data using many methods, computer tools, and algorithms to acquire, store, transmit, clean, transform, and analyze data (2, 3). Prediction research develops predictive models, validates their performance, and studies their clinical impact (4). Machine learning (ML), a growing discipline, offers a new paradigm for accelerating medical research (5, 6). ML algorithms are computer programs designed to automatically improve (learn) from experience with more data. ML is a natural extension of statistics and encompasses a spectrum of related algorithms\\u2014from logistic regression (LR) to deep neural networks (7). The fewer assumptions, the more complex, and automated are the models. Moving from human \\u201cexpert\\u201d knowledge or a simplifying mathematical formulation (e.g., linear or Gaussian forms) guiding \\u201cfeature\\u201d development and model structure to ensembles of modules that derive representations from that data yields greater algorithmic automation\\u2014and less human intervention\\u2014offering accelerated medical research and improved individual outcomes. Many think the best is yet to come. Others warn of a big data hype cycle at the point of inflated expectations, often followed by a chasm of disillusionment before the value of data science can be known (8). To avoid this chasm of disillusionment early, adopters have a responsibility to be conscientious and meticulous about the description of these new, paradigm shifting techniques. This requires clarity about what is being done and why, understandable, transparent method descriptions without hype. Prediction research promises much as an adjunct to clinical practice. Understanding ML will require clinicians to learn new data analytic techniques and new model assessment methods. Before accepting ML research as useful or generalizable, evaluation requires the same careful attention to objectives, methods, results, and interpretation as for any article. Kamaleswaran et al (9), published in this issue of Pediatric Critical Care Medicine, are to be congratulated for joining the zeitgeist and attempting to apply ML to ICU data. We will be worthy of congratulation when we are able to understand and evaluate such publications. Artificial intelligence (AI) has been through the disillusionment chasm several times. First defined in 1956, AI meant the ability of machines to mimic human intelligent behavior, which implies generalizable learning and directed action. AI lost credibility in the scientific community due to lack of computational power, lack of data, and hype. To be artificially intelligent, computer systems (think autonomous cars) must rapidly and accurately acquire data about the environment, curate (acquire, clean, transform, store) it, and analyze that data, in a timely fashion, often by ML predictive analytics as a part of an AI system. Subsequently, this analysis must direct action, and ideally the AI system learns from the consequences of these actions by acquiring more data. Clear language is important to understand ML and AI articles. The meaning of the term \\u201cAI\\u201d has been blurred to apply to almost anything that has a part in AI such as ML algorithms trained to specific tasks which, as used in this article (9), are a far cry from mimicking human intelligent behavior. Misleading claims will confuse the reader. In this article (9), AI, as the title states, was not used to identify the physiomarkers which were manually selected. What the authors did was explore the ability of three ML algorithms to identify predetermined sepsis targets. Confusion about what AI is and can do will lead to misunderstanding and disillusionment. ML algorithms, as in the study by Kamaleswaran et al (9) (LR, random forest, and convolutional neural networks [CNNs]), enable software to improve performance (learn) with more data\\u2014but they are data hungry (4, 5, 10). Lack of sufficient data led to disillusionment in the past\\u2014a solvable problem that still hinders us (11). Use of limited populations, with sparse targets, leads to imperfectly trained models without generalizability. Any model with data-derived coefficients, whether simple LR or neural networks, must be evaluated on datasets that are independent of development data. With sufficient sample size, standard practice randomly partitions data into training and test sets. With small sample size, partitioning results in inadequately small test sets with few targets, making conclusions impossible. A properly performed k-fold cross-validation mitigates issues arising from small sample size, but aggregate performance is measured from all k models, not merely the best performing. In classification problems (dichotomous e.g., predicting survival or sepsis), if the test set (or folds of a cross-validation) is too small for any one hypothesis, the performance of empirically derived models will have high uncertainty. For this article (9), the reader must ask, are 20 targets across a fivefold crossvalidation in 496 patients sufficient? *See also p. e495.\"}, {\"paperId\": \"829797080bf78e70a98d626a185b83c4cc5a043e\", \"abstract\": \"Abstract As demand for data scientists in audit/Governance, risk management and compliance (GRC), and industry in general, outpaces supply, data science in a box\\u2014packaged analytics powered by artificial intelligence (AI) and guided machine learning\\u2014can bridge the gap to bring analytics to every major enterprise. Packaged analytics harness the power of AI and machine learning technologies to help operations, finance executives, and GRC professionals do their jobs better; optimize business processes; and deliver actionable insights for better decision making. This article will explore real-world case studies of how companies have used packaged analytics to achieve process improvements, better oversight over financial spend, and significant return on investment. It is a guide to internal auditors and their GRC counterparts on what is available and suggests they can partner or use the products independently and significantly contribute to their companies.\"}, {\"paperId\": \"2a1feda9083c0a10f161ad1a7e55b2ba06054bbd\", \"abstract\": \"Heart disease and machine learning are the two different words where one is related to medical field and another one to artificial intelligence. In medical filed most of them are facing the problems with the heart disease and machine learning is developing area in computer science. Heart disease is general called cardiac disease where it gives the more data or information, it is to be collected to give the reports for the patients and the machine learning also requires the data for predicting and to solve the problems. Machine learning techniques are used in prediction of heart diseases where it gives the faster prediction with less computation time and better accuracy to progress their health. Heart disease prediction requires lot of data for predicting and in cloud computing also we have more data and the data available in cloud it is difficult to analyze. So we use machine learning algorithms or techniques to predict the heart disease and the in the similar way we can apply these algorithms or techniques to predict or analyze the data that is available in cloud. In this paper we are going to use machine learning algorithms called Backpropagation Algorithm and later we use optimization algorithm later. Backpropagation algorithm deals with the artificial neural networks. Backpropagation is a method used to calculate the error contribution of each neuron after a batch of data (in image recognition, multiple images) is processed. This is used by an enveloping optimization algorithm to adjust the weight of each neuron, completing the learning process for that case. Machine learning algorithms and techniques are used for recognize the intensity of risk issues in humans and it helps the patients to take safety measures in well advances to save the patient\\u2019s life.\\u00a0\"}, {\"paperId\": \"158383399d32723dc2de8cf67310fea6e78242fd\", \"abstract\": \"In July 2017, the Chinese government issued a guideline on developing artificial intelligence (AI), namely, the \\u2018New-Generation Artificial Intelligence Development Plan\\u2019, through 2030 to the public, setting a goal of becoming a global innovation center in this field by 2030. According to the development plan, breakthroughs should be made in basic theories of AI in terms of big data intelligence, cross-media computing, human-machine hybrid intelligence, collective intelligence, autonomous unmanned decisionmaking, brain-like computing, and quantum intelligent computing. The next-generation AI would be never-ending (self) learning from data and experience, intuitive reasoning and adaptation (Pan, 2016, 2017). From the perspective of overcoming the limitation of existing AI, it is generally recognized that the crossdisciplinary collaboration is a key for AI having real impact on the world. Thanks for the efforts from researchers in computer science, statistics, robotics, and psychiatry, the topics in this special issue consist mainly of five subjects: (1) fundamental issues in AI such as interpretable deep learning and unsupervised learning (i.e., domain adaptation and generative adversarial learning); (2) brain-like learning such as spiking neural network and memory-augmented reasoning; (3) human-in-the-loop learning such as crowdsourcing design and digital brain with crowd power; (4) creative applications such as social chatbots (i.e., XiaoICe) and automatic speech recognition; (5) Dr. Raj Reddy from CMU shared his view on the new-generation AI, Prof. Bin Yu from UC Berkeley advocated that AI should use statistical concepts through humanmachine collaboration, and researchers from the Chinese Academy of Sciences surveyed the acceleration of deep neural networks. All of interview, perspective, survey, and research papers target rethinking the appropriate ways for a general scenario or a specific application. In an interview, Dr. Raj Reddy shared his views on the new-generation AI and detailed the idea of cognition amplifiers and guardian angles (FITEE editorial staff, 2018). Yu and Kumbier (2018) discussed how humanmachine collaboration can be approached in AI through the statistical concepts of population, question of interest, representativeness of training data, and scrutiny of results (PQRS). The PQRS workflow provides a conceptual framework for integrating statistical ideas with human input into AI products and research. Shum et al. (2018) discussed the issue of social chatbots. The design of social chatbots must focus on user engagement and take both intellectual quotient (IQ) and emotional quotient (EQ) into account. Using XiaoIce as an illustrative example, authors introduced key technologies in building social chatbots from core chat to visual sense to skills. Zhang and Zhu (2018) reviewed recent studies in emerging directions of understanding neuralnetwork representations and learning interpretable neural networks. They revisited visualization of convolutional neural network (CNN) representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling CNN representations, learning of Editorial: Frontiers of Information Technology & Electronic Engineering www.jzus.zju.edu.cn; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn\"}, {\"paperId\": \"1c7d373b273b512692516ddf7bfd208c722199d7\", \"abstract\": \"Artificial intelligence (AI) is the new kid on the block. Every doctor and her/his medical student is talking about it. It seems like the answer to all of our problems. How will we overcome antibiotic resistance? AI will figure it out. What\\u00a0is on the X-ray? AI will know. Why is your partner mad at you? AI should be great for that. But what is AI exactly, and what will be its impact in sports medicine?\\n\\nAI is the theory and development of computer systems which are able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making and translation between languages.1 In order for a machine to become intelligent, it needs to learn. Enter machine learning.\\n\\nMachine learning is a branch of AI based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.2 \\n\\nMachine Learning is not a new concept in computer science circles but was popularised by Dr Geoffrey Hinton, a computer scientist from Toronto Ontario, who brought the technology into the limelight with his work on ImageNet, a computer program (called a neural network) that was able to identify the contents \\u2026\"}, {\"paperId\": \"106f6d9fb2c85b19593dc2a1fa4535d2c106fa1a\", \"abstract\": \"We are entering the era of artificial intelligence and big data, and thus, systems are becoming more intelligent with performance even to a human level in limited applications. We also connect every part of the globe with ultrahigh-speed Internet to share information in almost real time, and innovatively make changes on the life style of people. At the core of artificial intelligence, machine learning algorithms contribute to semiautomatically or automatically develop highly intelligent systems by overcoming existing difficulties for various fields including applications on engineering, business, science, and pure art.\"}, {\"paperId\": \"0c252c3591bfd2de0f75b6c2fd25eac61efa8f9c\", \"abstract\": null}, {\"paperId\": \"1fe3e30a08cd16aee578ebfb189dc94722ec0d25\", \"abstract\": \"Modern machine learning methods are increasingly powerful and opaque. This opaqueness is a concern across a variety of domains in which algorithms are making important decisions that should be scrutable. The explainabilty of machine learning systems is therefore of increasing interest. We propose an explanation-byexamples approach that builds on our recent research in Bayesian teaching in which we aim to select a small subset of the data that would lead the learner to similar conclusions as the entire dataset. We discuss this approach, explicating several key advantages. First, the ability to cover any model with a probabilistic interpretation including supervised, unsupervised, and reinforcement learning (including deep learning). Second, we discuss the empirical foundations of this approach in the cognitive science of learning from other agents. Third, we outline challenges to full realization of the promise of this approach. We conclude by discussing implications for machine learning and applications to real-world problems.\"}, {\"paperId\": \"42afb3469c257d1ca3da02944830b4d04556f74c\", \"abstract\": \": Artificial Intelligence (AI) aims to simulate information storage and processing mechanisms and other intelligent behaviors of a human brain, so that the machine has a certain level of intelligence. With the rapid development of the new generation of information technology, such as the Internet, big data, cloud computing, and deep learning, researches and applications of AI have made and are making important progresses. In this paper, the historical integration and evolution of computer science, control science, brain-inspired intelligence, human brain intelligence, and other disciplines or fields closely related to AI are analyzed in depth; then it is pointed out that the research results on the structure and functional mechanism of brain from neuroscience, brain science and cognitive science provide some important inspirations for the construction of an intelligent computing model. Moreover, the drives and developments of AI are discussed from the aspects of logic model and system, neuron network model, visual nerve hierarchy mechanism, etc. Finally, the development trend of AI is prospected from the following five aspects: the computational theory of the of feedback computation and the energy level of the control system.\"}, {\"paperId\": \"4c79f7c26c31f8922832fdd44358634314e32d23\", \"abstract\": null}, {\"paperId\": \"62ba959add5d835d24f582b4a2217d86176aeb60\", \"abstract\": null}, {\"paperId\": \"acc54eb44f0b1f3fae2704fc287400e7c9c8afd8\", \"abstract\": \"Areas where Artificial Intelligence (AI) & related fields are finding their applications are increasing day by day, moving from core areas of computer science they are finding their applications in various other domains.In recent times Machine Learning i.e. a sub-domain of AI has been widely used in order to assist medical experts and doctors in the prediction, diagnosis and prognosis of various diseases and other medical disorders. In this manuscript the authors applied various machine learning algorithms to a problem in the domain of medical diagnosis and analyzed their efficiency in predicting the results. The problem selected for the study is the diagnosis of the Chronic Kidney Disease.The dataset used for the study consists of 400 instances and 24 attributes. The authors evaluated 12 classification techniques by applying them to the Chronic Kidney Disease data. In order to calculate efficiency, results of the prediction by candidate methods were compared with the actual medical results of the subject.The various metrics used for performance evaluation are predictive accuracy, precision, sensitivity and specificity. The results indicate that decision-tree performed best with nearly the accuracy of 98.6%, sensitivity of 0.9720, precision of 1 and specificity of 1.\"}, {\"paperId\": \"32b84733c0f9898bab54c69a2de5534d2ed1b120\", \"abstract\": null}, {\"paperId\": \"ddbf642f0a64058d8e94cd1d621e5cef07c23bd7\", \"abstract\": \"Data analysis, machine learning and knowledge discovery are research areas at the intersection of computer science, artificial intelligence, mathematics and statistics. They cover general methods and techniques that can be applied to a vast set of applications such as web and text mining, marketing, medicine, bioinformatics and business intelligence. This volume contains the revised versions of selected papers in the field of data analysis, machine learning and knowledge discovery presented during the 36th annual conference of the German Classification Society (GfKl). The conference was held at the University of Hildesheim (Germany) in August 2012.\"}, {\"paperId\": \"bf2cabe0b7527f094fae2a88f1ddfd546d975c80\", \"abstract\": null}, {\"paperId\": \"895053ea9ecdbd235673c28876676b6d51bb042e\", \"abstract\": \"A Numerical optimization is a classical field in operation research and computer science, which has been widely used in areas such as physics and economics. Although optimization algorithms have achieved great success for plenty of applications, handling the big data in the best fashion possible is a very inspiring and demanding challenge in the artificial intelligence era. Stochastic gradient descent (SGD) is pretty simple but surprisingly, highly effective in machine learning models, such as support vector machine (SVM) and deep neural network (DNN). Theoretically, the performance of SGD for convex optimization is well understood. But, for the non-convex setting, which is very common for the machine learning problems, to obtain the theoretical guarantee for SGD and its variants is still a standing problem. In the paper, we do a survey about the SGD and its variants such as Momentum, ADAM and SVRG, differentiate their algorithms and applications and present some recent breakthrough and open problems.\"}, {\"paperId\": \"c876c9d0c315322e4073e64d49013dac649cad17\", \"abstract\": \"The confluence of science, technology, and medicine in our dynamic digital era has spawned new data applications to develop prescriptive analytics, to improve healthcare personalization and precision medicine, and to automate the reporting of health data for clinical decisions.1 Data science in health care has seen recent and rapid progress along 3 paths: (1) through big data via the aggregation of large and complex data sets including electronic medical records, social media, genomic databases, and digitized physiological data from wireless mobile health devices2; (2) through new open-access initiatives that seek to leverage the availability of clinical trial, research, and citizen science data sources for data sharing3; and (3) in analytic techniques particularly for big data, including machine learning and artificial intelligence that may enhance the analyses of both structured and unstructured data.4 As new data sets are created, analyzed, and become increasingly available, several key questions emerge including the following: What is the quality of unstructured data generation? Will the use of nonstandardized methods in data processing with traditional software and hardware lead to data fragmentation and analyses that are nonreproducible? Will healthcare systems incorporate and use big data especially from new publically and patient-generated sources? How will physicians and researchers learn from new open-sourced data and big-data analytics? And ultimately, How can they acquire the skills to create a knowledge translation in data sciences?5\\n\\nPracticing in an era of continuous payment reform and decline in research funding, early career investigators are challenged to keep up with the accelerating pace of change in medicine, all while being expected to provide meaningful contributions through productive clinical, educational, and research experiences.6 In this perspective, we aim to highlight how data science can catalyze professional advancement and discuss the implications of big data, open access, \\u2026\"}, {\"paperId\": \"09ac3902ce1b385ec3b5bfe4f68c5aaba00a81c4\", \"abstract\": \"Software engineering has been historically topdown. From a fully specified problem, a software engineer needs to detail each step of the resolution to get a solution. The resulting program will be functionally adequate as long as its execution environment complies with the original specifications. With their large amount of data, their ever changing multi-level dynamics, smart cities are too complex for a topdown approach. They prompt the need for a paradigm shift in computer science. Programs should be able to self-adapt on the fly, to handle unspecified events,, to efficiently deal with tremendous amount of data. To this end, bottom-up approach should become the norm. Machine learning is a first step,, distributed computing helps. Multi-Agent Systems (MAS) can combine machine learning, distributed computing, may be easily designed with a bottom-up approach. This paper explores how MASs can answer challenges at various levels of smart cities, from sensors networks to ambient intelligence.\"}, {\"paperId\": \"948861f88a295e4f1448fd5b32154af1f69509de\", \"abstract\": \"This chapter explains the Artificial Intelligence (AI) techniques in terms of Artificial Neural Networks (ANNs), fuzzy logic, expert systems, machine learning, Genetic Programming (GP), Evolutionary Polynomial Regression (EPR), and Support Vector Machine (SVM); the AI applications in modern education; the AI applications in software engineering development; the AI applications in Content-Based Image Retrieval (CBIR); and the multifaceted applications of AI in the digital age. AI is a branch of science which deals with helping machines find the suitable solutions to complex problems in a more human-like manner. AI technologies bring more complex data-analysis features to the existing applications in various industries and greatly contribute to management\\u2019s organization, planning, and controlling operations, and will continue to do so with more frequency as programs are refined.\"}, {\"paperId\": \"78a1e55ffb2ca07a612c033bb1b359f56f4ca9c3\", \"abstract\": null}, {\"paperId\": \"ba45f0a028ca2277b52cda726cd71a2b9941a441\", \"abstract\": \"Artificial Intelligence presents an important paradigm shift for science. Science is traditionally founded on theories and models, most often formalized with mathematical formulas handcrafted by theoretical scientists and refined through experiments. Machine learning, an important branch of modern Artificial Intelligence, focuses on learning from data. This leads to a fundamentally different approach to model-building: we step back and focus on the design of algorithms capable of building models from data, but the models themselves are not designed by humans. This is even more true with deep learning, which requires little engineering by hand and is responsible for many of Artificial Intelligence's spectacular successes. In contrast to logic systems, knowledge from a deep learning model is difficult to understand, reuse, and may involve up to a billion parameters. On the other hand, probabilistic machine learning techniques such as deep learning offer an opportunity to tackle large complex problems that are out of the reach of traditional theory-making. It is possible that the more intuition-like reasoning performed by deep learning systems is mostly incompatible with the logic formalism of mathematics. Yet recent studies have shown that deep learning can be useful to logic systems and vice versa. Success at unifying different paradigms of Artificial Intelligence from logic to probability theory offers unique opportunities to combine data-driven approaches with traditional theories. These advancements are susceptible to impact significantly biological sciences, where dimensionality is high and limit the investigation of traditional theories.\"}, {\"paperId\": \"db6ad6ded1cfa26fdc7437f27fb823ec533e96fe\", \"abstract\": null}, {\"paperId\": \"1e220f8f64e86d623fcb99896a7ed465ef430a6e\", \"abstract\": \"The overall goal of the present study was to illustrate the potential of artificial intelligence (AI) techniques in sports on the example of weight training. The research focused in particular on the implementation of pattern recognition methods for the evaluation of performed exercises on training machines. The data acquisition was carried out using way and cable force sensors attached to various weight machines, thereby enabling the measurement of essential displacement and force determinants during training. On the basis of the gathered data, it was consequently possible to deduce other significant characteristics like time periods or movement velocities. These parameters were applied for the development of intelligent methods adapted from conventional machine learning concepts, allowing an automatic assessment of the exercise technique and providing individuals with appropriate feedback. In practice, the implementation of such techniques could be crucial for the investigation of the quality of the execution, the assistance of athletes but also coaches, the training optimization and for prevention purposes. For the current study, the data was based on measurements from 15 rather inexperienced participants, performing 3-5 sets of 10-12 repetitions on a leg press machine. The initially preprocessed data was used for the extraction of significant features, on which supervised modeling methods were applied. Professional trainers were involved in the assessment and classification processes by analyzing the video recorded executions. The so far obtained modeling results showed good performance and prediction outcomes, indicating the feasibility and potency of AI techniques in assessing performances on weight training equipment automatically and providing sportsmen with prompt advice. Key pointsArtificial intelligence is a promising field for sport-related analysis.Implementations integrating pattern recognition techniques enable the automatic evaluation of data measurements.Artificial neural networks applied for the analysis of weight training data show good performance and high classification rates.\"}, {\"paperId\": \"4c705b2c056cf025da5d5809505d53c1de5aa97a\", \"abstract\": \"The fields of Data Engineering and Data Science have emerged in recent years as an exciting intersection between leading academic research and industry leaders, and include diverse, cutting-edge topics like distributed systems, machine learning, and artificial intelligence. While these topics are interesting in their own right, perhaps the most compelling aspect of these fields is how the tools, which are in widespread use, have been developed by an Open Source community. Individuals across several universities and companies have worked together in a distributed fashion to build and improve the leading generation of data technologies. These Open Source principles have enabled the latest industry-adopted tools to constantly evolve at an incredible rate. One way for educators to keep pace with these developments and maintain advanced curriculum is to adopt the same collaborative principles used in open source. At the Insight Data Fellowship, we've used this open source model to provide immediate feedback and drive our curriculum forward, while fostering a culture of independence and curiosity. This session will show Engineering educators how to use open source principles and tools to develop their own curriculum.\"}, {\"paperId\": \"aaaa95a4c0ac5a0b8d56657006627fdc81cf7027\", \"abstract\": \"In past, detection of network attacks has been almost solely done by human operators. They anticipated network anomalies in front of consoles, where based on their expert knowledge applied necessary security measures. With the exponential growth of network bandwidth, this task slowly demanded substantial improvements in both speed and accuracy. One proposed way how to achieve this is the usage of artificial intelligence (AI), progressive and promising computer science branch, particularly one of its sub-fields - machine learning (ML) - where main idea is learning from data. In this paper authors will try to give a general overview of AI algorithms, with main focus on their usage for network intrusion detection.\"}, {\"paperId\": \"a6ffce203cb7e587a5f4d36ca7442a7b26c65b07\", \"abstract\": \"Seismic waves from earthquakes and other sources are used to infer the structure and properties of Earth\\u2019s interior. The availability of large-scale seismic datasets and the suitability of deep-learning techniques for seismic data processing have pushed deep learning to the forefront of fundamental, long-standing research investigations in seismology. However, some aspects of applying deep learning to seismology are likely to prove instructive for the geosciences, and perhaps other research areas more broadly. Deep learning is a powerful approach, but there are subtleties and nuances in its application. We present a systematic overview of trends, challenges, and opportunities in applications of deep-learning methods in seismology. Description Large-scale learning The large amount and availability of datasets in seismology creates a great opportunity to apply machine learning and artificial intelligence to data processing. Mousavi and Beroza provide a comprehensive review of the deep-learning techniques being applied to seismic datasets, covering approaches, limitations, and opportunities. The trends in data processing and analysis can be instructive for geoscience and other research areas more broadly. \\u2014BG The ways in which deep learning can help process and analyze large seismological datasets are reviewed. BACKGROUND Seismology is the study of seismic waves to understand their origin\\u2014most obviously, sudden fault slip in earthquakes, but also explosions, volcanic eruptions, glaciers, landslides, ocean waves, vehicular traffic, aircraft, trains, wind, air guns, and thunderstorms, for example. Seismology uses those same waves to infer the structure and properties of planetary interiors. Because sources can generate waves at any time, seismic ground motion is recorded continuously, at typical sampling rates of 100 points per second, for three components of motion, and on arrays that can include thousands of sensors. Although seismology is clearly a data-rich science, it often is a data-driven science as well, with new phenomena and unexpected behavior discovered with regularity. And for at least some tasks, the careful and painstaking work of seismic analysts over decades and around the world has also made seismology a data label\\u2013rich science. This facet makes it fertile ground for deep learning, which has entered almost every subfield of seismology and outperforms classical approaches, often dramatically, for many seismological tasks. ADVANCES Seismic wave identification and onset-time, first-break determination for seismic P and S waves within continuous seismic data are foundational to seismology and are particularly well suited to deep learning because of the availability of massive, labeled datasets. It has received particularly close attention, and that has led, for example, to the development of deep learning\\u2013based earthquake catalogs that can feature more than an order of magnitude more events than are present in conventional catalogs. Deep learning has shown the ability to outperform classical approaches for other important seismological tasks as well, including the discrimination of earthquakes from explosions and other sources, separation of seismic signals from background noise, seismic image processing and interpretation, and Earth model inversion. OUTLOOK The development of increasingly cost-effective sensors and emerging ground-motion sensing technologies, such as fiber optic cable and accelerometers in smart devices, portend a continuing acceleration of seismological data volumes, so that deep learning is likely to become essential to seismology\\u2019s future. Deep learning\\u2019s nonlinear mapping ability, sequential data modeling, automatic feature extraction, dimensionality reduction, and reparameterization are all advantageous for processing high-dimensional seismic data, particularly because those data are noisy and, from the point of view of mathematical inference, incomplete. Deep learning for scientific discovery and direct extraction of insight into seismological processes is clearly just getting started. Aspects of seismology pose interesting additional challenges for deep learning. Many of the most important problems in earthquake seismology\\u2014such as earthquake forecasting, ground motion prediction, and rapid earthquake alerting\\u2014concern large and damaging earthquakes that are (fortunately) rare. That rarity poses a fundamental challenge for the data-hungry methods of deep learning: How can we train reliable models, and how do we validate them well enough to rely on them when data are scarce and opportunities to test models are infrequent? Further, how can we operationalize deep-learning techniques in such a situation, when the mechanisms by which they make predictions from data may not be easily explained, and the consequences of incorrect models are high? Incorporating domain knowledge through physics-based and explainable deep learning and setting up standard benchmarking and evaluation protocols will help ensure progress, as is the nascent emergence of a seismological data science ecosystem. More generally, a combination of data science literacy for geoscientists as well as recruiting data science expertise will help to ensure that deep-learning seismology reaches its full potential. Deep-learning processing of seismic data and incorporation of domain knowledge can lead to new capabilities and new insights across seismology. A. MASTIN/SCIENCE, TOP RIGHT: CARA HARWOOD/UNIVERSITY OF CALIFORNIA-DAVIS/CC BY-NC-SA 3.0 MIDDLE RIGHT: JOHAN SWANEPOEL/SCIENCE SOURCE\"}, {\"paperId\": \"ca5d032629e4218f3ce2b2acb36e27d92ca5d627\", \"abstract\": \"Machine learning is one of the most important subfields of computer science and can be used to solve a variety of interesting artificial intelligence problems. There are different languages, framework and tools to define the data needed to solve machine learning-based problems. However, there is a great number of very diverse alternatives which makes it difficult the intercommunication, portability and re-usability of the definitions, designs or algorithms that any developer may create. In this paper, we take the first step towards a language and a development environment independent of the underlying technologies, allowing developers to design solutions to solve machine learning-based problems in a simple and fast way, automatically generating code for other technologies. That can be considered a transparent bridge among current technologies. We rely on Model-Driven Engineering approach, focusing on the creation of models to abstract the definition of artifacts from the underlying technologies.\"}, {\"paperId\": \"43700535bb3e3812714d14a7143ce7a9dc2f996c\", \"abstract\": \"In this chapter, we overview the uses of machine learning for high frequency trading and market microstructure data and problems. Machine learning is a vibrant subfield of computer science that draws on models and methods from statistics, algorithms, computational complexity, artificial intelligence, control theory, and a variety of other disciplines. Its primary focus is on computationally and informationally efficient algorithms for inferring good predictive models from large data sets, and thus is a natural candidate for application to problems arising in HFT, both for trade execution and the generation of alpha. The inference of predictive models from historical data is obviously not new in quantitative finance; ubiquitous examples include coefficient estimation for the CAPM, Fama and French factors [5], and related approaches. The special challenges for machine learning presented by HFT generally arise from the very fine granularity of the data \\u2014 often microstructure data at the resolution of individual orders, (partial) executions, hidden liquidity, and cancellations \\u2014 and a lack of understanding of how such low-level data relates to actionable circumstances (such as profitably buying or selling shares, optimally executing a large order, etc.). In the language of machine learning, whereas models such as CAPM and its variants already prescribe what the relevant variables or \\u201cfeatures\\u201d are for prediction or modeling (excess returns, book-to-market ratios, etc.), in many HFT problems one may have no prior intuitions about how (say) the distribution of liquidity in the order book relates to future price movements, if at all. Thus feature selection or feature engineering becomes an important process in machine learning for HFT, and is one of our central themes. Since HFT itself is a relatively recent phenomenon, there are few published works on the application of machine learning to HFT. For this reason, we structure the chapter around a few case studies from our own work [6, 14]. In each case study, we focus on a specific trading problem we would like to solve or optimize; the (microstructure) data from which we hope to solve this problem; the variables or features derived from the data as inputs to a machine learning process; and the machine learning algorithm applied to these features. The cases studies we will examine are:\"}, {\"paperId\": \"a13ed9714436d08f7f35ddeb42c03ac0cbc3f960\", \"abstract\": \"The Python open-source programming language has become a de facto standard in applications ranging from engineering, scientific, data science, machine learning, information technology, and artificial intelligence. Modern systems-on-a-chip (SoCs), when used in embedded applications, make it possible to run Python to execute complex analytical algorithms with performance close to that of a desktop workstation\\u2014but with a much smaller form factor and significantly lower power requirements. By pre-processing the data read from sensors, the Xilinx\\u00ae Zynq portfolio provides a much higher level of performance and determinism as well as lower latency. This approach, referred to as the PYNQ framework, effectively offloads many critical but repetitive operations that consume processor bandwidth unnecessarily from the application processor. The offloading capability is critical to meeting the need for increased intelligence in I IoT edge applications.\"}, {\"paperId\": \"d09b67459d6bd6d8c7aae105552ad174883389c0\", \"abstract\": null}, {\"paperId\": \"327395582fe9f9d7efca7a962f6b99bbe3f610dc\", \"abstract\": null}, {\"paperId\": \"fd0e92b3a6c987bd80edbf3b55a39b5f9a6ffe40\", \"abstract\": null}, {\"paperId\": \"b7185125d44803132b7c2434c977dcfa4eff3d13\", \"abstract\": null}, {\"paperId\": \"60cf49038406fa732a48404d70f596103faac8f4\", \"abstract\": \"Deep learning is a subfield of machine learning which uses artificial neural networks that is inspired by the structure and function of the human brain. Despite being a very new approach, it has become very popular recently. Deep learning has achieved much higher success in many applications where machine learning has been successful at certain rates. In particular It is preferred in the classification of big data sets because it can provide fast and efficient results. In this study, we used Tensorflow, one of the most popular deep learning libraries to classify MNIST dataset, which is frequently used in data analysis studies. Using Tensorflow, which is an open source artificial intelligence library developed by Google, we have studied and compared the effects of multiple activation functions on classification results. The functions used are Rectified Linear Unit (ReLu), Hyperbolic Tangent (tanH), Exponential Linear Unit (eLu), sigmoid, softplus and softsign. In this Study, Convolutional Neural Network (CNN) and SoftMax classifier are used as deep learning artificial neural network. The results show that the most accurate classification rate is obtained using the ReLu activation function.\"}, {\"paperId\": \"844dcd971bef49217709400b92ba0107ad5e8d37\", \"abstract\": \"This special issue entitled Impact of Intelligence Methodologies on Education and Training Process contains a collection of research articles that focus on the use of intelligence methodologies such as artificial intelligence, machine learning, deep learning, and computer vision for education and training processes. The theme of the issue is the use of advanced technologies for education and training. Specifically, it focuses on the application of data science and machine learning methodologies in all aspects of modern education such as pedagogy management, E-learning, virtual lab management, improving hands-on online teaching methodologies, etc. The key objective is to share and exchange innovative ideas and experiences in modern education with evolving intelligent technologies. Today there are many successful applications of machine learning methodologies for education. A number of papers in this special issue apply machine learningbased models for the semantic annotation of English language educational resources. It creates linear decision-making models with machine learning algorithms such as support vector machine (SVM) and convolutional neural networks (CNN) to effectively address the issues of present-day E-learning platforms, specifically in the context of English language educational resources. Other papers focus on enhancing teaching and learning methodologies with artificial intelligence techniques. Since E-learning platforms are consid-\"}, {\"paperId\": \"ad6b2704a052c56382028c99ac576ac2abe00b60\", \"abstract\": \"This article surveys the interdisciplinary research of neuroscience, network science, and dynamic systems, with emphasis on the emergence of brain-inspired intelligence. To replicate brain intelligence, a practical way is to reconstruct cortical networks with dynamic activities that nourish the brain functions, instead of using only artificial computing networks. The survey provides a complex network and spatiotemporal dynamics (abbr. network dynamics) perspective for understanding the brain and cortical networks and, furthermore, develops integrated approaches of neuroscience and network dynamics toward building brain-inspired intelligence with learning and resilience functions. Presented are fundamental concepts and principles of complex networks, neuroscience, and hybrid dynamic systems, as well as relevant studies about the brain and intelligence. Other promising research directions, such as brain science, data science, quantum information science, and machine behavior are also briefly discussed toward future applications.\"}, {\"paperId\": \"6aa99d38d5851c217f3beffb26815f549c60d9c1\", \"abstract\": \"In contrast to artificial intelligence and machine learning approaches, KEGG (https://www.kegg.jp) has relied on human intelligence to develop \\u201cmodels\\u201d of biological systems, especially in the form of KEGG pathway maps that are manually created by capturing knowledge from published literature. The KEGG models can then be used in biological big data analysis, for example, for uncovering systemic functions of an organism hidden in its genome sequence through the simple procedure of KEGG mapping. Here we present an updated version of KEGG Mapper, a suite of KEGG mapping tools reported previously (Kanehisa and Sato, Protein Sci 2020; 29:28\\u201335), together with the new versions of the KEGG pathway map viewer and the BRITE hierarchy viewer. Significant enhancements have been made for BRITE mapping, where the mapping result can be examined by manipulation of hierarchical trees, such as pruning and zooming. The tree manipulation feature has also been implemented in the taxonomy mapping tool for linking KO (KEGG Orthology) groups and modules to phenotypes.\"}, {\"paperId\": \"acff518b945f15b1a687ac313b25048c50fed044\", \"abstract\": \"Political science, and social science in general, have traditionally been using computational methods to study areas such as voting behavior, policy making, international conflict, and international development. More recently, increasingly available quantities of data are being combined with improved algorithms and affordable computational resources to predict, learn, and discover new insights from data that is large in volume and variety. New developments in the areas of machine learning, deep learning, natural language processing (NLP), and, more generally, artificial intelligence (AI) are opening up new opportunities for testing theories and evaluating the impact of interventions and programs in a more dynamic and effective way. Applications using large volumes of structured and unstructured data are becoming common in government and industry, and increasingly also in social science research. This chapter offers an introduction to such methods drawing examples from political science. Focusing on the areas where the strengths of the methods coincide with challenges in these fields, the chapter first presents an introduction to AI and its core technology - machine learning, with its rapidly developing subfield of deep learning. The discussion of deep neural networks is illustrated with the NLP tasks that are relevant to political science. The latest advances in deep learning methods for NLP are also reviewed, together with their potential for improving information extraction and pattern recognition from political science texts.\"}, {\"paperId\": \"e50f2af57c55a0d81d3aca82915b57571547cdce\", \"abstract\": null}, {\"paperId\": \"419465e06811b67185c7edea861cedfdaebbfc86\", \"abstract\": \"Tackling data challenges and incorporating physics into machine learning models will help unlock the potential of artificial intelligence to answer Earth science questions.\"}, {\"paperId\": \"a8eafdaff041abeff9889c9aad03da3ed77cd5e0\", \"abstract\": \"Application of artificial intelligence (AI), and more specifically machine learning, to the physical sciences has expanded significantly over the past decades. In particular, science-informed AI, also known as scientific AI or inductive bias AI, has grown from a focus on data analysis to now controlling experiment design, simulation, execution and analysis in closed-loop autonomous systems. The CAMEO (closed-loop autonomous materials exploration and optimization) algorithm employs scientific AI to address two tasks: learning a material system\\u2019s composition-structure relationship and identifying materials compositions with optimal functional properties. By integrating these, accelerated materials screening across compositional phase diagrams was demonstrated, resulting in the discovery of a best-in-class phase change memory material. Key to this success is the ability to guide subsequent measurements to maximize knowledge of the composition-structure relationship, or phase map. In this work we investigate the benefits of incorporating varying levels of prior physical knowledge into CAMEO\\u2019s autonomous phase-mapping. This includes the use of ab-initio phase boundary data from the AFLOW repositories, which has been shown to optimize CAMEO\\u2019s search when used as a prior.\"}, {\"paperId\": \"277083fdacf033996e3249013da815655f2fbf5b\", \"abstract\": \"Conspectus The ongoing revolution of the natural sciences by the advent of machine learning and artificial intelligence sparked significant interest in the material science community in recent years. The intrinsically high dimensionality of the space of realizable materials makes traditional approaches ineffective for large-scale explorations. Modern data science and machine learning tools developed for increasingly complicated problems are an attractive alternative. An imminent climate catastrophe calls for a clean energy transformation by overhauling current technologies within only several years of possible action available. Tackling this crisis requires the development of new materials at an unprecedented pace and scale. For example, organic photovoltaics have the potential to replace existing silicon-based materials to a large extent and open up new fields of application. In recent years, organic light-emitting diodes have emerged as state-of-the-art technology for digital screens and portable devices and are enabling new applications with flexible displays. Reticular frameworks allow the atom-precise synthesis of nanomaterials and promise to revolutionize the field by the potential to realize multifunctional nanoparticles with applications from gas storage, gas separation, and electrochemical energy storage to nanomedicine. In the recent decade, significant advances in all these fields have been facilitated by the comprehensive application of simulation and machine learning for property prediction, property optimization, and chemical space exploration enabled by considerable advances in computing power and algorithmic efficiency. In this Account, we review the most recent contributions of our group in this thriving field of machine learning for material science. We start with a summary of the most important material classes our group has been involved in, focusing on small molecules as organic electronic materials and crystalline materials. Specifically, we highlight the data-driven approaches we employed to speed up discovery and derive material design strategies. Subsequently, our focus lies on the data-driven methodologies our group has developed and employed, elaborating on high-throughput virtual screening, inverse molecular design, Bayesian optimization, and supervised learning. We discuss the general ideas, their working principles, and their use cases with examples of successful implementations in data-driven material discovery and design efforts. Furthermore, we elaborate on potential pitfalls and remaining challenges of these methods. Finally, we provide a brief outlook for the field as we foresee increasing adaptation and implementation of large scale data-driven approaches in material discovery and design campaigns.\"}, {\"paperId\": \"396aa4db3e6c7a97f9962ed5834c0c0e196f77f1\", \"abstract\": \"From the Publisher: \\nCombines the theoretical foundations of intelligent problem-solving with he data structures and algorithms needed for its implementation. The book presents logic, rule, object and agent-based architectures, along with example programs written in LISP and PROLOG. \\nThe practical applications of AI have been kept within the context of its broader goal: understanding the patterns of intelligence as it operates in this world of uncertainty, complexity and change. \\nThe introductory and concluding chapters take a new look at the potentials and challenges facing artificial intelligence and cognitive science. An extended treatment of knowledge-based problem-solving is given including model-based and case-based reasoning. \\nIncludes new material on: \\nFundamentals of search, inference and knowledge representation \\nAI algorithms and data structures in LISP and PROLOG Production systems, blackboards, and meta-interpreters including planers, rule-based reasoners, and inheritance systems. \\nMachine-learning including ID3 with bagging and boosting, explanation based learning, PAC learning, and other forms of induction \\nNeural networks, including perceptrons, back propogation, Kohonen networks, Hopfield networks, Grossberg learning, and counterpropagation. Emergent and social methods of learning and adaptation, including genetic algorithms, genetic programming and artificial life. \\nObject and agent-based problem solving and other forms of advanced knowledge representation\"}, {\"paperId\": \"43a1ff971444c04b2f92a8b8f4ac9e759b36f044\", \"abstract\": null}, {\"paperId\": \"9ebaac6d959abf81f396186acac32902410611ea\", \"abstract\": \"Data analysis and machine learning are research areas at the intersection of computer science, artificial intelligence, mathematics and statistics. They cover general methods and techniques that can be applied to a vast set of applications such as web and text mining, marketing, medical science, bioinformatics and business intelligence. This volume contains the revised versions of selected papers in the field of data analysis, machine learning and applications presented during the 31st Annual Conference of the German Classification Society (Gesellschaft fr Klassifikation - GfKl). The conference was held at the Albert-Ludwigs-University in Freiburg, Germany, in March 2007.\"}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 400, \"next\": 500, \"data\": [{\"paperId\": \"a3792cc3d9110af93cb007872ba564d61fa21e60\", \"abstract\": \"This article reviews recent developments in the applications of machine learning, data-driven modeling, transfer learning, and autonomous experimentation for the discovery, design, and optimization of soft and biological materials. The design and engineering of molecules and molecular systems have long been a preoccupation of chemical and biomolecular engineers using a variety of computational and experimental techniques. Increasingly, researchers have looked to emerging and established tools in artificial intelligence and machine learning to integrate with established approaches in chemical science to realize powerful, efficient, and in some cases autonomous platforms for molecular discovery, materials engineering, and process optimization. This review summarizes the basic principles underpinning these techniques and highlights recent successful example applications in autonomous materials discovery, transfer learning, and multi-fidelity active learning. Expected final online publication date for the Annual Review of Chemical and Biomolecular Engineering, Volume 13 is October 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\"}, {\"paperId\": \"45d1ffd3b7e3fe1c21bfff6457559972fbd9310d\", \"abstract\": \"The emergence of machine learning in the artificial intelligence field led the world of technology to make great strides. Today\\u201fs advanced systems with the ability of being designed just like human brain functions has given practitioners the ability to train systems so that they could process, analyze, classify, and predict different data classes. Therefore, the machine learning field has become a hot topic for scientists and researchers to introduce the best network with the highest performance for such mentioned purposes. In this article, computer vision science, image classification implementation, and deep neural networks are presented. This article discusses how models have been designed based on the concept of the human brain. The development of a Convolutional Neural Network (CNN) and its various architectures, which have shown great efficiency and evaluation in object detection, face recognition, image classification, and localization, are also introduced. Furthermore, the utilization and application of CNNs, including voice recognition, image processing, video processing, and text recognition, are examined closely. A literature review is conducted to illustrate the significance and the details of Convolutional Neural Networks in various applications.\"}, {\"paperId\": \"cb030476911e638b856e54adad4bf1dbf499340d\", \"abstract\": \"Machine learning has recently found many applications in the geosciences and remote sensing. These applications range from bias correction to retrieval algorithms, from code acceleration to detection of disease in crops. As a broad subfield of artificial intelligence, machine learning is concerned with algorithms and techniques that allow computers to \\u201clearn\\u201d. The major focus of machine learning is to extract information from data automatically by computational and statistical methods. Over the last decade there has been considerable progress in developing a machine learning methodology for a variety of Earth Science applications involving trace gases, retrievals, aerosol products, land surface products, vegetation indices, and most recently, ocean products (Yi and Prybutok, 1996, Atkinson and Tatnall, 1997, Carpenter et al., 1997, Comrie, 1997, Chevallier et al., 1998, Hyyppa et al., 1998, Gardner and Dorling, 1999, Lary et al., 2004, Lary et al., 2007, Brown et al., 2008, Lary and Aulov, 2008, Caselli et al., 2009, Lary et al., 2009). Some of this work has even received special recognition as a NASA Aura Science highlight (Lary et al., 2007) and commendation from the NASA MODIS instrument team (Lary et al., 2009). The two types of machine learning algorithms typically used are neural networks and support vector machines. In this chapter, we will review some examples of how machine learning is useful for Geoscience and remote sensing, these examples come from the author\\u2019s own research.\"}, {\"paperId\": \"b51d98551c99bcb1fed97f58f2ad7ecbcd3fb38a\", \"abstract\": \"Data science and machine learning (DS/ML) are at the heart of the recent advancements of many Artificial Intelligence (AI) applications. There is an active research thread in AI, AutoML, that aims to develop systems for automating end-to-end the DS/ML Lifecycle. However, do DS and ML workers really want to automate their DS/ML workflow? To answer this question, we first synthesize a human-centered AutoML framework with 6 User Role/Personas, 10 Stages and 43 Sub-Tasks, 5 Levels of Automation, and 5 Types of Explanation, through reviewing research literature and marketing reports. Secondly, we use the framework to guide the design of an online survey study with 217 DS/ML workers who had varying degrees of experience, and different user roles \\u201cmatching\\u201d to our 6 roles/personas. We found that different user personas participated in distinct stages of the lifecycle \\u2013 but not all stages. Their desired levels of automation and types of explanation for AutoML also varied significantly depending on the DS/ML stage and the user persona. Based on the survey results, we argue there is no rationale from user needs for complete automation of the end-to-end DS/ML lifecycle. We propose new next steps for user-controlled DS/ML automation.\"}, {\"paperId\": \"696a97be6a70df6778b1a778605c9e20e52b089c\", \"abstract\": \"Automatic machine translation is gaining more and more attention in a particular segment of the research community that treats various topics from artificial intelligence, natural language processing, computational linguistics, machine learning and data science. Machine translation is a complex task in which a computer is utilised for the purpose of translating from source to one or more target languages without human involvement, or with a minimum of interventions. In general, machine translation could be implemented in higher education and academic curricula in a variety of possible fields and applications. Although there are several approaches to machine translation, two are dominant today \\u2013 statistical and neural machine translation. Both are being used in this research in form of two online machine translation systems. The aim of this paper is to examine the usability of machine translation for poetry and a low-resource language pair, such as Croatian-German. The authors chose to use a data set that contained the works of a relevant contemporary poet of the Croatian language and the translations of his poems in German that were conducted by two professional literary translators. The paper demonstrates the effectiveness of machine translation of poetry with regard to special automatic quality metrics.\"}, {\"paperId\": \"572559817f762e7feb7c9f935fca02184ee30ba3\", \"abstract\": null}, {\"paperId\": \"01cc430f180dce4d7edd278161eb09b65ae9dd49\", \"abstract\": \"As a broad subfield of artificial intelligence, machine learning is concerned with the design and development of algorithms and techniques that allow computers to \\\"learn\\\". At a general level, there are two types of learning: inductive, and deductive. Inductive machine learning methods extract rules and patterns out of massive data sets. The major focus of machine learning research is to extract information from data automatically, by computational and statistical methods. Hence, machine learning is closely related not only to data mining and statistics, but also theoretical computer science. This book presents new and important research in this field.\"}, {\"paperId\": \"a3bb2b7e302ef255f373ae63dd864165ff619d92\", \"abstract\": \"As a special case of machine learning, incremental learning can acquire useful knowledge from incoming data continuously while it does not need to access the original data. It is expected to have the ability of memorization and it is regarded as one of the ultimate goals of artificial intelligence technology. However, incremental learning remains a long term challenge. Modern deep neural network models achieve outstanding performance on stationary data distributions with batch training. This restriction leads to catastrophic forgetting for incremental learning scenarios since the distribution of incoming data is unknown and has a highly different probability from the old data. Therefore, a model must be both plastic to acquire new knowledge and stable to consolidate existing knowledge. This review aims to draw a systematic review of the state of the art of incremental learning methods. Published reports are selected from Web of Science, IEEEXplore, and DBLP databases up to May 2020. Each paper is reviewed according to the types: architectural strategy, regularization strategy and rehearsal and pseudo-rehearsal strategy. We compare and discuss different methods. Moreover, the development trend and research focus are given. It is concluded that incremental learning is still a hot research area and will be for a long period. More attention should be paid to the exploration of both biological systems and computational models.\"}, {\"paperId\": \"cec8a2e86f7e399386e73f9d638988abcd9a771c\", \"abstract\": \"Machine learning has recently found many applications in aerospace and remote sensing. These applications range from bias correction to retrieval algorithms, from code acceleration to detection of disease in crops. As a broad subfield of artificial intelligence, machine learning is concerned with algorithms and techniques that allow computers to \\u201clearn\\u201d. The major focus of machine learning is to extract information from data automatically by computational and statistical methods. Over the last decade there has been considerable progress in developing a machine learning methodology for a variety of Earth Science applications involving trace gases, retrievals, aerosol products, land surface products, vegetation indices, and most recently, ocean products (\"}, {\"paperId\": \"b0c95665d20db02d92b8213c6f617e8965d9ea9d\", \"abstract\": \"Laboratory medicine is a digital science. Every large hospital produces a wealth of data each day\\u2014from simple numerical results from, e.g., sodium measurements to highly complex output of \\u201c-omics\\u201d analyses, as well as quality control results and metadata. Processing, connecting, storing, and ordering extensive parts of these individual data requires Big Data techniques. Whereas novel technologies such as artificial intelligence and machine learning have exciting application for the augmentation of laboratory medicine, the Big Data concept remains fundamental for any sophisticated data analysis in large databases. To make laboratory medicine data optimally usable for clinical and research purposes, they need to be FAIR: findable, accessible, interoperable, and reusable. This can be achieved, for example, by automated recording, connection of devices, efficient ETL (Extract, Transform, Load) processes, careful data governance, and modern data security solutions. Enriched with clinical data, laboratory medicine data allow a gain in pathophysiological insights, can improve patient care, or can be used to develop reference intervals for diagnostic purposes. Nevertheless, Big Data in laboratory medicine do not come without challenges: the growing number of analyses and data derived from them is a demanding task to be taken care of. Laboratory medicine experts are and will be needed to drive this development, take an active role in the ongoing digitalization, and provide guidance for their clinical colleagues engaging with the laboratory data in research.\"}, {\"paperId\": \"3f0944bd1ee329078edcf88a86ade50c8401ba97\", \"abstract\": \"Abstract Deep Learning has boosted artificial intelligence over the past 5 years and is seen now as one of the major technological innovation areas, predicted to replace lots of repetitive, but complex tasks of human labor within the next decade. It is also expected to be \\u2018game changing\\u2019 for research activities in pharma and life sciences, where large sets of similar yet complex data samples are systematically analyzed. Deep learning is currently conquering formerly expert domains especially in areas requiring perception, previously not amenable to standard machine learning. A typical example is the automated analysis of images which are typically produced en-masse in many domains, e.\\u2009g., in high-content screening or digital pathology. Deep learning enables to create competitive applications in so-far defined core domains of \\u2018human intelligence\\u2019. Applications of artificial intelligence have been enabled in recent years by (i) the massive availability of data samples, collected in pharma driven drug programs (=\\u2018big data\\u2019) as well as (ii) deep learning algorithmic advancements and (iii) increase in compute power. Such applications are based on software frameworks with specific strengths and weaknesses. Here, we introduce typical applications and underlying frameworks for deep learning with a set of practical criteria for developing production ready solutions in life science and pharma research. Based on our own experience in successfully developing deep learning applications we provide suggestions and a baseline for selecting the most suited frameworks for a future-proof and cost-effective development.\"}, {\"paperId\": \"975c68a70dcf74455e130547ef50e538936e3d92\", \"abstract\": \"Abstract Devices of human-based senses such as e-noses, e-tongues and e-eyes can be used to analyze different compounds in several food matrices. These sensors allow the detection of one or more compounds present in complex food samples, and the responses obtained can be used for several goals when different chemometric tools are applied. In this systematic review, we used Preferred Reporting Items for Systematic Reviews and Meta-Analysis guidelines, to address issues such as e-sensing with chemometric methods for food quality control (FQC). A total of 109 eligible articles were selected from PubMed, Scopus and Web of Science. Thus, we predicted that the association between e-sensing and chemometric tools is essential for FQC. Most studies have applied preliminary approaches like exploratory analysis, while the classification/regression methods have been less investigated. It is worth mentioning that non-linear methods based on artificial intelligence/machine learning, in most cases, had classification/regression performances superior to non-liner, although their applications were seen less often. Another approach that has generated promising results is the data fusion between e-sensing devices or in conjunction with other analytical techniques. Furthermore, some future trends in the application of miniaturized devices and nanoscale sensors are also discussed.\"}, {\"paperId\": \"516da66e8a75435fb99f4d39dbbf0f65354e7d96\", \"abstract\": \"BACKGROUND Artificial intelligence, such as convolutional neural networks (CNNs), has been used in the interpretation of images and the diagnosis of hepatocellular cancer (HCC) and liver masses. CNN, a machine-learning algorithm similar to deep learning, has demonstrated its capability to recognise specific features that can detect pathological lesions. AIM To assess the use of CNNs in examining HCC and liver masses images in the diagnosis of cancer and evaluating the accuracy level of CNNs and their performance. METHODS The databases PubMed, EMBASE, and the Web of Science and research books were systematically searched using related keywords. Studies analysing pathological anatomy, cellular, and radiological images on HCC or liver masses using CNNs were identified according to the study protocol to detect cancer, differentiating cancer from other lesions, or staging the lesion. The data were extracted as per a predefined extraction. The accuracy level and performance of the CNNs in detecting cancer or early stages of cancer were analysed. The primary outcomes of the study were analysing the type of cancer or liver mass and identifying the type of images that showed optimum accuracy in cancer detection. RESULTS A total of 11 studies that met the selection criteria and were consistent with the aims of the study were identified. The studies demonstrated the ability to differentiate liver masses or differentiate HCC from other lesions (n = 6), HCC from cirrhosis or development of new tumours (n = 3), and HCC nuclei grading or segmentation (n = 2). The CNNs showed satisfactory levels of accuracy. The studies aimed at detecting lesions (n = 4), classification (n = 5), and segmentation (n = 2). Several methods were used to assess the accuracy of CNN models used. CONCLUSION The role of CNNs in analysing images and as tools in early detection of HCC or liver masses has been demonstrated in these studies. While a few limitations have been identified in these studies, overall there was an optimal level of accuracy of the CNNs used in segmentation and classification of liver cancers images.\"}, {\"paperId\": \"2067694b1850388ba934270472e0cfdfe28fe566\", \"abstract\": null}, {\"paperId\": \"45272c5683128d5482599c9f9872bf0ecc3d0911\", \"abstract\": \"Twitter has withstood the test of time as a successful social networking platform. In many circles globally, the majority of users choose Twitter when choosing a social media outlet for reliable scientific information and news. However, the Twitter application programming interface (API) limitations do not allow for low-cost data science options for academia. It becomes very expensive for academic researchers to gain the full potential of data analytics available from Twitter using a free API account. In this article, we present our big data analytics platform developed at our DaTALab at Lakehead University, Canada, that allows users to focus on their Twitter search criteria and gain access to large amounts of Twitter data at the touch of a button. The platform supports the collection of social media data and applies many filters for cleaning and further use for machine learning (ML) and artificial intelligence (AI)-based systems. Our focus has been primarily on healthcare-related research, which shows the strength of the presented platform. However, the platform itself is malleable to any topic of interest. Data collected and processed are suitable for further AI/ML analysis. We present our platform using a specific healthcare search topic to emphasize the power of our system for future research endeavors in the healthcare field.\"}, {\"paperId\": \"a879772ca9f2626e843e32466062f4d7e35c524a\", \"abstract\": \"Sharing data is a scientific imperative that accelerates scientific discoveries, reinforces open science inquiry, and allows for efficient use of public investment and research resources. Considering these benefits, data sharing has been widely promoted in diverse fields and neuroscience has been no exception to this movement. For all its promise, however, the sharing of human neuroimaging data raises critical ethical and legal issues, such as data privacy. Recently, the heightened risks to data privacy posed by the rapid advances in artificial intelligence and machine learning techniques have made data sharing more challenging; the regulatory landscape around data sharing has also been evolving rapidly. Here we present an in\\u2010depth ethical and regulatory analysis that examines how neuroimaging data are currently shared against the backdrop of the relevant regulations and policies in the United States and how advanced software tools and algorithms might undermine subjects' privacy in neuroimaging data sharing. The implications of these novel technological threats to privacy in neuroimaging data sharing practices and policies will also be discussed. We then conclude with a proposal for a legal prohibition against malicious use of neuroscience data as a regulatory mechanism to address privacy risks associated with the data while maximizing the benefits of data sharing and open science practice in the field of neuroscience.\"}, {\"paperId\": \"c7b263aadd97dfb9ac4072e03a4c2c56be491805\", \"abstract\": null}, {\"paperId\": \"a9c50c2dc596ff14233f660c6ff6e4dab94e7c95\", \"abstract\": \"How does one create an intelligent machine? This problem has proven difficult. Over the past several decades, scientists have taken one of three approaches: In the first, which is knowledge-based, an intelligent machine in a laboratory is directly programmed to perform a given task. In a second, learning-based approach, a computer is \\\"spoon-fed\\\" human-edited sensory data while the machine is controlled by a task-specific learning program. Finally, by a \\\"genetic search,\\\" robots have evolved through generations by the principle of survival of the fittest, mostly in a computer-simulated virtual world. Although notable, none of these is powerful enough to lead to machines having the complex, diverse, and highly integrated capabilities of an adult brain, such as vision, speech, and language. Nevertheless, these traditional approaches have served as the incubator for the birth and growth of a new direction for machine intelligence: autonomous mental development. As Kuhn wrote (1), \\\"Failure of existing rules is the prelude to a search for new ones.\\\"\"}, {\"paperId\": \"0c3870ac2df94ef6cb4bda5a18c28c7b23939c06\", \"abstract\": null}, {\"paperId\": \"a17bad00f1e031e22cdf5b83369c5defeaea9d1e\", \"abstract\": \"Deep learning is a recent emerging field of research in data science. Deep learning is essentially a combination of artificial intelligence and machine learning. Inspired by brain neurons, this has proven greater flexibility and builds more accurate models compared to machine learning. But making theoretical designs and to perform desired experiments are quite challenging due to many aspects. In the present paper, these challenges have been discussed to provide researchers a clear vision for the futuristic research in the field of deep learning.\"}, {\"paperId\": \"69e1e93c36250323812ed190b09a7d9bc77900ba\", \"abstract\": \"We present work on project MLExAI, funded by the National Science Foundation with a goal of unifying the artificial intelligence (AI) course around the theme of machine learning. Our work involves the development, implementation, and testing of an adaptable framework for the presentation of core AI topics that emphasizes the relationship between AI and computer science. A suite of adaptable hands-on laboratory projects that can be closely integrated into a one-term AI course and which would supplement introductory AI texts has been developed. The paper focuses on one of these projects, how it meets our goal, and presents our experiences using it. The project involves the development of a learning system for web document classification. Students investigate the process of classifying hypertext documents, called tagging, and apply machine learning techniques and data mining tools for automatic tagging. A summary of our experiences using the projects during four course offerings over the last two years are also presented.\"}, {\"paperId\": \"8d69eb2758a0ca1bad256438a79e7209f81ec178\", \"abstract\": null}, {\"paperId\": \"c2c235969781868884326b42fe7cbd8fda0f1f86\", \"abstract\": null}, {\"paperId\": \"e081f69dc033935935932e3d3918aff3125e79e1\", \"abstract\": \"Abstract In recent years, the field of artificial intelligence has contributed significantly to the science of meteorology, most notably in the now familiar form of expert systems. Expert systems have focused on rules or heuristics by establishing, in computer code, the reasoning process of a weather forecaster predicting, for example, thunderstorms or fog. In addition to the years of effort that goes into developing such a knowledge base is the time-consuming task of extracting such knowledge and experience from experts. In this paper, the induction of rules directly from meteorological data is explored-a process called machine learning. A commercial machine learning program called C4.5, is applied to a meteorological problem, forecasting maritime fog, for which a reliable expert system has been previously developed. Two detasets are used: 1) weather ship observations originally used for testing and evaluating the expert system, and 2) buoy measurements taken off the coast of California. For both dataset...\"}, {\"paperId\": \"eb135cbd77d68b8f75507c8c70834d1f877de068\", \"abstract\": \"This work focuses on methods and algorithms developed within the artificial intelligence community. These include machine learning, data mining, and pattern recognition. These methods provide solutions for the challenges posed by the progressive transformation of biology into data-massive science.\"}, {\"paperId\": \"9f87f0a96b3e8c6487e1532fb75a39f921ed0a1e\", \"abstract\": \"As one of the most rapidly developing artificial intelligence techniques, deep learning has been applied in various machine learning tasks and has received great attention in data science and statistics. Regardless of the complex model structure, deep neural networks can be viewed as a nonlinear and nonparametric generalization of existing statistical models. In this review, we introduce several popular deep learning models including convolutional neural networks, generative adversarial networks, recurrent neural networks, and autoencoders, with their applications in image data, sequential data and recommender systems. We review the architecture of each model and highlight their connections and differences compared with conventional statistical models. In particular, we provide a brief survey of the recent works on the unique overparameterization phenomenon, which explains the strengths and advantages of using an extremely large number of parameters in deep learning. In addition, we provide a practical guidance on optimization algorithms, hyperparameter tuning, and computing resources.\"}, {\"paperId\": \"249271a716502b845a47d541c77f8d0fc61e9fb7\", \"abstract\": \"Classification is one of the interesting areas in the academic field of Neural Networks. Artificial Neural Networks (ANNs) have been extensively used in pattern recognition and classification of data in the supervised and unsupervised environment. The ANNs use advanced concepts of computer science where a machine mimics human intelligence while learning from possible experience. To make a machine self-adaptive and autonomous, the machine is properly trained on a training data-set and then subsequently tested on new data. The excellent quality of training of ANNs typically depends on the underlying architecture of the network they employ, for a specific instance, a considerable number of deep layers, number of key nodes in each distinct layer, epoch size, and activation function. In this academic paper, the practical importance of these architectural components is carefully investigated. This paper is precisely about providing a solution that how ANNs can help us in Breast Cancer Classification. Furthermore, sufficient proofs of some extremely important terminologies used in ANNs are also discussed which will clarify the important concepts of ANNs.\"}, {\"paperId\": \"06c3747ab96e923bc7f2126d088e46c4995c0da2\", \"abstract\": null}, {\"paperId\": \"6ca294f37ef68858f1279a79836a7d5090e5676d\", \"abstract\": \"Many private and public actors are incentivized by the promises of big data technologies: digital tools underpinned by capabilities like artificial intelligence and machine learning. While many shared value propositions exist regarding what these technologies afford, public-facing concerns related to individual privacy, algorithm fairness, and the access to insights requires attention if the widespread use and subsequent value of these technologies are to be fully realized. Drawing from perspectives of data science, social science and technology acceptance, we present an interdisciplinary analysis that links these concerns with traditional research and development (R&D) activities. We suggest a reframing of the public R&D \\u2018brand\\u2019 that responds to legitimate concerns related to data collection, development, and the implementation of big data technologies. We offer as a case study Australian agriculture, which is currently undergoing such digitalization, and where concerns have been raised by landholders and the research community. With seemingly limitless possibilities, an updated account of responsible R&D in an increasingly digitalized world may accelerate the ways in which we might realize the benefits of big data and mitigate harmful social and environmental costs.\"}, {\"paperId\": \"25accd86a1ea68d9da03fee11b1f332cdae2f84a\", \"abstract\": \"Structural Health Monitoring (SHM) has been continuously benefiting from the advancements in the field of data science. Various types of Artificial Intelligence (AI) methods have been utilized to assess and evaluate civil structures. In AI, Machine Learning (ML) and Deep Learning (DL) algorithms require plenty of datasets to train; particularly, the more data DL models are trained with, the better output it yields. Yet, in SHM applications, collecting data from civil structures through sensors is expensive and obtaining useful data (damage associated data) is challenging. In this paper, one-dimensional (1-D) Wasserstein loss Deep Convolutional Generative Adversarial Networks using Gradient Penalty (1-D WDCGAN-GP) is utilized to generate damage-associated vibration datasets that are similar to the input. For the purpose of vibration-based damage diagnostics, a 1-D Deep Convolutional Neural Network (1-D DCNN) is built, trained, and tested on both real and generated datasets. The classification results from the 1-D DCNN on both datasets resulted in being very similar to each other. The presented work in this paper shows that, for the cases of insufficient data in DL or ML-based damage diagnostics, 1-D WDCGAN-GP can successfully generate data for the model to be trained on.\"}, {\"paperId\": \"0bc3e3f736e907cba3026e7b7b6bc86c2bfef09a\", \"abstract\": \"The core tools of science (data, software, and computers) are undergoing a rapid and historic evolution, changing what questions scientists ask and how they find answers. Earth science data are being transformed into new formats optimized for cloud storage that enable rapid analysis of multi\\u2010petabyte data sets. Data sets are moving from archive centers to vast cloud data storage, adjacent to massive server farms. Open source cloud\\u2010based data science platforms, accessed through a web\\u2010browser window, are enabling advanced, collaborative, interdisciplinary science to be performed wherever scientists can connect to the internet. Specialized software and hardware for machine learning and artificial intelligence are being integrated into data science platforms, making them more accessible to average scientists. Increasing amounts of data and computational power in the cloud are unlocking new approaches for data\\u2010driven discovery. For the first time, it is truly feasible for scientists to bring their analysis to data in the cloud without specialized cloud computing knowledge. This shift in paradigm has the potential to lower the threshold for entry, expand the science community, and increase opportunities for collaboration while promoting scientific innovation, transparency, and reproducibility. Yet, we have all witnessed promising new tools which seem harmless and beneficial at the outset become damaging or limiting. What do we need to consider as this new way of doing science is evolving?\"}, {\"paperId\": \"98b764edf465704a8d762db5a323697d00548509\", \"abstract\": null}, {\"paperId\": \"d2972490fb09ceb0c2b7949f0456770854d1cbfe\", \"abstract\": \"This chapter introduces data science with its history and importance in this modern era briefly. This chapter also elaborates the discussion by relating data science to various modern fields like big data analytics, artificial intelligence, deep learning, and machine learning. This chapter also discuss the necessary of data analytics in this big data era. This chapter also briefly introduces another emerging field, Internet of Things (IoT) and explores the contribution IoT towards big data analytics and data science in research perspective. It also briefly introduces the programming and non-programming tools used in the data science field.\"}, {\"paperId\": \"85300b7d9ed70f982f3eb51083971c03d5bb63fc\", \"abstract\": \"Abstract Data and data science offer tremendous potential to address some of our most intractable public problems (including the Covid-19 pandemic). At the same time, recent years have shown some of the risks of existing and emerging technologies. An updated framework is required to balance potential and risk, and to ensure that data is used responsibly. Data responsibility is not itself a new concept. However, amid a rapidly changing technology landscape, it has become increasingly clear that the concept may need updating, in order to keep up with new trends such as big data, open data, the Internet of things, and artificial intelligence, and machine learning. This paper seeks to outline 10 approaches and innovations for data responsibility in the 21st century. The 10 emerging concepts we have identified include: End-to-end data responsibility Decision provenance Professionalizing data stewardship From data science to question science Contextual consent Responsibility by design Data asymmetries and data collaboratives Personally identifiable inference Group privacy Data assemblies Each of these is described at greater length in the paper, and illustrated with examples from around the world. Put together, they add up to a framework or outline for policy makers, scholars, and activists who seek to harness the potential of data to solve complex social problems and advance the public good. Needless to say, the 10 approaches outlined here represent just a start. We envision this paper more as an exercise in agenda-setting than a comprehensive survey.\"}, {\"paperId\": \"5a4ee4b755bef0e52f54bcfc41d4a682fa7f23cb\", \"abstract\": null}, {\"paperId\": \"43a9a056ac215dbad3d8905ea514f9408a065612\", \"abstract\": \"Machine learning methods originated from artificial intelligence and today are applied in several fields concerning environmental sciences. Thanks to their powerful nonlinear modelling capability, machine learning methods today are utilized in satellite data processing, general circulation models(GCM), weather and climate prediction, air quality forecasting, analysis and modelling of environmental data, oceanographic and hydrological forecasting, ecological modelling, and monitoring of snow, ice and forests. Currently, the popularity of neural networks is growing; their areas of application are constantly expanding. In these conditions, the task of choosing a convenient tool for utilizing in environmental science with neural networks becomes urgent. There are many tools for working with neural networks, but each of them has its own drawbacks. So most of the existing tools require users to have programming knowledge; there are no tools to help quickly select the optimal network structure for the problem being solved. The purpose of the research is to simplify the process of choosing the optimal structure of an artificial neural network by developing an application with a graphical user interface with a visual representation of the stages of creating and learning neural networks in environmental sciences. The object of research is artificial feed-forward neural networks. Research work on the study, comparison and analysis of existing tools for the creation, learning and use of artificial neural networks has been carried out. Based on the research results, an application with a graphical interface aimed at solving the assigned tasks has been developed. An application developed to achieve this goal works correctly, without failures, and allows creating and learning feed-forward neural networks without programming knowledge.\"}, {\"paperId\": \"a0d7cca1783383880f9a331f7764da698d2ae597\", \"abstract\": \"Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation\\u2010based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples.\"}, {\"paperId\": \"83f03ed371bce26a212c6ec056f9db389daed7b5\", \"abstract\": \"Learning algorithms are central to pattern recognition, artificial intelligence, machine learning, data mining, and statistical learning. The term often implies analysis of large and complex data sets with minimal human intervention. Bayesian learning has been variously described as a method of updating opinion based on new experience, updating parameters of a process model based on data, modelling and analysis of complex phenomena using multiple sources of information, posterior probabilistic expectation, and so on. In all of these guises, it has exploded in popularity over recent years. General texts on Bayesian statistics include Bernardo and Smith (1994), Gelman, Carlin, Stern, and Rubin (1995), and Lee (1997). Texts that derive more from the information science discipline, such as Mitchell (1997) and Sarker, Abbass, and Newton (2002), also include sections on Bayesian learning. Given recent advances and the intuitive appeal of the methodology, Bayesian learning is poised to become one of the dominant platforms for modelling and analysis in the 21st century. This article provides an overview of Bayesian learning in this context.\"}, {\"paperId\": \"7beea5172d5215a2e36d935575470a556e2b645f\", \"abstract\": \"The term Artificial Intelligence (AI) is a branch of computer science to make computers perform human-like tasks, and thus, computers can appropriately sense and learn inputs for perception, knowledge representation, reasoning, problem-solving, and planning. Various types of innovative AI technologies are designed to imitate the cognitive abilities of human beings, which can, therefore, deal with more complicated and ill-defined problems in an intentional, intelligent, and adaptive manner. Typically, AI can be regarded as a conjunction of machine learning and data analytics.\"}, {\"paperId\": \"038bf2435d04d2fae617910d03ebab69ec2e0293\", \"abstract\": \"Recent progress in the field of artificial intelligence, machine learning and also in computer industry resulted in the ongoing boom of using these techniques as applied to solving complex tasks in both science and industry. Same is, of course, true for the financial industry and mathematical finance. In this paper we consider a classical problem of mathematical finance - calibration of option pricing models to market data, as it was recently drawn some attention of the financial society in the context of deep learning and artificial neural networks. We highlight some pitfalls in the existing approaches and propose resolutions that improve both performance and accuracy of calibration. We also address a problem of no-arbitrage pricing when using a trained neural net, that is currently ignored in the literature.\"}, {\"paperId\": \"3bccce47617ea58d6326ae40a2663c7b1f4aa577\", \"abstract\": \"The Internet of Things (IoT) devices generate a large amount of data over networks; therefore, the efficiency, complexity, interfaces, dynamics, robustness, and interaction need to be re-examined on a large scale. This phenomenon will lead to seamless network connectivity and the capability to provide support for the IoT. The traditional IoT is not enough to provide support. Therefore, we designed this study to provide a systematic analysis of next-generation advancements in the IoT. We propose a systematic catalog that covers the most recent advances in the traditional IoT. An overview of the IoT from the perspectives of big data, data science, and network science disciplines and also connecting technologies is given. We highlight the conceptual view of the IoT, key concepts, growth, and most recent trends. We discuss and highlight the importance and the integration of big data, data science, and network science along with key applications such as artificial intelligence, machine learning, blockchain, federated learning, etc. Finally, we discuss various challenges and issues of IoT such as architecture, integration, data provenance, and important applications such as cloud and edge computing, etc. This article will provide aid to the readers and other researchers in an understanding of the IoT\\u2019s next-generation developments and tell how they apply to the real world.\"}, {\"paperId\": \"632231776b7ce403dbd5b043d8f621eda366aae8\", \"abstract\": \"Recently, artificial intelligence (AI) receive increasing attention within the field of developing smart digital education. Researchers have been used the computational intelligence (CI) and machine learning techniques methodologies to develop a smart tutoring systems (STSs). On the other side, the convergence of AI, data science and Internet of Things (IoT) is enabling the creation of a new generation of web-based smart systems for all educational and learning tasks. This paper discusses the CI and knowledge engineering paradigms for developing the smart educational and learning systems. In this study the two popular CI paradigms; case-based reasoning and ontological engineering are discussed and analyzed namely. The main objective of this study is to determine and exploration the benefits and advantages of such intelligent paradigms to increase the effectiveness and enhancing the efficiency of the smart tutoring systems. Moreover, the paper addresses the challenges faced by the application developers and knowledge engineers in developing and deploying such systems.\"}, {\"paperId\": \"632c5a15d4e1ba0b3f11a6f5989829206f9044ae\", \"abstract\": \"Amazon.com Inc. seeks alternative ways to improve manual transactions system of granting employees resources access in the field of data science. The work constructs a modified Artificial Neural Network (ANN) by incorporating a Discrete Hopfield Neural Network (DHNN) and Clonal Selection Algorithm (CSA) with 3-Satisfiability (3-SAT) logic to initiate an Artificial Intelligence (AI) model that executes optimization tasks for industrial data. The selection of 3-SAT logic is vital in data mining to represent entries of Amazon Employees Resources Access (AERA) via information theory. The proposed model employs CSA to improve the learning phase of DHNN by capitalizing features of CSA such as hypermutation and cloning process. This resulting the formation of the proposed model, as an alternative machine learning model to identify factors that should be prioritized in the approval of employees resources applications. Subsequently, reverse analysis method (SATRA) is integrated into our proposed model to extract the relationship of AERA entries based on logical representation. The study will be presented by implementing simulated, benchmark and AERA data sets with multiple performance evaluation metrics. Based on the findings, the proposed model outperformed the other existing methods in AERA data extraction.\"}, {\"paperId\": \"b940da4f329442385a7e6b0df36714aa0cd2dab1\", \"abstract\": \"Those working on policy, digital ethics and governance often refer to issues in 'computer science', that includes, but is not limited to, common subfields such as Artificial Intelligence (AI), Computer Science (CS) Computer Security (InfoSec), Computer Vision (CV), Human Computer Interaction (HCI), Information Systems, (IS), Machine Learning (ML), Natural Language Processing (NLP) and Systems Architecture. Within this framework, this paper is a preliminary exploration of two hypotheses, namely 1) Each community has differing inclusion of minoritised groups (using women as our test case, by identifying female-sounding names); and 2) Even where women exist in a community, they are not published representatively. Using data from 20,000 research records, totalling 503,318 names, preliminary data supported our hypothesis. We argue that ACM has an ethical duty of care to its community to increase these ratios, and to hold individual computing communities to account in order to do so, by providing incentives and a regular reporting system, in order to uphold its own Code.\"}, {\"paperId\": \"669a051c6c3754c72979d708e45cf5757343cc6f\", \"abstract\": \"Prediction of chemical bioactivity and physical properties has been one of the most important applications of statistical and more recently, machine learning and artificial intelligence methods in chemical sciences. This field of research, broadly known as quantitative structure-activity relationships (QSAR) modeling, has developed many important algorithms and has found a broad range of applications in physical organic and medicinal chemistry in the past 55+ years. This Perspective summarizes recent technological advances in QSAR modeling but it also highlights the applicability of algorithms, modeling methods, and validation practices developed in QSAR to a wide range of research areas outside of traditional QSAR boundaries including synthesis planning, nanotechnology, materials science, biomaterials, and clinical informatics. As modern research methods generate rapidly increasing amounts of data, the knowledge of robust data-driven modelling methods professed within the QSAR field can become essential for scientists working both within and outside of chemical research. We hope that this contribution highlighting the generalizable components of QSAR modeling will serve to address this challenge.\"}, {\"paperId\": \"e24c6ad05258817427eec51d7b557c01f4699140\", \"abstract\": \"The development and promotion of teaching-enhanced learning tools in the academic field is leading to the collection of a large amount of data generated from the usual activity of students and teachers. The analysis of these data is an opportunity to improve many aspects of the learning process: recommendations of activities, dropout prediction, performance and knowledge analysis, resources optimization, etc. However, these improvements would not be possible without the application of computer science techniques that have demonstrated a high effectiveness for this purpose: data mining, big data, machine learning, deep learning, collaborative filtering, and recommender systems, among other fields related to intelligent systems. This Special Issue provides 17 papers that show advances in the analysis, prediction, and recommendation of applications propelled by artificial intelligence, big data, and machine learning in the teaching-enhanced learning context.\"}, {\"paperId\": \"427a680cb8ab78ad8dfe8d88511129bd353ec211\", \"abstract\": \"Measurement of biological systems containing biomolecules and bioparticles is a key task in the fields of analytical chemistry, biology, and medicine. Driven by the complex nature of biological systems and unprecedented amounts of measurement data, artificial intelligence (AI) in measurement science has rapidly advanced from the use of silicon-based machine learning (ML) for data mining to the development of molecular computing with improved sensitivity and accuracy. This review presents an overview of fundamental ML methodologies and discusses their applications in disease diagnostics, biomarker discovery, and imaging analysis. We next provide the working principles of molecular computing using logic gates and arithmetical devices, which can be employed for in situ detection, computation, and signal transduction for biological systems. This review concludes by summarizing the strengths and limitations of AI-involved biological measurement in fundamental and applied research. Expected final online publication date for the Annual Review of Analytical Chemistry, Volume 14 is June 2021. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.\"}, {\"paperId\": \"716819f2be1df499434ceec76436a3e261d39ef7\", \"abstract\": \"Hydrologic sciences depend on data monitoring, analyses, and simulations of hydrologic processes to ensure safe, sufficient, and equal water distribution. These hydrologic data come from but are not limited to primary (lab, plot, and field experiments) and secondary sources (remote sensing, UAVs, hydrologic models) that typically follow FAIR Principles (Findable, Accessible, Interoperable, and Reusable: (go-fair.org)). Easy availability of FAIR data has become possible because the hydrology\\u2010oriented organizations have pushed the community to increase coordination of the protocols for generating data and sharing model platforms. In addition, networking at all levels has emerged with an invigorated effort to activate community science efforts that complement conventional data collection methods. However, it has become difficult to decipher various complex hydrologic processes with increasing data. Machine learning, a branch of artificial intelligence, provide more accurate and faster alternatives to better understand different hydrological processes. The Integrated, Coordinated, Open, Networked (ICON) framework provides a pathway for water users to include and respect diversity, equity, and inclusivity. In addition, ICONs support the integration of peoples with historically marginalized identities into this professional discipline of water sciences. This article comprises three independent commentaries about the state of ICON principles in hydrology and discusses the opportunities and challenges of adopting them.\"}, {\"paperId\": \"14296e9c2eb75f1516885d92e7e029795ec9e8f2\", \"abstract\": null}, {\"paperId\": \"0ff48839dd4c027dd757ab43d1dd4c99f71ed22b\", \"abstract\": \"Structural Health Monitoring (SHM) of civil structures has been constantly evolving with novel methods, advancements in data science, and more accessible technology to address issues related to structural safety, operations, and resiliency. Research and development in the civil SHM field during the last few decades have been progressive due to the increasing use of Artificial Intelligence (AI) methods such as Machine Learning (ML) and Deep Learning (DL). Particularly, Generative Adversarial Networks (GAN), which is a subfamily of Deep Learning has been highly favored in the SHM community within the last couple of years. After its release in 2014, GANs (original GAN and other GAN variants) have been in use for a wide variety of applications in various disciplines, and it has been one of the most popular research topics in the AI-ML domain. While there has not been a review study on the applications of GAN in the civil SHM field, this paper aims to fill this gap by presenting a literature review of the studies that employed GAN specifically in civil SHM applications from 2014 to date, in a condensed format. This study intends to inform SHM practitioners and researchers about GANs and present the highlights of the published work on GANs in the civil SHM field.\"}, {\"paperId\": \"7722545dd33c3cac0250e0a322b2e17f54bd73a7\", \"abstract\": \"AI (Artificial Intelligence) technologies help with diagnostic procedures, patient monitoring, drug development, personalised medicine, and global health pandemic prediction. Without explicit programming, machine learning enables the technology to enhance and expand. Moreover, machine learning algorithms may use e-health records to evaluate vast quantities of data, dubbed big-data, for illness detection and prediction. Medical devices are used to continually monitor and record a person\\u2019s health condition. In light of recent research, the goal of this study is to investigate the potential advantages of extensive data analytics and machine learning (ML). We looked for articles in the literature using multiple high-quality databases, including Google Scholar, Scopus, Web of science (WoS), Scopus and PubMed. This article covers the ML (machine learning), Big Data, and Blockchain Technology in health-care, population health, and medical surveillance.\"}, {\"paperId\": \"dfedc348fb9438370978525d8c4213d379936a89\", \"abstract\": \"Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.\"}, {\"paperId\": \"7d3d85f9020be9b44ab3714cf2094f54fc65770f\", \"abstract\": \"A growing list of high-profile controversies involving the social impacts of artificial intelligence systems (AI), digital data collection and algorithmic analysis have forced difficult conversations around the ethics of data-intensive digital technologies and so-called \\\"big data\\\" research. These incidents are directly relevant to newly coalescing cultures of \\\"data science,\\\" an emergent field which seeks both to interpret and capitalize on the creation, collection, and processing of knowledge through large collections of digital data, often in conjunction with particular techniques like machine learning (ML). The long list of recent public controversies, as Brian Beaton observes, lays bare data science's extant lack of direction regarding professional ethics or values.\"}, {\"paperId\": \"a0350117b017deff660a0cb4ec2dfeaaf64c62b1\", \"abstract\": \"Computational propaganda is an emergent form of political manipulation that occurs over the Internet. The term describes the assemblage of social media platforms, autonomous agents, algorithms, and big data tasked with the manipulation of public opinion. Our research shows that this new mode of interrupting and influencing communication is on the rise around the globe. Advances in computing technology, especially around social automation, machine learning, and artificial intelligence mean that computational propaganda is becoming more sophisticated and harder to track at an alarming rate. This introduction explores the foundations of computational propaganda. It describes the key role that automated manipulation of algorithms plays in recent efforts to control political communication worldwide. We discuss the social data science of political communication and build upon the argument that algorithms and other computational tools now play an important political role in areas like news consumption, issue awareness, and cultural understanding. We unpack the key findings of the nine country case studies that follow\\u2014exploring the role of computational propaganda during events from local and national elections in Brazil to the ongoing security crisis between Ukraine and Russia. Our methodology in this work has been purposefully mixed, we make use of quantitative analysis of data from several social media platforms and qualitative work that includes interviews with the people who design and deploy political bots and disinformation campaigns. Finally, we highlight original evidence about how this manipulation and amplification of disinformation is produced, managed, and circulated by political operatives and governments and describe paths for both democratic intervention and future research in this space.\"}, {\"paperId\": \"bad5d32aa8cad56373babd7c08c1c46b8493256e\", \"abstract\": \"Nature-inspired metaheuristic algorithms have become powerful and popular in computational intelligence and many applications. There are some important developments in recent years, and this special issue aims to provide a timely review of such developments, including ant colony optimization, bat algorithm, cuckoo search, particle swarm optimization, genetic algorithms, support vector machine, neural networks, and others. In addition, these algorithms have been applied in a diverse range of applications, and some of these latest applications are also summarized here. \\n \\nComputational intelligence and metaheuristic algorithms have become increasingly popular in computer science, artificial intelligence, machine learning, engineering design, data mining, image processing, and data-intensive applications. Most algorithms in computational intelligence and optimization are based on swarm intelligence (SI) [1, 2]. For example, both particle swarm optimization [1] and cuckoo search [3] have attracted much attention in science and engineering. They both can effectively deal with continuous problems [2] and combinatorial problems [4]. These algorithms are very different from the conventional evolutionary algorithms such as genetic algorithms and simulated annealing [5, 6] and other heuristics [7]. \\n \\nMany new optimization algorithms are based on the so-called swarm intelligence (SI) with diverse characteristics in mimicking natural systems [1, 2]. Consequently, different algorithms may have different features and thus may behave differently, even with different efficiencies. However, It still lacks in-depth understanding why these algorithms work well and exactly under what conditions, though there were some good studies that may provide insight into algorithms [2, 8]. \\n \\nThis special issue focuses on the recent developments of SI-based metaheuristic algorithms and their diverse applications as well as theoretical studies. Therefore, this paper is organized as follows. Section 2 provides an introduction and comparison of the so-called infinite monkey theorem and metaheuristics, followed by the brief review of computational intelligence and metaheuristics in Section 3. Then, Section 4 touches briefly the state-of-the-art developments, and finally, Section 5 provides some open problems about some key issues concerning computational intelligence and metaheuristics.\"}, {\"paperId\": \"567221e67bfd38ee9d94084697cd5d99a224e18b\", \"abstract\": \"Artificial Intelligence (AI) is one of the major fields in computer science that is used to create the intellectual machine. AI refers to the imitation of human intelligence in machines that are programmed to work like humans and mimic their actions.[6] Deep Learning (DL) is one of the podium of artificial intelligence that work like a human brain in processing data and creating forms for use in decision making[1]. Sleep plays the important role in human life, they cannot sleep properly because some sleep related disorders can occur like sleep apnea, insomnia, hypersomnia. The preprocessing the feature extraction data, the sleep pattern features are import into a Bidirectional LSTM network for classifying the sleep stages [3]. The Bi-directional and Stacked LSTM are used to predict these disorders. Bi-directional LSTM is the part of recurrent neural network structure, it performed both forward and backward to predict the sleep related disorders.\"}, {\"paperId\": \"c62fdc1881dd26001b8f7eff582b4d615762754f\", \"abstract\": \"Abstract The increase in the expectations of artificial intelligence (AI) technology has led to machine learning technology being actively used in the medical field. Non-negative matrix factorization (NMF) is a machine learning technique used for image analysis, speech recognition, and language processing; recently, it is being applied to medical research. Precision medicine, wherein important information is extracted from large-scale medical data to provide optimal medical care for every individual, is considered important in medical policies globally, and the application of machine learning techniques to this end is being handled in several ways. NMF is also introduced differently because of the characteristics of its algorithms. In this review, the importance of NMF in the field of medicine, with a focus on the field of oncology, is described by explaining the mathematical science of NMF and the characteristics of the algorithm, providing examples of how NMF can be used to establish precision medicine, and presenting the challenges of NMF. Finally, the direction regarding the effective use of NMF in the field of oncology is also discussed.\"}, {\"paperId\": \"7e633583dfb80a6320779c8fa18290dab833a17e\", \"abstract\": \"Probabilistic numerical computation formalises the connection between machine learning and applied mathematics. Numerical algorithms approximate intractable quantities from computable ones. They estimate integrals from evaluations of the integrand, or the path of a dynamical system described by differential equations from evaluations of the vector field. In other words, they infer a latent quantity from data. This book shows that it is thus formally possible to think of computational routines as learning machines, and to use the notion of Bayesian inference to build more flexible, efficient, or customised algorithms for computation. The text caters for Masters' and PhD students, as well as postgraduate researchers in artificial intelligence, computer science, statistics, and applied mathematics. Extensive background material is provided along with a wealth of figures, worked examples, and exercises (with solutions) to develop intuition.\"}, {\"paperId\": \"516707e871504f07e8ff64d2f8858fb67190ac4e\", \"abstract\": \"Introduction\\u2014 Driven by advances in computational hardware such as graphical processing units and research in optimization and expressibility, recent progress in machine learning and artificial intelligence has allowed analyzing and modeling large amounts of high-dimensional data. These models and methods are now extensively used in numerous applications, both in industry and in science [1]. A particularly general approach to capturing the features of a given dataset is modeling its underlying probability distribution. This subfield is called generative modeling, and allows to apply statistical techniques on the distribution, such as drawing samples from it, and calculating marginal and conditional distribution of the variables. Techniques from many-body physics have always played a major role in the development of generative machine learning [2, 3]. This can be traced back to the parallels between the respective problems one has to deal with in both fields. For example, classical manybody physics concerns modeling the partition function of a collection of many degrees of freedom, and subsequently evaluating expectation values from it.\"}, {\"paperId\": \"c25ab5fa8c705f3416393fc94bcef77b5b9c9438\", \"abstract\": \"With the increasing role of ICT in enabling and supporting smart cities, the demand for big data analytics solutions is increasing. Various artificial intelligence, data mining, machine learning and statistical analysis\\u2010based solutions have been successfully applied in thematic domains like climate science, energy management, transport, air quality management and weather pattern analysis. In this paper, we present a systematic review of the literature on smart city big data analytics. We have searched a number of different repositories using specific keywords and followed a structured data mining methodology for selecting material for the review. We have also performed a technological and thematic analysis of the shortlisted literature, identified various data mining/machine learning techniques and presented the results. Based on this analysis we also present a classification model that studies four aspects of research in this domain. These include data models, computing models, security and privacy aspects and major market drivers in the smart cities domain. Moreover, we present a gap analysis and identify future directions for research. For the thematic analysis we identified the themes smart city governance, economy, environment, transport and energy. We present the major challenges in these themes, the major research work done in the field of data analytics to address these challenges and future research directions.\"}, {\"paperId\": \"a44c042e7c119107c3f9069383c0381c1a69be85\", \"abstract\": \"Since the invention of computers or machines, their capability to perform various tasks has experienced an exponential growth. In the current times, data science and analytics, a branch of computer science, has revived due to the major increase in computer power, presence of huge amounts of data, and better understanding in techniques in the area of Data Analytics, Artificial Intelligence, Machine Learning, Deep Learning etc. Hence, they have become an essential part of the technology industry, and are being used to solve many challenging problems. In the search for a good programming language on which many data science applications can be developed, python has emerged as a complete programming solution. Due to the low learning curve, and flexibility of Python, it has become one of the fastest growing languages. Python\\u2019s ever-evolving libraries make it a good choice for Data analytics. The paper talks about the features and characteristics of Python programming language and later discusses reasons behind python being credited as one of the fastest growing programming language and why it is at the forefront of data science applications, research and development.\"}, {\"paperId\": \"6bef49c93a57d258469c4b2b14aaf7676b0eb480\", \"abstract\": \"Decision trees have become one of the most powerful and popular approaches in knowledge discovery and data mining; it is the science of exploring large and complex bodies of data in order to discover useful patterns. Decision tree learning continues to evolve over time. Existing methods are constantly being improved and new methods introduced. This 2nd Edition is dedicated entirely to the field of decision trees in data mining; to cover all aspects of this important technique, as well as improved or new methods and techniques developed after the publication of our first edition. In this new edition, all chapters have been revised and new topics brought in. New topics include Cost-Sensitive Active Learning, Learning with Uncertain and Imbalanced Data, Using Decision Trees beyond Classification Tasks, Privacy Preserving Decision Tree Learning, Lessons Learned from Comparative Studies, and Learning Decision Trees for Big Data. A walk-through guide to existing open-source data mining software is also included in this edition. This book invites readers to explore the many benefits in data mining that decision trees offer: Self-explanatory and easy to follow when compacted Able to handle a variety of input data: nominal, numeric and textual Scales well to big data Able to process datasets that may have errors or missing values High predictive performance for a relatively small computational effort Available in many open source data mining packages over a variety of platforms Useful for various tasks, such as classification, regression, clustering and feature selection Readership: Researchers, graduate and undergraduate students in information systems, engineering, computer science, statistics and management.\"}, {\"paperId\": \"479028717a10bd94c35aa8ab0ec9f0579e42d3d7\", \"abstract\": \"The combination of computational chemistry and computational materials science with machine learning and artificial intelligence provides a powerful way of relating structural features of nanomaterials with functional properties. However, combining these fundamentally different scientific approaches is not as straightforward as it seems. Machine learning methods were developed for large data sets with small numbers of consistent features. Typically nanomaterials data sets are small, with high dimensionality and high variance in the feature space, and suffer from numerous destructive biases. None of the established data science or machine learning methods in widespread use today were devised with (nano)materials data sets in mind, but there are ways to overcome these challenges and use them reliably. In this review we will discuss domain-specific constraints on data-driven nanomaterials design, and explore the differences between nanomaterials simulation and nanoinformatics that can be leveraged for greater impact.\"}, {\"paperId\": \"4fdabdc5b7f359312f2488e17259767e43489a3b\", \"abstract\": null}, {\"paperId\": \"a40bf4700d4fa10a206144fa4469e5efec68fa3a\", \"abstract\": \": Knowledge graph (KG) is a topic of great interests to geoscientists as it can be deployed through-out the data life cycle in data-intensive geoscience studies. Nevertheless, comparing with the large amounts of publications on machine learning applications in geosciences, summaries and reviews of geoscience KGs are still limited. The aim of this paper is to present a comprehensive review of KG construction and implementation in geosciences. It consists of four major parts: 1) concepts relevant to KG and approaches for KG construction, 2) KG application in data collection, curation, and service, 3) KG application in data analysis, and 4) challenges and trends of geoscience KG creation and application in the near future. For each of the first three parts, a list of concepts, exemplar studies, and best practices are summarized. Those summaries are synthesized together in the challenge and trend analyses. As artificial intelligence and data science are thriving in geosciences, we hope this review of geoscience KGs can be of value to practitioners in data-intensive geoscience studies.\"}, {\"paperId\": \"ccd189f33414bfe8715105ed23f762bfa7f40783\", \"abstract\": \"Computer audition (CA) has experienced a fast development in the past decades by leveraging advanced signal processing and machine learning techniques. In particular, for its noninvasive and ubiquitous character by nature, CA-based applications in healthcare have increasingly attracted attention in recent years. During the tough time of the global crisis caused by the coronavirus disease 2019 (COVID-19), scientists and engineers in data science have collaborated to think of novel ways in prevention, diagnosis, treatment, tracking, and management of this global pandemic. On the one hand, we have witnessed the power of 5G, Internet of Things, big data, computer vision, and artificial intelligence in applications of epidemiology modeling, drug and/or vaccine finding and designing, fast CT screening, and quarantine management. On the other hand, relevant studies in exploring the capacity of CA are extremely lacking and underestimated. To this end, we propose a novel multitask speech corpus for COVID-19 research usage. We collected 51 confirmed COVID-19 patients\\u2019 in-the-wild speech data in Wuhan city, China. We define three main tasks in this corpus, i.e., three-category classification tasks for evaluating the physical and/or mental status of patients, i.e., sleep quality, fatigue, and anxiety. The benchmarks are given by using both classic machine learning methods and state-of-the-art deep learning techniques. We believe this study and corpus cannot only facilitate the ongoing research on using data science to fight against COVID-19, but also the monitoring of contagious diseases for general purpose.\"}, {\"paperId\": \"5d630c2ff833887abb596f5c6a1b0e2caaeacc3f\", \"abstract\": \"Research has become increasingly more interdisciplinary over the past few years. Artificial intelligence and its sub-fields have proven valuable for interdisciplinary research applications, especially physical sciences. Recently, machine learning-based mechanisms have been adapted for material science applications, meeting traditional experiments\\u2019 challenges in a time and cost-efficient manner. The scientific community focuses on harnessing varying mechanisms to process big data sets extracted from material databases to derive hidden knowledge that can successfully be employed in technical frameworks of material screening, selection, and recommendation. However, a plethora of underlying aspects of the existing material discovery methods needs to be critically assessed to have a precise and collective analysis that can serve as a baseline for various forthcoming material discovery problems. This study presents a comprehensive survey of state-of-the-art benchmark data sets, detailed pre-processing and analysis, appropriate learning model mechanisms, and simulation techniques for material discovery. We believe that such an in-depth analysis of the mentioned aspects provides promising directions to the young interdisciplinary researchers from computing and material science fields. This study will help devise useful modeling in the materials discovery to positively contribute to the material industry, reducing the manual effort involved in the traditional material discovery. Moreover, we also present a detailed analysis of experimental and computation-based artificial intelligence mechanisms suggested by the existing literature.\"}, {\"paperId\": \"f60856ad4d8e0f88da818930bb18aedb58b0dbdf\", \"abstract\": null}, {\"paperId\": \"94fd596b8197fd0eca14a5aa6255d8012b10ca75\", \"abstract\": \"In recent years, Deep Learning, Machine Learning, and Artificial Intelligence are highly focused concepts of data science. Deep learning has achieved success in the field of Computer Vision, Speech and Audio Processing, and Natural Language Processing. It has the strong learning ability that can improve utilization of datasets for the feature extraction compared to traditional Machine Learning Algorithm. Perceptron is the essential building block for creating a deep Neural Network. The perceptron model is the more general computational model. It analyzes the unsupervised data, making it a valuable tool for data analytics. A key task of this paper is to develop and analyze learning algorithm. It begins with deep learning with perceptron and how to apply it using TensorFlow to solve various issues. The main part of this paper is to make perceptron learning algorithm well behaved with non-separable training datasets. This type of algorithm is suitable for Machine Learning, Deep Learning, Pattern Recognition, and Connectionist Expert System.\"}, {\"paperId\": \"ca8fbaa02af1b7b1dd22d30371801d6930fe9d1d\", \"abstract\": \"Machine learning is a branch of Artificial Intelligence(AI) which is heavily used in the field of data science. It has a strong potential in health-related data analysis for automated disease prediction. The work focuses on three different machine learning techniques, i.e., DBSCAN, K-Means, and Affinity Propagation to compare their prediction accuracy and computational complexity. The study concentrates on liver disease-related health care data set and uses the Silhouette coefficient for comparative performance measurement of the three techniques mentioned above. The Silhouette coefficient determines prediction accuracy giving K-Means as the optimal method. The overall results will then be analyzed on the basis of prediction accuracy and computational complexity to determine the best technique for prediction of liver diseases using unsupervised machine learning.\"}, {\"paperId\": \"616e20a7101a927af5f4139dcf5ee0cb233de80c\", \"abstract\": \"This synthesis report presents the scientific results of the international conference \\\"Global Challenges and Data-Driven Science\\\" which took place in St. Petersburg, Russian Federation from 8 October to 13 October 2017. This event facilitated multidisciplinary scientific dialogue between leading scientists, data managers and experts, as well as Big Data researchers of various fields of knowledge. The St. Petersburg conference covered a wide range of topics related to data science. It featured discussions covering the collection and processing of large amounts of data, the implementation of system analysis methods into data science, machine learning, data mining, pattern recognition, decision-making robotics and algorithms of artificial intelligence. The conference was an outstanding event in the field of scientific diplomacy and brought together more than 150 participants from 35 countries. It's success ensured the effective data science dialog between nations and continents and established a new platform for future collaboration.\"}, {\"paperId\": \"f39dddca3988ad3613bdf2b7c3a90d420b1681b5\", \"abstract\": null}, {\"paperId\": \"e39a61f65f09a2c04643c29d663cc2faa644d6af\", \"abstract\": \"ABSTRACT Big data analytics is playing a pivotal role in big data, artificial intelligence, management, governance, and society with the dramatic development of big data, analytics, artificial intelligence. However, what is the spectrum of big data analytics and how to develop the spectrum are still a fundamental issue in the academic community. This article addresses these issues by presenting a big data derived small data approach. It then uses the proposed approach to analyze the top 150 profiles of Google Scholar, including big data analytics as one research field and proposes a spectrum of big data analytics. The spectrum of big data analytics mainly includes data mining, machine learning, data science and systems, artificial intelligence, distributed computing and systems, and cloud computing, taking into account degree of importance. The proposed approach and findings will generalize to other researchers and practitioners of big data analytics, machine learning, artificial intelligence, and data science.\"}, {\"paperId\": \"ea6fe9591ed85558435b0719d33bd9ff0a1784e1\", \"abstract\": \"As big data, machine learning, and artificial intelligence continue to penetrate into and transform many facets of our lives, we are witnessing the emergence of these powerful technologies within health care. The use and growth of these technologies has been contingent on the availability of reliable and usable data, a particularly robust resource in critical care medicine where continuous monitoring forms a key component of the infrastructure of care. The response to this opportunity has included the development of open databases for research and other purposes; the development of a collaborative form of clinical data science intended to fully leverage these data resources, and the creation of data-driven applications for purposes such as clinical decision support. Most recently, data levels have reached the thresholds required for the development of robust artificial intelligence features for clinical purposes. The systematic capture and analysis of clinical data in both individuals and populations allows us to begin to move toward precision medicine in the intensive care unit (ICU). In this perspective review, we examine the fundamental role of data as we present the current progress that has been made toward an artificial intelligence (AI)-supported, data-driven precision critical care medicine.\"}, {\"paperId\": \"83335941eeb038f39557709c6e92c6065f308051\", \"abstract\": null}, {\"paperId\": \"28dce6ba2c00513b1c5cae16d6bd86d3e9e1f62c\", \"abstract\": \"A digital twin is the virtual replica of a physical system. Digital twins are useful because they provide models and data for design, production, operation, diagnostics, and autonomy of machines and products. Hence, the digital twin has been projected as the key enabler of the Visions of Industry 4.0. The digital twin concept has become increasingly sophisticated and capable over time, enabled by many technologies. In this paper, we propose the cognitive digital twin as the next stage of advancement of a digital twin that will help realize the vision of Industry 4.0. Cognition, which is inspired by advancements in cognitive science, machine learning, and artificial intelligence, will enable a digital twin to achieve some critical elements of cognition, e.g., attention (selective focusing), perception (forming useful representations of data), memory (encoding and retrieval of information and knowledge), etc. Our main thesis is that cognitive digital twins will allow enterprises to creatively, effectively, and efficiently exploit implicit knowledge drawn from the experience of existing manufacturing systems and enable the transfer of higher performance decisions and control and improve the performance across the enterprise (at scale). Finally, we present open questions and challenges to realize these capabilities in a digital twin.\"}, {\"paperId\": \"bcc432e2730404ca0cdae05d4eb3d4577b722908\", \"abstract\": \"Bibliometric analysis was performed to study the development of publications related to Industry 4.0 and its key technologies in Vietnam. Comparisons with data from other ASEAN countries, and with global data have been done to identify distinctive characteristics of Industry 4.0 literature from Vietnam. The collection of 1,470 retrieved papers was analysed to answer seven research questions. Our results highlighted some valuable insights of Industry 4.0 literature in Vietnam. The number of papers in Industry 4.0 in Vietnam increased rapidly in recent years, mostly focused on Computer Science, Engineering, and Mathematics. Iran, China, and South Korea were the most productive partner countries with Vietnam in Industry 4.0. Machine learning, artificial intelligence, big data, deep learning, Internet of things, neural networks, and data mining were among the most popular research themes in Industry 4.0 in Vietnam. Vietnam ranked third among 10 Southeast Asian countries, based on the number of published papers in Industry 4.0, but the gap with the two top countries was large. Compared to the global data, the annual growth rate of Industry 4.0 papers in Vietnam, and other Southeast Asian countries was lower. Findings from this work can be helpful for other scholars in establishing potential future research lines related to Industry 4.0 in Vietnam.\"}, {\"paperId\": \"52b5a3baf789b6347e8e95d43318197e60780842\", \"abstract\": \"Satellite monitoring of development Recent years have witnessed rapid growth in satellite-based approaches to quantifying aspects of land use, especially those monitoring the outcomes of sustainable development programs. Burke et al. reviewed this recent progress with a particular focus on machine-learning approaches and artificial intelligence methods. Drawing on examples mostly from Africa, they conclude that satellite-based methods enhance rather than replace ground-based data collection, and progress depends on a combined approach. Science, this issue p. eabe8628 BACKGROUND Accurate and comprehensive measurements of a range of sustainable development outcomes are fundamental inputs into both research and policy. For instance, good measures are needed to monitor progress toward sustainability goals and evaluate interventions designed to improve development outcomes. Traditional approaches to measurement of many key outcomes rely on household surveys that are conducted infrequently in many parts of the world and are often of low accuracy. The paucity of ground data stands in contrast to the rapidly growing abundance and quality of satellite imagery. Multiple public and private sensors launched in recent years provide temporal, spatial, and spectral information on changes happening on Earth\\u2019s surface. Here we review a rapidly growing scientific literature that seeks to use this satellite imagery to measure and understand various outcomes related to sustainable development. We pay particular attention to recent approaches that use methods from artificial intelligence to extract information from images, as these methods typically outperform earlier approaches and enable new insights. Our focus is on settings and applications where humans themselves, or what they produce, are the outcome of interest and on where these outcomes are being measured using satellite imagery. ADVANCES We describe and synthesize the variety of approaches that have been used to extract information from satellite imagery, with particular attention given to recent machine learning\\u2013based approaches and settings in which training data are limited or noisy. We then quantitatively assess predictive performance of these approaches in the domains of smallholder agriculture, economic livelihoods, population, and informal settlements. We show that satellite-based performance in predicting these outcomes is reasonably strong and improving. Performance improvements have come through a combination of more numerous and accurate training data, more abundant and higher-quality imagery, and creative application of advances in computer vision to satellite inputs and sustainability outcomes. Further, our analyses suggest that reported model performance likely understates true performance in many settings, given the noisy data on which predictions are evaluated and the types of noise typically observed in sustainability applications. For multiple outcomes of interest, satellite-based estimates can now equal or exceed the accuracy of traditional approaches to outcome measurement. We describe multiple methods through which the true performance of satellite-based approaches can be better understood. Integration of satellite-based sustainability measurements into research has been broad, and we describe applications in agriculture, fisheries, health, and economics. Documented uses of these measurements in public-sector decision-making are rarer, which we attribute in part to the novelty of the approaches, their lack of interpretability, and the potential benefits to some policy-makers of not having certain outcomes be measured. OUTLOOK The largest constraint to satellite-based model performance is now training data rather than imagery. While imagery has become abundant, the scarcity and frequent unreliability of ground data make both training and validation of satellite-based models difficult. Expanding the quantity and quality of such data will quickly accelerate progress in this field. Other opportunities for advancement include improvements in model interpretability, fusion of satellites with other nontraditional data that provide complementary information, and more-rigorous evaluation of satellite-based approaches (relative to available alternatives) in the context of specific use cases. Nevertheless, despite the current and future promise of satellite-based approaches, we argue that these approaches will amplify rather than replace existing ground-based data collection efforts in most settings. Many outcomes of interest will likely never be accurately estimated with satellites; for outcomes where satellites do have predictive power, high-quality local training data can nearly always improve model performance. Increasing collection of satellite imagery can help measure livelihood outcomes in areas where ground data are sparse. (Left) Interval between nationally representative economic surveys over the past three decades shows long lags in many developing countries. (Middle) Recently added public and private satellites have broken the traditional trade-off between temporal and spatial resolution. (Right) Performance in measuring the presence of informal settlements, crop yields on smallholder agricultural plots, and village-level asset wealth. R2, coefficient of determination. Accurate and comprehensive measurements of a range of sustainable development outcomes are fundamental inputs into both research and policy. We synthesize the growing literature that uses satellite imagery to understand these outcomes, with a focus on approaches that combine imagery with machine learning. We quantify the paucity of ground data on key human-related outcomes and the growing abundance and improving resolution (spatial, temporal, and spectral) of satellite imagery. We then review recent machine learning approaches to model-building in the context of scarce and noisy training data, highlighting how this noise often leads to incorrect assessment of model performance. We quantify recent model performance across multiple sustainable development domains, discuss research and policy applications, explore constraints to future progress, and highlight research directions for the field.\"}, {\"paperId\": \"437b34bddf4db1d03999be515f294b0c322588ac\", \"abstract\": \"In this survey, results from an investigation on collision avoidance and path planning methods developed in recent research are provided. In particular, existing methods based on Artificial Intelligence, data-driven methods based on Machine Learning, and other Data Science approaches are investigated to provide a comprehensive overview of maritime collision avoidance techniques applicable to Maritime Autonomous Surface Ships. Relevant aspects of those methods and approaches are summarized and put into suitable perspectives. As autonomous systems are expected to operate alongside or in place of conventionally manned vessels, they must comply with the COLREGs for robust decision-support/-making. Thus, the survey specifically covers how COLREGs are addressed by the investigated methods and approaches. A conclusion regarding their utilization in industrial implementations is drawn.\"}, {\"paperId\": \"cb33114e9ab8d72a9df9ca6cdba46da3cbed14e5\", \"abstract\": \"RapidMiner tool is considered among the advanced analytics and powerful platform services in the field of artificial intelligence besides the Big Data storage. This solution has emerged towards several industries such as Financial Services, Energy, Logistics, Life Science and Healthcare and has shown a crucial impact for predictive decisions in this area. This work seeks to describe the solution strategy of this tool for data scientist by depicting a depth insight of this concept which contains more 1500 native algorithms, data preparation and data science functions. This features allows professionals to support any machine learning libraries and integrate python and R codes. RapidMiner offers three different modalities to access to their products which are the main Platform, the automated data science and the AI cloud.\"}, {\"paperId\": \"0f61f9cd3483eef14614a301557439f6008b9074\", \"abstract\": null}, {\"paperId\": \"9cffae0c9009e28251113e47591b4f0c69266534\", \"abstract\": \"Dimension reduction is the vital area in data science & analytics for visualization, and significant pre-processing step for artificial intelligence and machine learning based analysis. For 3D visualization and data analytics of higher dimensional data, it is mandatory to reduce it into lower dimensional subspace. Higher dimensional data existence is everywhere in all type of sectors like Telecom, healthcare infrastructure, Finance, Banking, Transport, eCommerce etc. Applying regression analysis directly on higher dimensional data in machine learning or AI based analytics not recommended. Generally, before analysis, such data is reduced to lower dimensional topological subspace, maintaining the essence of original data. In this paper, a performance comparison of two competitive projection-based non-linear dimension reduction techniques - UMAP and t-SNE with a combination of PCA as a linear based method is analyzed with telecom gateway data. Apart from this, both non-linear techniques are compared based on 3D visualization of handwritten digits images.\"}, {\"paperId\": \"8f7626c6eafa05949c9e200444379ed1f3f0a1b9\", \"abstract\": \"Artificial intelligence (AI) is changing healthcare and the practice of medicine as data-driven science and machine-learning technologies, in particular, are contributing to a variety of medical and clinical tasks. Such advancements have also raised many questions, especially about public trust. As a response to these concerns there has been a concentrated effort from public bodies, policy-makers and technology companies leading the way in AI to address what is identified as a \\\"public trust deficit\\\". This paper argues that a focus on trust as the basis upon which a relationship between this new technology and the public is built is, at best, ineffective, at worst, inappropriate or even dangerous, as it diverts attention from what is actually needed to actively warrant trust. Instead of agonising about how to facilitate trust, a type of relationship which can leave those trusting vulnerable and exposed, we argue that efforts should be focused on the difficult and dynamic process of ensuring reliance underwritten by strong legal and regulatory frameworks. From there, trust could emerge but not merely as a means to an end. Instead, as something to work in practice towards; that is, the deserved result of an ongoing ethical relationship where there is the appropriate, enforceable and reliable regulatory infrastructure in place for problems, challenges and power asymmetries to be continuously accounted for and appropriately redressed.\"}, {\"paperId\": \"9517aa213a3c3b912867477541adb94daa76eaa0\", \"abstract\": null}, {\"paperId\": \"77698d4bdb2ccbb73a07d0402c1dba3d7630826e\", \"abstract\": \"Artificial intelligence (AI) is part computer science and part cognitive science, encompassing the phenomena of computers performing tasks that require human intelligence.1 Current interest in AI is motivated, in part, by recent developments in machine learning, in which algorithms learn from data without human direction.2 Machine learning is a group of different mathematical methods to build AI rules. Each of these methods has its strengths and weaknesses based on the types of data being analyzed. Some machine learning approaches, such as speech recognition when talking into the speaker of a smart phone or smart home device, are now mainstream.\"}, {\"paperId\": \"eaf53a43cc4c43f2ae8e02287922c19da4e3b5fd\", \"abstract\": null}, {\"paperId\": \"f9661e599537cf561f1c78c12397b552b5771e15\", \"abstract\": \"Computer scientists and researchers have focused their efforts on advancing from traditional computing techniques to the use of \\u201cIndustry 4.0\\u201d. Specifically, this term includes breakthroughs in numerous fields, including manufacturing, Artificial Intelligence, machine learning and data science. Additionally, emphasis is placed on the so-called \\u201cInternet of Things\\u201d, i.e. the development of interconnected devices capable of communicating and processing information, mainly from sensory networks. This article presents electronic devices used for home automation. Specifically, it presents different types of residential sensors which can transform a traditional house to a \\u201csmart home\\u201d. Furthermore, it reviews their possible uses, such as for ventilation, security and temperature monitoring. Lastly, for each of the sensors, the article recommends low-cost sensory devices to set up affordable home automation projects, such as an Arduino and a Raspberry Pi.\"}, {\"paperId\": \"fd68308638d8e334fb61e8271b918edd531b5b9d\", \"abstract\": \"Industry 4.0 can be considered the 21st century's industrial revolution and will soon be the new form of manufacturing delight. The definitive customer would experience manufacturing requests determined by artificial intelligence, machine learning, and automated technologies linked with data science support for gauging customer necessities. Phenomenally, Industry 4.0 is rapidly changing the firm's management and organizational systems, and competencies, as well as making its environment much more explored, even if more complexed than in the past. This new industrial revolution would possess systems with transformative technologies for managing interconnected systems between its physical assets and computational capabilities. Such enterprises would require skilled workforce to improve and operate advanced manufacturing tools and systems, and investigate the machine data, clients, and global capitals, resulting in an escalating need for trained employees proficient in cross-functional capacities and with competencies to cope new processes and IT systems.\"}, {\"paperId\": \"c1f0aed39e8cba4e74e9fdb0a25f7974ece64ae7\", \"abstract\": \"Artificial intelligence enabled medical big data analysis has the potential to revolutionize medical practice from diagnosis and prediction of complex diseases to making recommendations and resource allocation decisions in an evidence-based manner. However, big data comes with big disclosure risks. To preserve privacy, excessive data anonymization is often necessary, leading to significant loss of data utility. In this paper, we develop a systematic data scrubbing procedure for large datasets when key variables are uncertain for re-identification risk assessment and assess the trade-off between anonymization of electronic health record data for sharing in support of open science and performance of machine learning models for early acute kidney injury risk prediction using the data. Results demonstrate that our proposed data scrubbing procedure can maintain good feature diversity and moderate data utility but raises concerns regarding its impact on knowledge discovery capability.\"}, {\"paperId\": \"5e21d7ce531a81817d5f4de8ff5de3e444cf6629\", \"abstract\": \"Interdisciplinary research is often at the core of scientific progress. As an example, artificial neural networks, currently an essential tool in many applications that require learning from data, were originally inspired by insights from biological neural networks. Since its inception as a field, the progress of artificial intelligence has at times converged and at times diverged from the field of neuroscience. While occasional divergence can be fruitful, in this dissertation we will explore some advantageous synergies between machine learning, cognitive science and neuroscience. In particular, this thesis focuses on vision and images. The human visual system has been widely studied from both behavioural and neuroscientific points of view, as vision is the dominant sense of most people. In turn, machine vision has also been an active area of research, currently dominated by the use of artificial neural networks. Despite their origin and some similarities with biological networks, the recent progress in neural networks for image understanding has shown signs of divergence from neuroscience. One likely cause is the focus on benchmark performance, regardless of what the models learn. This work focuses instead on learning representations that are more aligned with visual perception and the biological vision. For that purpose, I have studied tools and aspects from cognitive science and computational neuroscience, and attempted to incorporate them into machine learning models of vision. A central subject of this dissertation is data augmentation, a commonly used technique for training artificial neural networks to augment the size of data sets through transformations of the images. Although often overlooked, data augmentation implements transformations that are perceptually plausible, since they correspond to the transformations we see in our visual world\\u2014 changes in viewpoint or illumination, for instance. Furthermore, neuroscientists have found that the brain invariantly represents objects under these transformations. Throughout this dissertation, I use these insights to analyse data augmentation as a particularly useful inductive bias, a more effective regularisation method for artificial neural networks, and as the framework to analyse and improve the invariance of vision models to perceptually plausible transformations. Overall, this work aims to shed more light on the properties of data augmentation and demonstrate the potential of interdisciplinary research. The code produced in this thesis is open source and available at www.github.com/alexhernandezgarcia\"}, {\"paperId\": \"9540b924ec52eeee94d447854ce12d5ee2349c77\", \"abstract\": \"COVID-19 pandemic has a catastrophic consequence globally since its first case was detected in December 2019, with an aggressive spread. Currently an exponential growth is expected. If not diagnosed at the proper time, COVID-19 may lead to death of the infected individuals. Thus, continuous screening, early diagnosis and prompt actions are crucial to control the spread and reduce the mortality. In this paper we focus on developing a Medical Diagnosis Humanoid (MDH) which is a cost effective, safety critical mobile robotic system that provides a complete diagnostic test to check whether an individual is infected by Covid-19 or not. This paper highlights the development of a system based on Artificial Intelligence for Medical Science, where humanoids can navigate through desired destinations, diagnose an individual for Covid-19 through various parameters and make a survey of a locality for the same. The humanoid uses the concept of real time data sensing and processing through machine learning produced by various sensors used in the context.\"}, {\"paperId\": \"c2177be611a143385c4f608e3381badae44c2644\", \"abstract\": \"In the recent years, data science methods have been developed considerably and have consequently found their way into many business processes in banking and finance. One example is the review and approval process of credit applications where they are employed with the aim to reduce rare but costly credit defaults in portfolios of loans. But there are challenges. Since defaults are rare events, it is\\u2014even with machine learning (ML) techniques\\u2014difficult to improve prediction accuracy and improvements are often marginal. Furthermore, while from an event prediction point of view, a non-default is the same as a default, from an economic point of view much more relevant to the end user it is not due to the high asymmetry in cost. Last, there are regulatory constraints when it comes to the adoption of advanced ML, hence the call for explainable artificial intelligence (XAI) issued by regulatory bodies like FINMA and BaFin. In our study, we will address these challenges. In particular, based on an exemplary use case, we show how ML methods can be adapted to the specific needs of credit assessment and how, in the case of strongly asymmetric costs of wrong forecasts, it makes sense to optimize not for accuracy but for an economic target function. We showcase this for two simple and ad hoc explainable ML algorithms, finding that in the case of credit approval, surprisingly high rejection rates contribute to maximizing profit.\"}, {\"paperId\": \"52d144a554ecc5249c1f1c0550ef48171da1afce\", \"abstract\": \"Abstract In recent years, the legal informatics area has slowly begun to develop as artificial intelligence and its related techniques and technologies expand their reach in the field of law. The legal, computational and data science communities are collaborating to build computational and data-driven innovative legal models to improve and advance all aspects of the existing legal system with the effective use of modern computer technologies such as machine learning, deep learning and natural language processing. In this research paper, the authors - Sugam Sharma, Samia Gamoura, Deva Prasad and Arti Aneja - explore these important factors with the potential to transform the existing approach to jurisprudence into a smart and intelligent legal system utilising automation. Such a justice system could be envisioned, in the near future which would be faster, fairer and economically more feasible, even for the highly marginalized, underprivileged and poor societies in the world.\"}, {\"paperId\": \"8be7f1ad98e54b69ac0c0937f51ff07cf5d48df3\", \"abstract\": \"Artificial Intelligence or AI is the theory and development of computer systems that can think and act humanly and rationally [1]. In recent years, more and more practitioners and researchers start to refer AI to \\u201caugmented intelligence\\u201d [2]. Augmented Intelligence is a new perspective to look at the artificial intelligence, social computing, machine learning, big data, data mining, and related areas. It has a clear emphasis that humanity, not machines, is the core of this scientific inquiry. And the ultimate goal of AI is to augment human, not to replace human. Augmented Intelligence describes a system consisting of both artificial intelligence agents and human agents, therefore opening up new research opportunities not only for researchers in technical areas such as artificial intelligence, machine learning, big data, and data mining, but also for researchers in behavioral science, social science, organization science, human-computer interaction and many other areas. This minitrack proposes that Augmented Intelligence can be studied from the following three perspectives.\"}, {\"paperId\": \"3406252e6c59e2003c32fcd959ee5b6c830ac192\", \"abstract\": \"Wild and uncontrolled spread of the corona pandemic over several months throughout the world is a global issue. To combat this global issue advancement in information technology is also used along with medicine, biotechnology, and medical Instrumentation. Machine learning, artificial intelligence, and data science are providing a great contribution to the battle with COVID-19. By using such technologies there are high prospects to overcome with this pandemic and to lead a standard routine as it was before the pandemic. This paper analyses different technologies used in different conditions such as social distancing and prevention, quarantine and isolation, COVID detection and testing, patient treatment and care, and hospital management. This paper introduces transparent planning, technological methods, and digital procedures, with the latest smart tools in various sectors that help to defeat the intensity of coronavirus.\"}, {\"paperId\": \"0a78b7f794b98a90bc92a6e7cd88f11923287b6c\", \"abstract\": \"Machine learning can be a technique of nursing lysis that automatically develops an analytical model. It is a branch of synthetic intelligence that believes that systems are going to learn information, determine patterns of information and decide with degraded human intervention. Machine learning addresses the question of how computers can be constructed that improve mechanically through knowledge. It lies at the intersection of technology and statistics and at the center of artificial data and information science, one in all the quickest increasing technical fields of nowadays. Recent advances in machine learning were driven by the event of latest learning and theories also as by the constant explosion. The event of latest learning algorithms and also theory and the in-progress growth within the accessibility of on-line information also as low-priced computation crystal rectifier to recent progress within the field of machine learning. Additional evidence-based decisionmaking could be carried out in science, technology and trade, including healthcare, production, education and monetary modelling, enforcement and promotion, with adoption of mechanical learning techniques based on data-intensive methods. The results are also available. The infection can be a life-threatening disease. The bite of a nursing partner is often transmitted in dipterous Anopheles. In infected mosquitoes, plasmodium parasite is a gift. The parasite is discharged into your blood after you bite this dipterous insect once it bites you. Once your body is composed of the parasites, they mature into the liver. The mature parasites enter the blood for several days when red blood cells start to infect. In red blood cells, parasites increase over 48-72 hours, causing infected cells to divide. The parasites still infect red blood cells, which last 2 to 3 days in cycles. This paper is used for observation of protozoan infection with a deep learning idea.\"}, {\"paperId\": \"22e29bf7fa925e2c32f92ef728d34631a0fe91b2\", \"abstract\": \"Artificial intelligence (AI) has dramatically changed the landscape of science, industry, defence, and medicine in the last several years. Supported by considerably enhanced computational power and cloud storage, the field of AI has shifted from mostly theoretical studies in the discipline of computer science to diverse real-life applications such as drug design, material discovery, speech recognition, self-driving cars, advertising, finance, medical imaging, and astronomical observation, where AI-produced outcomes have been proven to be comparable or even superior to the performance of human experts. In these applications, what is essentially important for the development of AI is the data needed for machine learning. Despite its prominent importance, the very first process of the AI development, namely data collection and data preparation, is typically the most laborious task and is often a limiting factor of constructing functional AI algorithms. Lab-on-a-chip technology, in particular microfluidics, is a powerful platform for both the construction and implementation of AI in a large-scale, cost-effective, high-throughput, automated, and multiplexed manner, thereby overcoming the above bottleneck. On this platform, high-throughput imaging is a critical tool as it can generate high-content information (e.g., size, shape, structure, composition, interaction) of objects on a large scale. High-throughput imaging can also be paired with sorting and DNA/RNA sequencing to conduct a massive survey of phenotype-genotype relations whose data is too complex to analyze with traditional computational tools, but is analyzable with the power of AI. In addition to its function as a data provider, lab-on-a-chip technology can also be employed to implement the developed AI for accurate identification, characterization, classification, and prediction of objects in mixed, heterogeneous, or unknown samples. In this review article, motivated by the excellent synergy between AI and lab-on-a-chip technology, we outline fundamental elements, recent advances, future challenges, and emerging opportunities of AI with lab-on-a-chip technology or \\\"AI on a chip\\\" for short.\"}, {\"paperId\": \"69c5c97bc8e55099d050c9134858d033c356dda1\", \"abstract\": \"Covering both quantitative and qualitative methods, this book examines the breadth of modern market research methods for upper level students across business schools and social science faculties. Modern and trending topics including social networks, machine learning, big data, and artificial intelligence are addressed and real world examples and case studies illustrate the application of the methods. This text examines potential problems, such as researcher bias, and discusses effective solutions in the preparation of research reports and papers, and oral presentations. Assuming no prior knowledge of statistics or econometrics, discrete chapters offer a clear introduction to both, opening up the quantitative methods to all students. Each chapter contains rigorous academic theory, including a synthesis of the recent literature as well as key historical references, applied contextualization and recent research results, making it an excellent resource for practitioners. Online resources include extensive chapter bibliographies, lecture slides, an instructor guide and extra extension material and questions.\"}, {\"paperId\": \"d59bab71cdcb5f11e4108491bd09c7091f278a3e\", \"abstract\": \"Data mining is an interdisciplinary subfield of computer science involving methods at the intersection of artificial intelligence, machine learning and statistics. One of the data mining tasks is anomaly detection which is the analysis of large quantities of data to identify items, events or observations which do not conform to an expected pattern. Anomaly detection is applicable in a variety of domains, e.g., fraud detection, fault detection, system health monitoring but this article focuses on application of anomaly detection in the field of network intrusion detection.The main goal of the article is to prove that an entropy-based approach is suitable to detect modern botnet-like malware based on anomalous patterns in network. This aim is achieved by realization of the following points: (i) preparation of a concept of original entropy-based network anomaly detection method, (ii) implementation of the method, (iii) preparation of original dataset, (iv) evaluation of the method.\"}, {\"paperId\": \"78adbe77d041e5bb882fbbc887e71af63e788684\", \"abstract\": \"\\nArtificial intelligence (AI) and machine learning (ML) have become important tools for environmental scientists and engineers, both in research and in applications. Although these methods have become quite popular in recent years, they are not new. The use of AI methods began in the 1950s and environmental scientists were adopting them by the 1980s. Although an \\u201cAI Winter\\u201d temporarily slowed the growth, a more recent resurgence has brought it back with gusto. This paper tells the story of the evolution of AI in the field through the lens of the AMS Committee on Artificial Intelligence Applications to Environmental Science.\\nThe environmental sciences possess a host of problems amenable to advancement by intelligent techniques. We review a few of the early applications along with the ML methods of the time and how their progression has impacted these sciences. While AI methods have changed from expert systems in the eighties to neural networks and other data-driven methods, and more recently deep learning, the environmental problems tackled have remained similar. We discuss the types of applications that have shown some of the biggest advances due to AI usage and how they have evolved over the past decades, including topics in weather forecasting, probabilistic prediction, climate estimation, optimization problems, image processing, and improving forecasting models. We finish with a look at where AI as employed in environmental science appears to be headed and some thoughts on how it might be best blended with physical / dynamical modeling approaches to further advance our science.\"}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 500, \"next\": 600, \"data\": [{\"paperId\": \"78adbe77d041e5bb882fbbc887e71af63e788684\", \"abstract\": \"\\nArtificial intelligence (AI) and machine learning (ML) have become important tools for environmental scientists and engineers, both in research and in applications. Although these methods have become quite popular in recent years, they are not new. The use of AI methods began in the 1950s and environmental scientists were adopting them by the 1980s. Although an \\u201cAI Winter\\u201d temporarily slowed the growth, a more recent resurgence has brought it back with gusto. This paper tells the story of the evolution of AI in the field through the lens of the AMS Committee on Artificial Intelligence Applications to Environmental Science.\\nThe environmental sciences possess a host of problems amenable to advancement by intelligent techniques. We review a few of the early applications along with the ML methods of the time and how their progression has impacted these sciences. While AI methods have changed from expert systems in the eighties to neural networks and other data-driven methods, and more recently deep learning, the environmental problems tackled have remained similar. We discuss the types of applications that have shown some of the biggest advances due to AI usage and how they have evolved over the past decades, including topics in weather forecasting, probabilistic prediction, climate estimation, optimization problems, image processing, and improving forecasting models. We finish with a look at where AI as employed in environmental science appears to be headed and some thoughts on how it might be best blended with physical / dynamical modeling approaches to further advance our science.\"}, {\"paperId\": \"52ff647be667749c4b7dec07486fa328b54fa595\", \"abstract\": \"Machine learning and artificial intelligence algorithms can assist human decision making and analysis tasks. While such technology shows promise, willingness to use and rely on intelligent systems may depend on whether people can trust and understand them. To address this issue, researchers have explored the use of explainable interfaces that attempt to help explain why or how a system produced the output for a given input. However, the effects of meaningful and meaningless explanations (determined by their alignment with human logic) are not properly understood, especially with users who are non-experts in data science. Additionally, we wanted to explore how explanation inclusion and level of meaningfulness would affect the user\\u2019s perception of accuracy. We designed a controlled experiment using an image classification scenario with local explanations to evaluate and better understand these issues. Our results show that whether explanations are human-meaningful can significantly affect perception of a system\\u2019s accuracy independent of the actual accuracy observed from system usage. Participants significantly underestimated the system\\u2019s accuracy when it provided weak, less human-meaningful explanations. Therefore, for intelligent systems with explainable interfaces, this research demonstrates that users are less likely to accurately judge the accuracy of algorithms that do not operate based on human-understandable rationale.\"}, {\"paperId\": \"2f438d12458188c42a305085da12515ca4024761\", \"abstract\": \"Natural language processing is the communication between the humans and the computers. It is the field of computer science which incorporates artificial intelligence and linguistics where machine learning algorithms are used to analyze and process the enormous variety of data. This chapter delivers the fundamental concepts of language processing in Python such as text and word operations. It also gives the details about the preference of Python language for language processing and its advantages. It specifies the basic concept of variables, list, operators, looping statements in Python and explains how it can be implemented in language processing. It also specifies how a structured program can be written using Python, categorizing and tagging of words, how an information can be extracted from a text, syntactic and semantic analysis, and NLP applications. It also concentrates some of the research applications where NLP is applied and the challenges of NLP processing in the real-time area of applications.\"}, {\"paperId\": \"4f3f40fc6484c3bf87867f7d954ec79c6f52f7c4\", \"abstract\": \"Classification in data mining is a technique based on machine learning algorithms which uses mathematics, statistics, probability distributions and artificial intelligence... To predict group membership for data items or to represent descriptive analysis of data items for effective decision making .Now a day\\u2019s data mining is touching every aspect of individual life includes Data Mining for Financial Data Analysis, Data Mining for the Telecommunications Industry Data Analysis, Data Mining for the Retail Industry Data Analysis, Data Mining in Healthcare and Biomedical Research Data Analysis, and Data Mining in Science and Engineering Data Analysis, etc. The goal of this survey is to provide a comprehensive review of different classification techniques in data mining based on decision tree, rule based Algorithms, neural networks, support vector machines, Bayesian networks, and Genetic Algorithms and Fuzzy logic.\"}, {\"paperId\": \"3319318a1b2c21a3844665173ca541b9961bf2e9\", \"abstract\": \"Cardiovascular diseases cause an estimated 17.9 million deaths each year. Early diagnosis and treatments have an important place for heart diseases, which make up the majority of the total number of deaths. Studies carried out in the field of medicine for a long time have been made more successful by being supported by new techniques such as machine learning and artificial intelligence, thanks to the rapid rise of computer science in the last quarter century. In this study, machine learning techniques were applied on the sample data set to detect heart disease and the results were compared. First, the data set was analyzed. It has been noted which data may be indicative of heart disease. Then, a sample model was created using three different machine learning methods and individuals with heart disease were identified. When the obtained results are compared, it has been observed that the Random Forest algorithm is more successful with an accuracy rate of 88%. This was followed by Logistic Regression with 85% accuracy and kNN algorithm with 70% accuracy, respectively. The findings how that heart disease can be easily detected with a few basic data.\"}, {\"paperId\": \"3d4fba647d593b9e7bbd50282747e2be97018ebb\", \"abstract\": \"The use of Artificial Intelligence (AI) in solving real- time problems are increasing day by day with the increase in the availability of data and computation power. It is now substantial to use AI-based tools and techniques in space science. Asteroids, rocky objects that orbit around the sun, often produce an array of effects that cause harm to humans and biodiversity on earth. Such effects can cause wind blast, overpressure shock, thermal radiation, cratering, seismic shaking, ejecta deposition, tsunami, and many more. With the availability of data on asteroid parameters and nature, it provides an opportunity to use Machine Learning (ML) to address this problem and reduce the risk. This paper presents a thorough study on the impact of Potentially Hazardous Asteroids (PHAs) and proposes a supervised machine learning method to detect whether an asteroid with specific parameters is hazardous or not. We compare manifold classification algorithms that were implemented on the data. Random forest gave the best performance in terms of accuracy (99.99%) and average F1- score (99.22%).\"}, {\"paperId\": \"b5c969888518593e7dd7129eb00c44df232aeef7\", \"abstract\": \"\\nPopulation data science [1] researchers are not alone in recognizing the value of health and health-related data. In the era of big data, and with advent of machine learning and other artificial intelligence methods, organizations around the world are actively working to turn data into knowledge, and, in some cases, profit. The media and members of the public have taken notice, with high profile news stories about data breaches and privacy concerns [2-4] alongside some stories that call for increased use of data [5,6]. In response, public and private sector data-holding organizations and jurisdictions are turning their attention to policies, processes and regulations intended to ensure that personal data are used in ways that that the public supports. In some cases, these efforts include involving \\u201cpublics\\u201d in decisions about data, such as using patient and lay person advice and other inputs to help shape policies [7-10]. \\n\"}, {\"paperId\": \"d8e675f446e279e689fa91dcb6ea80defb65ebb5\", \"abstract\": \"In this paper we describe eBird, a citizen-science project that takes advantage of the human observational capacity to identify birds to species, which is then used to accurately represent patterns of bird occurrences across broad spatial and temporal extents. eBird employs artificial intelligence techniques such as machine learning to improve data quality by taking advantage of the synergies between human computation and mechanical computation. We call this a Human-Computer Learning Network, whose core is an active learning feedback loop between humans and machines that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. In this paper we explore how Human-Computer Learning Networks can leverage the contributions of a broad recruitment of human observers and processes their contributed data with Artificial Intelligence algorithms leading to a computational power that far exceeds the sum of the individual parts.\"}, {\"paperId\": \"719e53bfc30db078357a77166feac6918612f4e4\", \"abstract\": \"Emerging technologies such as cloud computing, integration of Internet of Things, data science, self\\u2010powered data centers, dense sensor network, artificial intelligence convergence, machine learning and deep learning, self\\u2010service IT for business users and others play an important role in daily life. Dense sensor networks (DSNs) can be useful in fields such as structured health monitoring and turbine blades monitoring. Given the high number of sensors and the required small size, these sensors usually have very low processing capabilities for fitting to the size restrictions and limiting the production costs of the whole DSN. In this context, algorithms need to be really efficient so the algorithms can be achieved. This article focuses on providing an efficient algorithm for solving integral equations that can be useful in common problems in for emerging telecommunications. More concretely, this article presents an efficient numerical scheme for solution of nonlinear delay Fredholm integral equations, nonlinear delay Volterra integral equations and nonlinear delay Fredholm Volterra integral equations which are based on the use of Haar wavelets. Maximum absolute errors and experimental rates of convergence are computed using different numbers of collocation points.Numerical examples are given to show the computational efficiency of the proposed method. The numerical results exhibit that the technique is efficient and effective. The proposed research work can be used in wireless sensor network, emerging technologies, hereditary's phenomena in physics and heat transfer problems, electron emission and X\\u2010rays radiography.\"}, {\"paperId\": \"b1d39e7c33623d7cb78fb101358961af55b95404\", \"abstract\": \"From machine learning (ML) and computer vision to robotics and natural language processing, the application of data science and artificial intelligence (AI) is expected to transform health care (Ce...\"}, {\"paperId\": \"0d2962a2993e49d0632fb6191eda526b6d21d6ac\", \"abstract\": \"The smart technologies led by advances in artificial intelligence, machine learning, and the emerging data science in recent years are transforming many facets of society in profound ways. One of these affected areas is the experience of human dynamics in general and human mobility in particular with the growing maturity of smart technologies. The goal of this article is to critically examine the concepts of space and place in geography in general and in geographic information science (GIScience) in particular so that intelligent geographic information systems incorporating concepts of smart space and smart place can be developed to support human dynamics research. We argue that the current discussions on smart technologies are conceptually constrained due to their confinement to absolute space and physical place. By engaging research on smart technologies with geography and GIScience, we seek to move beyond the crude, and often simplistic, conceptualizations of space and place by synthesizing the multiple dimensions of both space and place. By doing so, we can better understand human dynamics through a synergistic perspective of both space and place. The space\\u2013place (splatial) framework proposed in this article will enable us to creatively study the human dynamics in the age of smart technologies. Our approach will not only allow us to better understand human dynamics but also advance and enrich our theoretical and methodological frameworks for studying smart technologies and the profound social impacts from a geographic perspective. Challenges for the implementation of the proposed framework are discussed and directions for future research are highlighted. Key Words: GIScience, human dynamics, place, space, splatial framework.\"}, {\"paperId\": \"a9bf43daae1933317d285ce6027334db25ba86ca\", \"abstract\": \"Creating human-level intelligent system is the long-standing mission for the field of Artificial Intelligence (AI) since its establishment nearly 60 years ago. Until now, however, there is still no general purpose intelligent system which can reach the human intelligence level in terms of coordinating various cognitive behaviors, adaptability of complex environments, and autonomous learning under new environments. With the advancement of Brain Science, Neuroscience, and Cognitive Science, it is now possible for partially observing and obtaining data on the activities of brain neural networks at multiple scales while they are conducting various cognitive tasks. Hence, Brain-inspired Intelligence (Brain-inspired AI) is becoming a focus and promising trend of AI. Brain-inspired Intelligence is a field for creating machine intelligence through computational modeling\"}, {\"paperId\": \"c9ffd73a2abd0ecba62365f32c8cde40620693cb\", \"abstract\": \"This paper describes the first step of a research project with the aim of predicting students' performance during an online curriculum on a LMS and keeping them from falling behind. Our research project aims to use data mining, machine learning and artificial intelligence methods for monitoring students in e-learning trainings. This project takes the shape of a partnership between computer science / artificial intelligence researchers and an IT firm specialized in e-learning software. We wish to create a system that will gather and process all data related to a particular e-learning course. To make monitoring easier, we will provide reliable statistics, behaviour groups and predicted results as a basis for an intelligent virtual tutor using the mentioned methods. This system will be described in this article. In this step of the project, we are clustering students by mining Moodle log data. A first objective is to define relevant clustering features. We will describe and evaluate our proposal. A second objective is to determine if our students show different learning behaviours. We will experiment whether there is an overall ideal number of clusters and whether the clusters show mostly qualitative or quantitative differences. Experiments in clustering were carried out using real data obtained from various courses dispensed by a partner institute using a Moodle platform. We have compared several classic clustering algorithms on several group of students using our defined features and analysed the meaning of the clusters they produced.\"}, {\"paperId\": \"021b3224179788869b0f2bcebf94b0850f6ad65e\", \"abstract\": \"As data science and artificial intelligence become ubiquitous, they have an increasing impact on society. While many of these impacts are beneficial, others may not be. So understanding and managing these impacts is required of every responsible data scientist. Nevertheless, most human decision-makers use algorithms for efficiency purposes and not to make a better (i.e., fairer) decisions. Even the task of risk assessment in the criminal justice system enables efficiency instead of (and often at the expense of) fairness. So we need to frame the problem with fairness, and other societal impacts, as primary objectives. In this context, most attention has been paid to the machine learning of a model for a task, such as recognition, prediction, or classification. However, issues arise in all parts of the data eco-system, from data acquisition to data presentation. For example, the majority of the population is not white and male, yet this demographic is over-represented in the training data. It is challenging for a data scientist to satisfactorily discharge this broad responsibility.\"}, {\"paperId\": \"4ec9cb1b79a388b7d47eeb751b4aaa92f9d3b02e\", \"abstract\": \"This paper explores novel research directions arising from the revolutions in artificial intelligence and the related fields of machine learning, data science, etc. We identify opportunities for system design to leverage the advances in these disciplines, as well as to identify and study new problems. Specifically, we propose Data-driven and Model-based Design (DMD) as a new system design paradigm, which combines model-based design with classic and novel techniques to learn models from data.\"}, {\"paperId\": \"3575b0b4d4ad3ec251c9775d46cc66c7cc93d37f\", \"abstract\": \"Given the focus on deep learning and machine learning, there is a need to address this problem of low participation of Africans in data science and artificial intelligence. The Deep Learning Indaba was thus born to stimulate the participation of Africans within the research and innovation landscape surrounding deep learning and machine learning. This column reports on the Deep Learning Indaba event, which consisted of a 5-day series of introductory lectures on Deep Learning, held from 10-15 September 2017, coupled with tutorial sessions where participants gained practical experience with deep learning software packages. The column also includes interviews with some of the organisers to learn more about the origin and future plans of the Deep Learning Indaba.\"}, {\"paperId\": \"e3f047eac765aa450120fe3fbdb578c9bf4b7674\", \"abstract\": \"This publication revises the deteriorated performance of field calibrated low-cost sensor systems after spatial and temporal relocation, which is often reported for air quality monitoring devices that use machine learning models as part of their software to compensate for cross-sensitivities or interferences with environmental parameters. The cause of this relocation problem and its relationship to the chosen algorithm is elucidated using published experimental data in combination with techniques from data science. Thus, the origin is traced back to insufficient sampling of data that is used for calibration followed by the incorporation of bias into models. Biases often stem from non-representative data and are a common problem in machine learning, and more generally in artificial intelligence, and as such a rising concern. Finally, bias is believed to be partly reducible in this specific application by using balanced data sets generated in well-controlled laboratory experiments, although not trivial due to the need for infrastructure and professional competence.\"}, {\"paperId\": \"f6965f362366c2671e3af0e264e027c2906c290c\", \"abstract\": \"Data mining (the analysis step of the \\\"Knowledge Discovery in Databases\\\" process, or KDD) an interdisciplinary subfield of computer science, is the computational process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. A warehouse is a commercial building for storage of goods. It\"}, {\"paperId\": \"3076aa03e4f5516e8fd50cc3e2b849d83f9ccc45\", \"abstract\": \"We analyze the performance of the greedy algorithm, and also a discrete semi-gradient based algorithm, for maximizing the sum of a suBmodular and suPermodular (BP) function (both of which are non-negative monotone non-decreasing) under two types of constraints, either a cardinality constraint or $p\\\\geq 1$ matroid independence constraints. These problems occur naturally in several real-world applications in data science, machine learning, and artificial intelligence. The problems are ordinarily inapproximable to any factor (as we show). Using the curvature $\\\\kappa_f$ of the submodular term, and introducing $\\\\kappa^g$ for the supermodular term (a natural dual curvature for supermodular functions), however, both of which are computable in linear time, we show that BP maximization can be efficiently approximated by both the greedy and the semi-gradient based algorithm. The algorithms yield multiplicative guarantees of $\\\\frac{1}{\\\\kappa_f}\\\\left[1-e^{-(1-\\\\kappa^g)\\\\kappa_f}\\\\right]$ and $\\\\frac{1-\\\\kappa^g}{(1-\\\\kappa^g)\\\\kappa_f + p}$ for the two types of constraints respectively. For pure monotone supermodular constrained maximization, these yield $1-\\\\kappa^g$ and $(1-\\\\kappa^g)/p$ for the two types of constraints respectively. We also analyze the hardness of BP maximization and show that our guarantees match hardness by a constant factor and by $O(\\\\ln(p))$ respectively. Computational experiments are also provided supporting our analysis.\"}, {\"paperId\": \"34dd7e659a238aa5a8c6cea0bb480cbc7ff0bc92\", \"abstract\": \"We present a new strategy using artificial intelligence (AI) to build the first AI-based Monte Carlo event generator (MCEG) capable of faithfully generating final state particle phase space in lepton-hadron scattering. We show a blueprint for integrating machine learning strategies with calibrated detector simulations to build a vertex-level, AI-based MCEG, free of theoretical assumptions about femtometer scale physics. As the first steps towards this goal, we present a case study for inclusive electron-proton scattering using synthetic data from the PYTHIA MCEG for testing and validation purposes. Our quantitative results validate our proof of concept and demonstrate the predictive power of the trained models. The work suggests new venues for data preservation to enable future QCD studies of hadrons structure, and the developed technology can boost the science output of physics programs at facilities such as Jefferson Lab and the future Electron-Ion Collider.\"}, {\"paperId\": \"5aeae1890a187386516f6be574c31e99845a5216\", \"abstract\": \"This paper reviews the impact of data science and artificial intelligence (AI) on future \\u2018data-driven\\u2019 Insurance Markets. The impact of insurance automation (driven by so-called Black Swan events such as Covid-19) mirrors the impact of algorithmic trading that changed radically the Capital Markets (Koshiyama, et al., 2020). The data science technologies driving change include: Big data, AI analytics, Internet of Things, and Blockchain technologies. These technologies are important since they underpin the automation of the Insurance Markets and risk analysis, and provide the context for the algorithms, such as AI machine learning and computational statistics, which provide powerful analytics capabilities. \\n \\nNew AI algorithms are constantly emerging, with each \\u2018strain\\u2019 mimicking a new form of human learning, reasoning, knowledge, and decision-making. The current main disrupting forms of learning include Deep Learning, Adversarial Learning, Federated Learning, Transfer and Meta Learning. Albeit these modes of learning have been in the AI/ML field more than a decade, they are now more applicable due to the availability of data, computing power and infrastructure. These forms of learning have produced new models (e.g., Long Short-Term Memory, Generative Adversarial Networks) and leverage important applications (e.g., Natural Language Processing, Adversarial Examples, Deep Fakes, etc.). These new models and applications will drive changes in future Insurance Markets, so it is important to understand their computational strengths and weaknesses. \\n \\nThe contribution of this paper is to review the data science technologies and specifically AI algorithms, their computational strengths and weaknesses, and discuss their future impact on the Insurance Markets.\"}, {\"paperId\": \"788b4483ce6dc440e1f73752d4a7edff6410be3f\", \"abstract\": \"The authors predict asset returns and measure risk premiums using a prominent technique from artificial intelligence: deep sequence modeling. Because asset returns often exhibit sequential dependence that may not be effectively captured by conventional time-series models, sequence modeling offers a promising path with its data-driven approach and superior performance. In this article, the authors first overview the development of deep sequence models, introduce their applications in asset pricing, and discuss their advantages and limitations. They then perform a comparative analysis of these methods using data on US equities. They demonstrate how sequence modeling benefits investors in general through incorporating complex historical path dependence and that long short-term memory\\u2013based models tend to have the best out-of-sample performance. TOPICS: Big data/machine learning, security analysis and valuation, performance measurement Key Findings \\u25aa This article provides a concise synopsis of deep sequence modeling with an emphasis on its historical development in the field of computer science and artificial intelligence. It serves as a reference source for social scientists who aim to use the tool to supplement conventional time-series and panel methods. \\u25aa Deep sequence models can be adapted successfully for asset pricing, especially in predicting asset returns, which allow the model to be flexible to capture the high-dimensionality, nonlinear, interactive, low signal-to-noise, and dynamic nature of financial data. In particular, the model\\u2019s ability to detect path-dependence patterns makes it versatile and effective, potentially outperforming existing models. \\u25aa This article provides a horse-race comparison of various deep sequence models for the tasks of forecasting returns and measuring risk premiums. Long short-term memory has the best performance in terms of out-of-sample predictive R2, and long short-term memory with an attention mechanism has the best portfolio performance when excluding microcap stocks.\"}, {\"paperId\": \"1d1bcfb8adc877dfd3aaced7c63d21881e6fb184\", \"abstract\": null}, {\"paperId\": \"2e8c2b663eb2bf1b2d92a2bc016a73440f40c284\", \"abstract\": \"Two important tasks of machine learning are the statistical learning from sample data (SL) and the unsupervised learning from unlabelled data (UL) (Hastie et al., 2001; Theodoridis & Koutroumbas, 2006). The synthesis of the two parts \\u2013 the unsupervised statistical learning (USL) \\u2013 is frequently used in the cyclic process of inductive and deductive scientific inference. This applies especially to those fields of science where promising, testable hypotheses are unlikely to be obtained based on manual work, the use of human senses or intuition. Instead, huge and complex experimental data have to be analyzed by using machine learning (USL) methods to generate valuable hypotheses. A typical example is the field of functional genomics (Kell & Oliver, 2004). When machine learning methods are used for the generation of hypotheses, human intelligence is replaced by artificial intelligence and the proper functioning of this type of \\u2018intelligence\\u2019 has to be validated. This chapter is focused on the validation of cluster analysis which is an important element of USL. It is assumed that the data set is a sample from a mixture population which is statistically modeled as a mixture distribution. Cluster analysis is used to \\u2018learn\\u2019 the number and characteristics of the components of the mixture distribution (Hastie et al., 2001). For this purpose, similar elements of the sample are assigned to groups (clusters). Ideally, a cluster represents all of the elements drawn from one population of the mixture. However, clustering results often contain errors due to lacking robustness of the algorithms. Rather different partitions may result even for samples with small differences. That is, the obtained clusters have a random character. In this case, the generalization from clusters of a sample to the underlying populations is inappropriate. If a hypothesis derived from such clustering results is used to design an experiment, the outcome of this experiment will hardly lead to a model with a high predictive power. Thus, a new study has to be performed to find a better hypothesis. Even a single cycle of hypothesis generation and hypothesis testing can be time-consuming and expensive (e.g., a gene expression study in cancer research, with 200 patients, lasts more than a year and costs more than 100.000 dollars). Therefore, it is desirable to increase the efficiency and effectiveness of the scientific progress by using suitable validation tools. An approach for the statistical validation of clustering results is data resampling (Lunneborg, 2000). It can be seen as a special Monte Carlo method that is, as a method for\"}, {\"paperId\": \"360ec168d4c7600ee5bfe8673bc7e6e5638464eb\", \"abstract\": \"Abstract Data mining has become a well-established discipline within the domain of artificial intelligence (AI) and knowledge engineering (KE). It has its roots in machine learning and statistics, but encompasses other areas of computer science. It has received much interest over the last decade as advances in computer hardware have provided the processing power to enable large-scale data mining to be conducted. Unlike other innovations in AI and KE, data mining can be argued to be an application rather then a technology and thus can be expected to remain topical for the foreseeable future. This paper presents a brief review of the history of data mining, up to the present day, and some insights into future directions.\"}, {\"paperId\": \"8a73a3b640e38d23e804b13fe393ab79cd7c1c97\", \"abstract\": \"Distributed collaborative optimization is the effective implementation of optimization tasks through cooperation and collaboration of multiple agents. The theory and application of distributed collaborative optimization is one of the important development directions in Control Science and Engineering. In recent years, with the development of emerging technologies such as cloud computing, big data, mobile internet, and artificial intelligence, distributed collaborative optimization has been facing new challenges and opportunities. This paper reviews and summarizes some hot research directions of distributed collaborative optimization in recent years, including distributed accelerated optimization algorithms, distributed non-convex optimization algorithms, and distributed gradient-free optimization algorithms. Moreover, guided by three practical applications of intelligent manufacturing, Energy Internet and distributed machine learning, key future research directions of distributed collaborative optimization are prospected.\"}, {\"paperId\": \"776885bf97ef3e0654fd83ee08fb0ce9f5dea201\", \"abstract\": \"\\n \\n \\nIn this paper we describe eBird, a citizen science project that takes advantage of human observational capacity and machine learning methods to explore the synergies between human computation and mechanical computation. We call this model a Human/Computer Learning Network, whose core is an active learning feedback loop between humans and machines that dramatically improves the quality of both, and thereby continually improves the effectiveness of the network as a whole. Human/Computer Learning Networks leverage the contributions of a broad recruitment of human observers and processes their contributed data with Artificial Intelligence algorithms leading to a computational power that far exceeds the sum of the individual parts. \\n \\n \\n\"}, {\"paperId\": \"8a16c9867e0df2e10e5f9564e9e21e8f806ae83c\", \"abstract\": \"The emerging paradigm of the cognitive city, which augments smart cities with learning and behavioral change capabilities, is gaining increasing attention as a promising solution to the challenges of future mega-cities. Cognitive cities are built upon artificial learning and behavioral analysis techniques founded on the exploitation of human-machine collective intelligence. Hence, cognitive cities rely on the sharing of citizens\\u2019 daily-life data, which might be considered sensitive personal data. In this context, privacy and security of the shared information become critical issues that have to be addressed to guarantee the proper deployment of cognitive cities and the fundamental rights of people. This article provides a thorough literature review using the recommendations for systematic reviews proposed by Vom Brocke et al. and the PRISMA statement. We analyze peer-reviewed publications indexed in ACM Digital Library, IEEE Xplore, Scopus, and Web of Science until July 2020. We identify the main challenges on privacy and information security within cognitive cities, and the proposals described in the literature to address them. We conclude that many challenges remain open and we suggest several research lines that will require further examination in the years to come.\"}, {\"paperId\": \"d11f37233e233bba11fb40e4249d6919698e0977\", \"abstract\": \"An activity fundamental to science is building mathematical models. These models are used to both predict the results of future experiments and gain insight into the structure of the system under study. We present an algorithm that automates the model building process in a scientifically principled way. The algorithm can take observed trajectories from a wide variety of mechanical systems and, without any other prior knowledge or tuning of parameters, predict the future evolution of the system. It does this by applying the principle of least action and searching for the simplest Lagrangian that describes the system\\u2019s behaviour. By generating this Lagrangian in a human interpretable form, it can also provide insight into the workings of the system. Subjects Artificial Intelligence, Data Mining and Machine Learning, Scientific Computing and Simulation\"}, {\"paperId\": \"ef9684395619ddb536bd3ca2591cbf501809de10\", \"abstract\": \"This article is based on the New Horizons lecture delivered at the 2016 Radiological Society of North America Annual Meeting. It addresses looming changes for radiology, many of which stem from the disruptive effects of the Fourth Industrial Revolution. This is an emerging era of unprecedented rapid innovation marked by the integration of diverse disciplines and technologies, including data science, machine learning, and artificial intelligence-technologies that narrow the gap between man and machine. Technologic advances and the convergence of life sciences, physical sciences, and bioengineering are creating extraordinary opportunities in diagnostic radiology, image-guided therapy, targeted radionuclide therapy, and radiology informatics, including radiologic image analysis. This article uses the example of oncology to make the case that, if members in the field of radiology continue to be innovative and continuously reinvent themselves, radiology can play an ever-increasing role in both precision medicine and value-driven health care. \\u00a9 RSNA, 2018.\"}, {\"paperId\": \"6c39ded46031a904e3448d29e71a8482e0aa2801\", \"abstract\": \"Data mining systems are commonly applied to obtain a set of novel, potentially useful, and ultimately interesting patterns from a given (large) data set [1]. This can be achieved using exploratory methods, e. g., utilizing descriptive data mining techniques like methods for association rule mining [2] or subgroup discovery, e. g., [3], [4]. While the resulting patterns are typically interpretable, the large results sets in pattern mining, i. e., a large sets of potentially interesting patterns that the user needs to assess, require further exploration and interpretation techniques. Such problems also occur for other complex models in data mining which often require explanations of their results and/or structure. In this context, explanation-aware approaches have been a prominent research direction in artificial intelligence and data science, e. g., [5]\\u2013 [8]. Recently, the concept of transparent and explainable models has also gained a strong focus and momentum in the data mining and machine learning community, e. g., [9]\\u2013[11]. We introduce explicative data mining as a comprehensive paradigm, tackling all these different aspects. Similar to the philosopical process of explication cf. [12], [13] which aims to make the implicit explicit, explicative data mining aims to model, describe and explain the underlying structure in the data. By targeting interpretable (and transparent) models utilizing exploratory and explanation-aware methods, these can be constructed and inspected on different layers and levels. This ranges from pure data summarization to patternbased exploratory data mining. Furthermore, these features also provide for different options for including the human in the loop, e. g., using visualization methods. We outline and discuss the explicative data mining paradigm in detail: We introduce foundational aspects, present the respective approaches summarized above, and discuss current perspectives and challenges. In particular, we focus on explicative data mining methods, e. g., in the areas of pattern mining [14]\\u2013[16] and feature engineering for machine learning approaches [17], [18], with exemplary applications in network analysis and anomaly detection. The pattern mining methods are discussed in the context of the VIKAMINE1 system [19]. Furthermore, we also discuss the relation to incorporating prior knowledge, e. g., in the form of knowledge graphs [20]\\u2013 [22] into the data mining process [18]. This enables hybrid approaches that incorporate semantic knowledge [23] into the process, e. g., supporting modeling and explanation methods.\"}, {\"paperId\": \"3b5b16d86dccbb96502876315d1a56508c856e46\", \"abstract\": \"As the Architecture, Engineering, Construction, and Facilities Management industry undergoes a profound change with Building Information Modeling (BIM), it seems the right moment to properly re-structure the inherent processes to promote a new wave of innovation. To leverage digital information from each individual project into business value for the whole industry, researchers must borrow knowledge and solutions from computational fields, such as Machine Learning, Artificial Intelligence, Data Mining and Data Science. They will provide a guide to the development, and even transformation of current BIM processes, with potential for development of new tools and automation of many tasks. What is not entirely clear is if BIM could take advantage also from Big Data Analytics, as some professionals are been advocating. In this paper, the author analyzes Big Data problems and the BIM context, and argues that BIM could not immediately take advantage from Big Datainfrastructure. Nevertheless, a route of development is suggested, which extends BIM from its predominantly building-focused models to models that encompass an entire city, which certainly will demand Big Data Analytics. Thus, a new City Information Modeling seems to be the right path of development for BIM as it turns to be integrated with Geographic Information Systems and will lead to tools that would be adequate for future Smart Cities planning and management.\"}, {\"paperId\": \"0983a46960d3542adf11f019c4db80d15ebf516e\", \"abstract\": \"The purpose of the work presented herein is to provide multimedia course material with animations to assist learning some key Computer Science topics on the World Wide Web. The work is presented in eight learning modules: Algorithms, Artificial Intelligence, Data Structures, Machine Architecture, Number Systems, Operating Systems, Programming Languages, and Software Engineering. Each module consists of a set of lessons with animations and interactive components including review questions. The work consists of 44 animations, 2,008 files, and 255 folders, totaling 15.5MB. The learning modules enable educators to provide a learning environment beyond the bounds of the classroom either to supplement their in-class teaching or as part of a distance learning course. The animations and interactive components are expected to improve the effectiveness of learning the covered topics.\"}, {\"paperId\": \"d9a1c9b16166363238a228490df816f3e22b527b\", \"abstract\": \"Educating the workforce of tomorrow is an increasingly critical challenge for areas such as data science, machine learning, and artificial intelligence. These core skills may revolutionize progress in areas such as health care and precision medicine, autonomous systems and robotics, and neuroscience. Skills in data science and artificial intelligence are in high demand in industrial research and development, but we do not believe that traditional recruiting and training models in industry (e.g., internships, continuing education) are serving the needs of the diverse populations of students who will be required to revolutionize these fields. Our program, the Cohort-based Integrated Research Community for Undergraduate Innovation and Trailblazing (CIRCUIT), targets trailblazing, high-achieving students who face barriers in achieving their goals and becoming leaders in data science, machine learning, and artificial intelligence research. Traditional recruitment practices often miss these ambitious and talented students from nontraditional backgrounds, and these students are at a higher risk of not persisting in research careers. In the CIRCUIT program we recruit holistically, selecting students on the basis of their commitment, potential, and need. We designed a training and support model for our internship. This model consists of a compressed data science and machine learning curriculum, a series of professional development training workshops, and a team-based robotics challenge. These activities develop the skills these trailblazing students will need to contribute to the dynamic, team-based engineering teams of the future.\"}, {\"paperId\": \"008e160ba46f49bb0513875525103d0a0bb902b5\", \"abstract\": \"Feature extraction is a critical component of many applied data science workflows. In recent years, rapid advances in artificial intelligence and machine learning have led to an explosion of feature extraction tools and services that allow data scientists to cheaply and effectively annotate their data along a vast array of dimensions--ranging from detecting faces in images to analyzing the sentiment expressed in coherent text. Unfortunately, the proliferation of powerful feature extraction services has been mirrored by a corresponding expansion in the number of distinct interfaces to feature extraction services. In a world where nearly every new service has its own API, documentation, and/or client library, data scientists who need to combine diverse features obtained from multiple sources are often forced to write and maintain ever more elaborate feature extraction pipelines. To address this challenge, we introduce a new open-source framework for comprehensive multimodal feature extraction. Pliers is an open-source Python package that supports standardized annotation of diverse data types (videos, images, audio, and text), and is expressly implemented with both ease-of-use and extensibility in mind. Users can apply a wide range of pre-existing feature extraction tools to their data in just a few lines of Python code, and can also easily add their own custom extractors by writing modular classes. A graph-based API enables rapid development of feature extraction pipelines that output results in a single, standardized format. We describe the package's architecture, detail its advantages over previous feature extraction toolboxes, and use a sample application to a large functional MRI dataset to illustrate how pliers can significantly reduce the time and effort required to construct simple feature extraction workflows while increasing code clarity and maintainability.\"}, {\"paperId\": \"1ecebd0626d43533beed7876a8d09ce96a9b8d14\", \"abstract\": \"Machine learning is applied in every field of human activity using digital data. In recent years, many papers have been published concerning artificial intelligence use in classification, regression and segmentation purposes in medicine and in ophthalmology, in particular. Artificial intelligence is a subsection of computer science and its principles, and concepts are often incomprehensible or used and interpreted by doctors incorrectly. Diagnostics of ophthalmology patients is associated with a significant amount of medical data that can be used for further software processing. By using of machine learning methods, it\\u2019s possible to find out, identify and count almost any pathological signs of diseases by analyzing medical images, clinical and laboratory data. Machine learning includes models and algorithms that mimic the architecture of biological neural networks. The greatest interest in the field is represented by artificial neural networks, in particular, networks based on deep learning due to the ability of the latter to work effectively with complex and multidimensional databases, coupled with the increasing availability of databases and performance of graphics processors. Artificial neural networks have the potential to be used in automated screening, determining the stage of diseases, predicting the therapeutic effect of treatment and the diseases outcome in the analysis of clinical data in patients with diabetic retinopathy, age-related macular degeneration, glaucoma, cataracts, ocular tumors and concomitant pathology. The main characteristics were the size of the training and validation datasets, accuracy, sensitivity, specificity, AUROC (Area Under Receiver Operating Characteristic Curve). A number of studies investigate the comparative characteristics of algorithms. Many of the articles presented in the review have shown the results in accuracy, sensitivity, specificity, AUROC, error values that exceed the corresponding indicators of an average ophthalmologist. Their introduction into routine clinical practice will increase the diagnostic, therapeutic and professional capabilities of a clinicians, which is especially important in the field of ophthalmic oncology, where there is a patient survival matter.\"}, {\"paperId\": \"55b8d0313d25bee5a9533052006bceeda1336e95\", \"abstract\": \"In this paper we address the problem of including the gender dimension in the content of Computer Science, notably in Artificial Intelligence (AI). We analyze first the fairness of Machine Learning (ML) algorithms from a gender point of view. Due to their nature of being bottom-up data-driven algorithms, the most common biases diffused in society about gender and ethnicity can be captured, subsumed and reinforced by them, as many ML applications show. Then, to understand how to develop a new gendered (Computer) Science and promote a gendered innovation in AI, we show a formal reflection on the scientific method utilized to produce innovation and a critical analysis of the logical rules underlying it.\"}, {\"paperId\": \"66c099f600a5fa13176134ed88665f5e473586aa\", \"abstract\": null}, {\"paperId\": \"177b071e53ebe2ec99134fa7d7b79cfcf9a59b99\", \"abstract\": null}, {\"paperId\": \"504f67dae8d296731bb98f1c2b31eed8479fd251\", \"abstract\": \"OBJECTIVE\\nArtificial intelligence (AI) neural networks rapidly convert disparate facts and data into highly predictive analytic models. Machine learning maps image-patient phenotype correlations opaque to standard statistics. Deep learning performs accurate image-derived tissue characterization and can generate virtual CT images from MRI datasets. Natural language processing reads medical literature and efficiently reconfigures years of PACS and electronic medical record information.\\n\\n\\nCONCLUSION\\nAI logistics solve radiology informatics workflow pain points. Imaging professionals and companies will drive health care AI technology insertion. Data science and computer science will jointly potentiate the impact of AI applications for medical imaging.\"}, {\"paperId\": \"210b749e168c868cf04bd95d9dc8ba3a067615da\", \"abstract\": \"The application of data science in cancer research has been boosted by major advances in three primary areas: (1) Data: diversity, amount, and availability of biomedical data; (2) Advances in Artificial Intelligence (AI) and Machine Learning (ML) algorithms that enable learning from complex, large-scale data; and (3) Advances in computer architectures allowing unprecedented acceleration of simulation and machine learning algorithms. These advances help build in silico ML models that can provide transformative insights from data including: molecular dynamics simulations, next-generation sequencing, omics, imaging, and unstructured clinical text documents. Unique challenges persist, however, in building ML models related to cancer, including: (1) access, sharing, labeling, and integration of multimodal and multi-institutional data across different cancer types; (2) developing AI models for cancer research capable of scaling on next generation high performance computers; and (3) assessing robustness and reliability in the AI models. In this paper, we review the National Cancer Institute (NCI) -Department of Energy (DOE) collaboration, Joint Design of Advanced Computing Solutions for Cancer (JDACS4C), a multi-institution collaborative effort focused on advancing computing and data technologies to accelerate cancer research on three levels: molecular, cellular, and population. This collaboration integrates various types of generated data, pre-exascale compute resources, and advances in ML models to increase understanding of basic cancer biology, identify promising new treatment options, predict outcomes, and eventually prescribe specialized treatments for patients with cancer.\"}, {\"paperId\": \"168966fbc0a692eac37b7478c8e10a7025965f64\", \"abstract\": \"The discovery of the relationships between chemical structure and biological function is central to biological science and medicine. In this paper we apply data mining to the problem of predicting chemical carcinogenicity. This toxicology application was launched at IJCAI'97 as a research challenge for artificial intelligence. Our approach to the problem is descriptive rather than based on classification; the goal being to find common substructures and properties in chemical compounds, and in this way to contribute to scientific insight. This approach contrasts with previous machine learning research on this problem, which has mainly concentrated on predicting the toxicity of unknown chemicals. Our contribution to the field of data mining is the ability to discover useful frequent patterns that are beyond the complexity of association rules or their known variants. This is vital to the problem, which requires the discovery of patterns that are out of the reach of simple transformations to frequent itemsets. We present a knowledge discovery method for structured data, where patterns reflect the one-to-many and many-to-many relationships of several tables. Background knowledge, represented in a uniform manner in some of the tables, has an essential role here, unlike in most data mining settings for the discovery of frequent patterns.\"}, {\"paperId\": \"e6a2f04b7276a7f16610fc44aef8e95965de3fa6\", \"abstract\": \"Over the past decade, cloud software has transformed numerous industries\\u2014from finance to logistics, marketing to manufacturing. The simplified aggregation of data, enabled by cloud computing, empowers individuals to glean insights and make data-driven decisions rapidly. In science, however, such a transformation has yet to emerge. The domain lacks centralized, machine-readable repositories of scientific data; this absence inhibits analytics and expedient decision-making. Recently, the Internet of Things (IoT) has served as a catalyst for digitizing and automating science. IoT enables the centralized collection and analysis of scientific data (e.g., instruments, sensors, and environments). Here, we discuss this new technology trend, its applications in laboratories and promise as a platform for improved efficiency, more innovative capabilities, and machine learning/artificial intelligence.\"}, {\"paperId\": \"b583e771f2393e0b82af4423335d9c5e4607d82b\", \"abstract\": null}, {\"paperId\": \"f9bc8f62a11868dcdc66e8b7dec19ef1ac209926\", \"abstract\": \"Translational bioinformatics (TBI) is focused on the integration of biomedical data science and informatics. This combination is extremely powerful for scientific discovery as well as translation into clinical practice. Several topics where TBI research is at the leading edge are 1) the use of large-scale biobanks linked to electronic health records, 2) pharmacogenomics, and 3) artificial intelligence and machine learning. This perspective discusses these three topics and points to the important elements for driving precision medicine into the future.\"}, {\"paperId\": \"4ca1744a06ae8dc4bcbbce40e965bdfd7bb3a769\", \"abstract\": \"The problem discussed in this paper is the paradox associated with the hundreds of contributions of the scientific community over three decades, mainly leveraged by the Strategic Alignment Model (SAM) and its perspectives, but whose benefits arising from its applicability to the real world are scarce. The context of public sector organizations, which does not fit into any of the four perspectives presented by Henderson and Venkatraman (1993), led to the intertwining of the theme with the Social Era, predominantly people-driven or relationship-driven. The hypothesis formulation involves interconnecting convergent concepts, such as Data Science, Artificial Intelligence, Machine Learning and e-participation, to potentiate the change of perspective, allowing to achieve the desired strategic alignment (IT-Business).\"}, {\"paperId\": \"1c49444cff4b5931ecef3186083349bc1521401f\", \"abstract\": null}, {\"paperId\": \"ac0de449272f332f154b1fbbfe8fe90f7f15849a\", \"abstract\": \"In the latter years, we are witnessing a movement from the standard Data Mining towards a more profitable and challenging scenario known as Data Science. It can be defined as a set of quantitative and qualitative approaches that are applied to current relevant problems. In order to be able to \\\"dig\\\" to the deepest level considering the whole information available, the knowledge domain and the analysis of the data must have a strong synergy.There are many fields of application where it is necessary, if not essential, to give an explanation of the phenomenon under study. It is no longer enough to simply apply a Machine Learning model, but it must be comprehensible in order to provide a real decision support system. For this reason, a strong movement has emerged in favour of the eXplainable Artificial Intelligence that aims to respond to the \\\"how\\\" and \\\"why\\\" of the operation of automatic models.In this work, our objective is to show the benefits of one of the learning paradigms of Computational Intelligence: Fuzzy Rule Based Systems and Evolutionary Fuzzy Systems. To this end, we focus on biomedical applications by presenting a case study based on lung cancer prediction from samples taken by liquid biopsy. Liquid biopsy enable us to study genomic alterations for each individual independently, a step towards personalised medicine. The results show the goodness of the solution based on Evolutionary Fuzzy Systems in terms of interpretability and comprehensibility, obtaining a low number of rules with less than 3 fuzzy linguistic labels per antecedent.\"}, {\"paperId\": \"89f864a8f8c22b440d8a2ecd4e27c4fa48b1e641\", \"abstract\": \"The aim of this series is to publish a Reference Library, including novel advances and developments in all aspects of Intelligent Systems in an easily accessible and well structured form. The series includes reference works, handbooks, compendia, textbooks, well-structured monographs, dictionaries, and encyclopedias. It contains well integrated knowledge and current information in the field of Intelligent Systems. The series covers the theory, applications, and design methods of Intelligent Systems. Virtually all disciplines such as engineering, computer science, avionics, business, e-commerce, environment, healthcare, physics and life science are included. The list of topics spans all the areas of modern intelligent systems such as: Ambient intelligence, Computational intelligence, Social intelligence, Computational neuroscience, Artificial life, Virtual society, Cognitive systems, DNA and immunity-based systems, e-Learning and teaching, Humancentred computing and Machine ethics, Intelligent control, Intelligent data analysis, Knowledge-based paradigms, Knowledge management, Intelligent agents, Intelligent decision making, Intelligent network security, Interactive entertainment, Learning paradigms, Recommender systems, Robotics and Mechatronics including human-machine teaming, Self-organizing and adaptive systems, Soft computing including Neural systems, Fuzzy systems, Evolutionary computing and the Fusion of these paradigms, Perception and Vision, Web intelligence and Multimedia.    Indexed by SCOPUS, DBLP, zbMATH, SCImago.  All books published in the series are submitted for consideration in Web of Science.\"}, {\"paperId\": \"1050c586cd00e2b65e2bbcbfe34499b1adc6e9f5\", \"abstract\": \"Property Technology (PropTech) is the next big thing that is going to disrupt the real estate market. Nowadays, we see applications of Machine Learning (ML) and Artificial Intelligence (AI) in almost all the domains but for a long time the real estate industry was quite slow in adopting data science and machine learning for problem solving and improving their processes. However, things are changing quite fast as we see a lot of adoption of AI and ML in the US and European real estate markets. But the Indian real estate market has to catch-up a lot. This paper proposes a machine learning approach for solving the house price prediction problem in the classified advertisements. This study focuses on the Indian real estate market. We apply advanced machine learning algorithms such as Random forest, Gradient boosting and Artificial neural networks on a real world dataset and compare the performance of these methods. We find that the Random forest method is the best performer in terms of prediction accuracy.\"}, {\"paperId\": \"3bd6e3a17d5bbd8de7a1173ba83ddbb53d07463b\", \"abstract\": \"In the era of big data, one of the key challenges is the development of novel optimization algorithms that can accommodate vast amounts of data while at the same time satisfying constraints and limitations of the problem under study. The need to solve optimization problems is ubiquitous in essentially all quantitative areas of human endeavor, including industry and science. In the last decade there has been a surge in the demand from practitioners, in fields such as machine learning, computer vision, artificial intelligence, signal processing and data science, for new methods able to cope with these new large scale problems. \\nIn this thesis we are focusing on the design, complexity analysis and efficient implementations of such algorithms. In particular, we are interested in the development of randomized iterative methods for solving large scale linear systems, stochastic quadratic optimization problems, the best approximation problem and quadratic optimization problems. A large part of the thesis is also devoted to the development of efficient methods for obtaining average consensus on large scale networks.\"}, {\"paperId\": \"369cf149a9116272ae51d4da1aec47a30d916b9c\", \"abstract\": \"Rough set theory has been used extensively in fields of complexity, cognitive sciences, and artificial intelligence, especially in numerous fields such as expert systems, knowledge discovery, information system, inductive reasoning, intelligent systems, data mining, pattern recognition, decision-making, and machine learning. Rough sets models, which have been recently proposed, are developed applying the different fuzzy generalisations. Currently, there is not a systematic literature review and classification of these new generalisations about rough set models. Therefore, in this review study, the attempt is made to provide a comprehensive systematic review of methodologies and applications of recent generalisations discussed in the area of fuzzy-rough set theory. On this subject, the Web of Science database has been chosen to select the relevant papers. Accordingly, the systematic and meta-analysis approach, which is called \\u201cPRISMA,\\u201d has been proposed and the selected articles were classified based on the author and year of publication, author nationalities, application field, type of study, study category, study contribution, and journal in which the articles have appeared. Based on the results of this review, we found that there are many challenging issues related to the different application area of fuzzy-rough set theory which can motivate future research studies.\"}, {\"paperId\": \"99ce75a4817cd54c721e7610388a46912cc29e02\", \"abstract\": \"Text mining or knowledge discovery is that sub process of data mining, which is widely being used to discover hidden patterns and significant information from the huge amount of unstructured written material. The proliferation of clouds, research and technologies are responsible for the creation of vast volumes of data. This kind of data cannot be used until or unless specific information or pattern is discovered. For this text mining uses techniques of different fields like machine learning, visualization, case-based reasoning, text analysis, database technology statistics, knowledge management, natural language processing and information retrieval. Text mining is largely growing field of computer science simultaneously to big data and artificial intelligence. This paper contains the review of text mining techniques, tools and various applications.\"}, {\"paperId\": \"e7cd661554ce6c656f9ca4c31d3126f4a1874bf5\", \"abstract\": \"In recent years, emotion detection in text has become more popular due to its vast potential applications in marketing, political science, psychology, human-computer interaction, artificial intelligence, etc. In this work, we argue that current methods which are based on conventional machine learning models cannot grasp the intricacy of emotional language by ignoring the sequential nature of the text, and the context. These methods, therefore, are not sufficient to create an applicable and generalizable emotion detection methodology. Understanding these limitations, we present a new network based on a bidirectional GRU model to show that capturing more meaningful information from text can significantly improve the performance of these models. The results show significant improvement with an average of 26.8 point increase in F-measure on our test data and 38.6 increase on the totally new dataset.\"}, {\"paperId\": \"c64b1c09ccc027ef1efa97011b863f9fe8c88fc8\", \"abstract\": null}, {\"paperId\": \"0d9deb24acd43ac6d0a780b76d325acf15e4a19f\", \"abstract\": \"In the last two decades, a highly instrumentalist form of statistical and machine learning has achieved an extraordinary success as the computational heart of the phenomenon glossed as \\u201cpredictive analytics,\\u201d \\u201cdata mining,\\u201d or \\u201cdata science.\\u201d This instrumentalist culture of prediction emerged from subfields within applied statistics, artificial intelligence, and database management. This essay looks at representative developments within computational statistics and pattern recognition from the 1950s onward, in the United States and beyond, central to the explosion of algorithms, techniques, and epistemic values that ultimately came together in the data sciences of today. This essay is part of a special issue entitled Histories of Data and the Database edited by Soraya de Chadarevian and Theodore M. Porter.\"}, {\"paperId\": \"92cf280721d2c8108133f999548f572467519d34\", \"abstract\": null}, {\"paperId\": \"83aa1fb5ed9a3cc1ed88b6bc7f8c6d802ef2f3a9\", \"abstract\": null}, {\"paperId\": \"e91d193f316d37b953a64a25ae20bde56b0e6617\", \"abstract\": \"https://jkms.org Artificial intelligence (AI) has been highlighted as a mechanism to realize precision medicine because it contributes to analyzing healthcare big data.1,2 Especially, among diverse AI methods, machine learning (ML) methods including deep learning algorithms are widely applied to analyze healthcare data.2 ML requires a vast amount of data due to its nature. This means that collecting as much relevant data as possible is a critical task. The Precision Medicine Initiative3 or Observational Health Data Sciences and Informatics (OHDSI)4 might be representative cases for collecting healthcare big data. However, since healthcare data contain the most sensitive personal information, the concerns on protecting patients' privacy are increasing.\"}, {\"paperId\": \"fd08484d50f4bfafd8078bf5fdee57e3e948f471\", \"abstract\": \"Data mining is commonly defined as the computer-assisted search for interesting patterns and relations in large databases. It is a relatively young area of research that builds on the older disciplines of statistics, databases, artificial intelligence (machine learning) and data visualization. The emergence of data mining is often explained by the ever increasing size of databases together with the availability of computing power and algorithms to analyse them. Data mining is usually considered to be a form of secondary data analysis. This means that it is often performed on data collected and stored for a different purpose than analysis; usually for administrative purposes. In this chapter we consider the possibilities of applying data mining in economic science. In doing so, we must naturally be aware of the considerable amount of research that has already been done in economic data analysis. To what extent can data mining contribute to the analysis of economic data? In answering this question we could consider data mining as a collection of techniques and algorithms that have been developed in this area of research. In doing so we could compare data mining algorithms to data analysis techniques more commonly used in economics, and see if they allow us to answer different questions, or to answer existing questions in a better way. Alternatively, we can also consider data mining as a highly exploratory form of data analysis that is data driven rather than theory driven. The latter aspect of data mining is most important in this contribution. This chapter is organized as follows. In section 2 we give a brief description of the object of study of economics. Then we consider economic modelling as a way to apply economic theory to particular problems and as a tool to deduce the consequences of particular assumptions. In order to give empirical content to\"}, {\"paperId\": \"ae1e67421a9e45fa016562001e4015c3ac08c46a\", \"abstract\": \"This report documents the program and the outcomes of Dagstuhl Seminar 15041 \\\"Model-driven Algorithms and Architectures for Self-Aware Computing Systems\\\". The design of self-aware computing systems calls for an integrated interdisciplinary approach building on results from multiple areas of computer science and engineering, including software and systems engineering, systems modeling, simulation and analysis, autonomic and organic computing, machine learning and artificial intelligence, data center resource management, and so on. The Dagstuhl Seminar 15041 served as a platform to raise the awareness about the relevant research efforts in the respective research communities as well as existing synergies that can be exploited to advance the state-of-the-art, formulate a new research agenda that takes a broader view on the problem following an integrated and interdisciplinary approach, and establish collaborations between academia and industry.\"}, {\"paperId\": \"aeac3979a0c20b71d0e1aff4668d0a036c300da5\", \"abstract\": \"Integrating games into the computer science curriculum has been gaining acceptance in recent years, particularly when used to improve student engagement in introductory courses. This paper argues that games can also be useful in upper level courses, such as general artificial intelligence and machine learning. We provide a case study of using a Mario game in a machine learning class to provide one successful data point where both content-specific and general learning outcomes were successfully achieved\"}, {\"paperId\": \"2c2fd1a5920b768f50d11ef3e48b0ffee59a6392\", \"abstract\": \"Explainable Artificial Intelligence (XAI) is an emergent research field which tries to cope with the lack of transparency of AI systems, by providing human understandable explanations for the underlying Machine Learning models. This work presents a new explanation extraction method called LEAFAGE. Explanations are provided both in terms of feature importance and of similar classification examples. The latter is a well known strategy for problem solving and justification in social science. LEAFAGE leverages on the fact that the reasoning behind a single decision/prediction for a single data point is generally simpler to understand than the complete model; it produces explanations by generating simpler yet locally accurate approximations of the original model. LEAFAGE performs overall better than the current state of the art in terms of fidelity of the model approximation, in particular when Machine Learning models with non-linear decision boundaries are analysed. LEAFAGE was also tested in terms of usefulness for the user, an aspect still largely overlooked in the scientific literature. Results show interesting and partly counter-intuitive findings, such as the fact that providing no explanation is sometimes better than providing certain kinds of explanation.\"}, {\"paperId\": \"870cd4eba34cbf60dca0afd41fd39bd1df9fff50\", \"abstract\": null}, {\"paperId\": \"1f1e587af0ed864c7030ff86e96dfdff05a854e6\", \"abstract\": \"Artificial intelligence is the outlet of computer science apprehensive with creating computers that perform as humans. It compromises expert systems, playing games, natural language, and robotics. However, soft computing (SC) varies from the hard (conventional) computing in its tolerant of partial truth, uncertainty, imprecision, and approximation, thus, it models the human mind. The most common SC techniques include neural networks, fuzzy systems, machine learning, and the meta-heuristic stochastic algorithms (e.g., Cellular automata, ant colony optimization, Memetic algorithms, particle swarms, Tabu search, evolutionary computation and simulated annealing. Due to the required accurate diseases analysis, magnetic resonance imaging, computed tomography images and images of other modalities segmentation remains a challenging problem. Over the past years, soft computing approaches attract attention of several researchers for problems solving in medical data applications. Image segmentation is the process that partitioned an image into some groups based on similarity measures. This process is employed for abnormalities volumetric analysis in medical images to identify the disease nature. Recently, meta-heuristic algorithms are conducted to support the segmentation techniques. In the current chapter, different segmentation procedures are addressed. Several meta-heuristic approaches are reported with highlights on their procedures. Finally, several medical applications using meta-heuristic based-approaches for segmentation are discussed. Meta-Heuristic Algorithms in Medical Image Segmentation: A Review\"}, {\"paperId\": \"ee1ef904203baeb6f35e3880eb5f693c9cb08b40\", \"abstract\": \"15 As meteorological observing systems and models grow in complexity and number, the size of 16 the data becomes overwhelming for humans to analyze using traditional techniques. Com17 puter scientists, and specifically machine learning and data mining researchers, are develop18 ing frameworks for analyzing big data. The AMS Committee on Artificial Intelligence and 19 its Applications to Environmental Science aims to bring AI researchers and environmental 20 scientists together to increase the synergy between the two. The AI committee has spon21 sored 4 previous contests on a variety of meteorological problems including wind energy, 22 storm classification, winter hydrometeor classification, and air pollution, with the goal of 23 bringing together the two fields of research. Although these were successful, the audience 24 was limited to existing environmental science researchers (usually 10-20 teams of people 25 primarily within the AMS community). For the 2013/14 contest, we expanded to a global 26 audience by focusing on the compelling problem of solar energy prediction and by having 27 the established forum Kaggle host our contest. Using this forum, we had over 160 teams 28 from all around the world participate. Improved solar energy forecasting is a necessary com29 ponent of making solar energy a viable alternative power source. This paper summarizes 30 our experiences in the 2013/14 contest, discusses the data in detail, and presents the win31 ning prediction methods. The contest data come from the NOAA/ESRL Global Ensemble 32 Forecasting System Reforecast Version 2 and the Oklahoma Mesonet with sponsorship from 33 EarthRisk Technologies. All winning methods utilized gradient boosted regression trees but 34 differed in parameter choices and interpolation methods. 35\"}, {\"paperId\": \"ae45f035d155b4001c53436fc7e7147c3d39eb39\", \"abstract\": \"Recent debates have revealed the urgent need for work addressing social and ethical implications of the algorithmic and data\\u2010driven systems that govern our lives. Focusing largely on machine learning (ML) and artificial intelligence (AI), conversations among scholars, journalists, and advocates have started to address questions of fairness, bias, transparency, access, participation, and discrimination often under the monikers \\u201cAI and Ethics\\u201d (AI+Ethics) or \\u201cfairness, accountability, and transparency in algorithms\\u201d (FAT*). Underlying these discourses are concerns about how to mitigate discrimination and bias in data, as well as open questions over whether algorithmic systems can be fair and if their use will help promote equitable futures (or instead further perpetuate existing inequalities). Despite the seeming \\u201cnewness\\u201d of these issues, many of the underlying concerns have a longstanding tradition and intellectual lineage in the library and information science (LIS) field. In this panel, we will discuss and elaborate on these connections, drawing on our research and experiences in a number of contexts and outlining opportunities these relationships present for future research and engagement.\"}, {\"paperId\": \"46841195d91aebf4c06c8d28d81f8a2283a20f1e\", \"abstract\": \"Data Mining is one of the most motivating areas of research that is become increasingly popular in health organization. Mining plays an important role for uncovering new trends in healthcare organization which in turn helpful for all the parties associated with this field. It is a new powerful technology which is of high interest in computer world. It is a sub field of computer science that uses already existing data in different databases to transform it into new researches and results. It makes use of Artificial Intelligence, machine learning and database management to extract new patterns from large data sets and the knowledge associated with these patterns. The actual task is to extract data by automatic or semi-automatic means. The different parameters included in data mining include clustering, forecasting, path analysis and predictive analysis.\"}, {\"paperId\": \"a5502187140cdd98d76ae711973dbcdaf1fef46d\", \"abstract\": \"This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding. AllenNLP is designed to support researchers who want to build novel language understanding models quickly and easily. It is built on top of PyTorch, allowing for dynamic computation graphs, and provides (1) a \\ufb02exible data API that handles intelligent batching and padding, (2) high-level abstractions for common operations in working with text, and (3) a modular and extensible experiment framework that makes doing good science easy. It also includes reference implementations of high quality approaches for both core semantic problems (e.g. semantic role labeling (Palmer et al., 2005)) and language understanding applications (e.g. machine comprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source effort maintained by engineers and researchers at the Allen Institute for Arti\\ufb01cial Intelligence.\"}, {\"paperId\": \"c3907fe1890840f3932d99e67a5cd6e8532ae222\", \"abstract\": \"EDUCATION University of Toronto, Toronto, Canada Sep 2018 Present \\u2022 Ph.D. in Computer Science GPA: 4.0/4.0 Georgia Institute of Technology, Atlanta, USA Aug 2016 \\u2013 May 2018 \\u2022 M.S. in Computer Science \\u2013 Specialization in Machine Learning GPA: 4.0/4.0 \\u2022 Selected Coursework: Natural Language Processing, Deep Learning, Machine Learning, Artificial Intelligence, Advanced Computer Vision, High Performance Computer Architecture University of Waterloo, Waterloo, Canada Sep 2012 \\u2013 Jun 2016 \\u2022 B.S. in Computer Science with Economics Minor GPA: 93.38% \\u2022 Graduated with Distinction \\u2013 Dean\\u2019s Honours List \\u2022 Selected Coursework: Artificial Intelligence, Machine Learning, Medical Image Processing, Computational Audio, Graphics, Network, Security, Architecture, Database, Data Structure and Algorithms, Probability & Statistics PUBLICATIONS 2. Slang Detection and Identification \\u2022 Zhengqi Pei, Zhewei Sun, Yang Xu \\u2022 In Proceedings of the 2019 Conference on Computational Natural Language Learning. CoNLL 2019.\"}, {\"paperId\": \"d29d23101251c9dcb84df17e8ef8ef467a89e792\", \"abstract\": \"Machine learning is the science of discovering statistical dependencies in data, and the use of those dependencies to perform predictions. During the last decade, machine learning has made spectacular progress, surpassing human performance in complex tasks such as object recognition, car driving, and computer gaming. However, the central role of prediction in machine learning avoids progress towards general-purpose artificial intelligence. As one way forward, we argue that causal inference is a fundamental component of human intelligence, yet ignored by learning algorithms. \\nCausal inference is the problem of uncovering the cause-effect relationships between the variables of a data generating system. Causal structures provide understanding about how these systems behave under changing, unseen environments. In turn, knowledge about these causal dynamics allows to answer \\\"what if\\\" questions, describing the potential responses of the system under hypothetical manipulations and interventions. Thus, understanding cause and effect is one step from machine learning towards machine reasoning and machine intelligence. But, currently available causal inference algorithms operate in specific regimes, and rely on assumptions that are difficult to verify in practice. \\nThis thesis advances the art of causal inference in three different ways. First, we develop a framework for the study of statistical dependence based on copulas and random features. Second, we build on this framework to interpret the problem of causal inference as the task of distribution classification, yielding a family of novel causal inference algorithms. Third, we discover causal structures in convolutional neural network features using our algorithms. The algorithms presented in this thesis are scalable, exhibit strong theoretical guarantees, and achieve state-of-the-art performance in a variety of real-world benchmarks.\"}, {\"paperId\": \"b9bedd5baef3121001207cc8be292572aa6f9870\", \"abstract\": \"Since 2010, machine-learning-based predictive techniques, and more specifically deep learning neural networks, have achieved spectacular performances in the fields of image recognition and automatic translation, under the umbrella term of \\u201cArtificial Intelligence\\u201d (AI). But their relation to this field of research is not straightforward. In the tumultuous history of AI, learning techniques using so-called \\u201cconnectionist\\u201d neural networks have long been mocked and ostracized by the \\u201csymbolic\\u201d movement. This article retraces the history of artificial intelligence through the lens of the tension between symbolic and connectionist approaches. From a social history of science and technology perspective, it highlights how researchers, relying on the availability of massive data and the multiplication of computing power, have undertaken to reformulate the symbolic AI project by reviving the spirit of adaptive and inductive machines dating back from the era of cybernetics.\"}, {\"paperId\": \"044ac54ae1e575655de6465a1a38774f85e6f310\", \"abstract\": null}, {\"paperId\": \"01bc6e9fc63e46213a84d2e1962f57effdd7a7c6\", \"abstract\": null}, {\"paperId\": \"c4ccf9a9a5cf66be0305b858a14ba259ea86bdf3\", \"abstract\": null}, {\"paperId\": \"1b993b6e66c79a16ffa8b59b820fe63072a45b92\", \"abstract\": \"Computational linguistics is not a specialization of lin- guistics at all; it is a branch of computer science. A large majority of computational linguists have degrees in computer science and positions in computer science departments. It was founded as an offshoot of an en- gineering discipline (machine translation), and has been subsequently shaped by its place within artificial intelligence, and by a heavy influx of theory and method from speech recognition (another engineering discipline) and machine learning. But computation is a means to an end; the essential feature is data collection, analysis, and prediction on the large scale. I will call it data-intensive experimental linguistics. I wish to explain how data-intensive linguistics differs from mainstream practice, why I consider it to be genuine linguistics, and why I believe that it enables fundamental advances in our understanding of language.\"}, {\"paperId\": \"0dbc21e4e09d3c862b29d0714208f4d9a3738770\", \"abstract\": \"While research on emotions has become one of the most productive areas at the intersection of cognitive science, artificial intelligence and natural language processing, the diversity and incommensurability of emotion models seriously hampers progress in the field. We here propose kNN regression as a simple, yet effective method for computationally mapping between two major strands of emotion representations, namely dimensional and discrete emotion models. In a series of machine learning experiments on data sets of textual stimuli we gather evidence that this approach reaches a human level of reliability using a relatively small number of data points only.\"}, {\"paperId\": \"f57853ffb4a55581245f41a75eb72acacff39ab9\", \"abstract\": \"The authors discuss opportunities, challenges and limitations of machine learning (ML) in cardiovascular medicine1 and they classified ML as being part of artificial intelligence (AI). AI was defined as the set of analytical algorithms that can find structures or patterns in data without explicitly being programmed where to look. We fully support the quest of the authors to increase the use of algorithms in medicine, but we argue that the role of statistics as part of data science could be acknowledged and positioned better.\\n\\nStatistics has been defined in similar terms as data science,2 but due to algorithms like support vector machines and decision trees, statistics is undeniably a subset of \\u2026\"}, {\"paperId\": \"86b62982452e301258e1aa7c90ff14e920f0c616\", \"abstract\": null}, {\"paperId\": \"39e0f51a90080da3daea903fb89ac7408bdb26f2\", \"abstract\": null}, {\"paperId\": \"5ece8f317000985927e2c2158723844fcadf6b00\", \"abstract\": \"Introduction Data mining is the process of discovering useful and previously unknown information and relationships in large data sets (Campos, Stengard, & Milenova, 2005; Tan, Steinbach, & Kumar, 2006). Accordingly, data mining is the purposeful use of information technology to implement algorithms from machine learning, statistics, and artificial intelligence to analyze large data sets for the purpose of decision support. The field of data mining grew out of limitations in standard data analysis techniques (Tan et al., 2006). Advancements in machine learning, pattern recognition, and artificial intelligence algorithms coupled with computing trends (CPU power, massive storage devices, high-speed connectivity, and software academic initiatives from companies like Microsoft, Oracle, and IBM) enabled universities to bring data mining courses into their curricula (Jafar, Anderson, & Abdullat, 2008b). Accordingly, Computer Science and Information Systems programs have been aggressively introducing data mining courses into their curricula (Goharian, Grossman, & Raju, 2004; Jafar, Anderson, & Abdullat 2008a; Lenox & Cuff, 2002; Saquer, 2007). Computer Science programs focus on the deep understanding of the mathematical aspects of data mining algorithms and their efficient implementation. They require advanced programming and data structures as prerequisites for their courses (Goharian et al., 2004; Musicant, 2006; Rahal, 2008). Information Systems programs on the other hand, focus on the data analysis and business intelligence aspects of data mining. Students learn the theory of data mining algorithms and their applications. Then they use tools that implement the algorithms to build mining models to analyze data for the purpose of decision support. Accordingly, a first course in programming, a database management course, and a statistical data analysis course suffice as prerequisites. For Information Systems programs, a data centric, algorithm understanding and process-automation approach to data mining similar to Jafar et al. (2008a) and Campos et al. (2005) is more appropriate. A data mining course in an Information Systems program has an (1) analytical component, (2) a tools-based, hands-on component ,and (3) a rich collection of data sets. (1) The analytical component covers the theory and practice of the lifecycle of a data mining analysis project, elementary data analysis, market basket analysis, classification and prediction (decision trees, neural networks, naive Bayes, logistic regression, etc.), cluster analysis and category detection, testing and validation of mining models, and finally the application of mining models for decision support and prediction. Textbooks from Han and Kamber (2006) and Tan et al. (2006) provide a comprehensive coverage of the terminology, theory, and algorithms of data mining. (2) The hands-on component requires the use of tools to build projects based on the algorithms learned in the analytical component. We chose Microsoft Excel with its data mining add-in(s) as the front-end and Microsoft's Cloud Computing and SQL Server 2008 data mining computing engines as the back-end. Microsoft Excel is ubiquitous. It is a natural front-end for elementary data analysis and presentation of data. Its data mining add-in(s) are available as a free download. The add-in(s) are automatically configured to send data to Microsoft's Cloud Computing engine server. The server performs the necessary analysis and receives analysis results back into Excel to present them in tabulated and chart formats. Using wizards, the add-in(s) are easily configured to connect to a SQL Server 2008 running analysis services to send data and receive analysis results back into Excel for presentation. The add-in(s) provide a rich wizard-based, uniform graphical user interface to manage the data, the data mining models, the configurations, and the pre and post view of data and mining models. \\u2026\"}, {\"paperId\": \"3606d46c590c7ace6f161b4c7cec51be9a35e55f\", \"abstract\": \"The novel research area of functional genomics investigates biochemical, cellular, or physiological properties of gene products with the goal of understanding the relationship between the genome and the phenotype. These developments have made analgesic drug research a data\\u2010rich discipline mastered only by making use of parallel developments in computer science, including the establishment of knowledge bases, mining methods for big data, machine\\u2010learning, and artificial intelligence, (Table ) which will be exemplarily introduced in the following.\"}, {\"paperId\": \"e12e588be9b8df29ca1af22be87a342b8805be98\", \"abstract\": \"Courses and research are offered in a variety of subfields of computer science, including operating systems, computer architecture, computer graphics, pattern recognition, automata theory, combinatorics, artificial intelligence, machine learning, database design, computer networks, programming languages, software systems, analysis of algorithms, computational complexity, parallel processing, VLSI, computational geometry, design automation, cyber security, information assurance and data science.\"}, {\"paperId\": \"b7a6ea5fdadaa35d1df01c5ec36d3b4de4b37418\", \"abstract\": null}, {\"paperId\": \"c6a744a38c5dd65727ec922a2279dfc60ef0adb9\", \"abstract\": \"This volume maps the watershed areas between two 'holy grails' of computer science: the identification and interpretation of affect including sentiment and mood. The expression of sentiment and mood involves the use of metaphors, especially in emotive situations. Affect computing is rooted in hermeneutics, philosophy, political science and sociology, and is now a key area of research in computer science. The 24/7 news sites and blogs facilitate the expression and shaping of opinion locally and globally. Sentiment analysis, based on text and data mining, is being used in the looking at news and blogs for purposes as diverse as: brand management, film reviews, financial market analysis and prediction, homeland security. There are systems that learn how sentiments are articulated. This work draws on, and informs, research in fields as varied as artificial intelligence, especially reasoning and machine learning, corpus-based information extraction, linguistics, and psychology.\"}, {\"paperId\": \"eee3f8d994d1f96f063f8ec593ca0e3ddfd4e31f\", \"abstract\": null}, {\"paperId\": \"15044f53b0bc0f2e7bc965ac2663e4ca33fb50ff\", \"abstract\": \"learning concept has been incorporated by number of software and devices in the computer science and information industry. These software and devices are capable in decision making just like a human brain. This capability of decision making is govern by artificial intelligence techniques. These techniques follow many algorithms developed for decision making and machine learning. Decision making depends upon the profound training of contemporary data in a particular domain. Data plays a major and important part as one of the element in any machine learning algorithm. The main focus of this paper is on developing a machine learning algorithm that helps in training the available medical domain data to prepare a data model that negotiates with the query. This is achieved through the analysis of different machine learning methodologies like Support Vector Machines (SVM), Decision Trees and Recursive Partitioning (RP) algorithm and their model building processes. A new data mining and machine learning algorithm is proposed along with the performance analysis over the medical domain dataset. The analysis indicates that as the data size increases there is a continuous increase in algorithm accuracy but concurrently its time consumption also increases.\"}, {\"paperId\": \"5fe462c3f68b671e493376e821c716864cd1a1ae\", \"abstract\": \"The history of research in finance and economics has been widely impacted by the field of Agent-based Computational Economics (ACE). While at the same time being popular among natural science researchers for its proximity to the successful methods of physics and chemistry for example, the field of ACE has also received critics by a part of the social science community for its lack of empiricism. Yet recent trends have shifted the weights of these general arguments and potentially given ACE a whole new range of realism. At the base of these trends are found two present-day major scientific breakthroughs: the steady shift of psychology towards a hard science due to the advances of neuropsychology, and the progress of artificial intelligence and more specifically machine learning due to increasing computational power and big data. These two have also found common fields of study in the form of computational neuroscience, and human-computer interaction, among others. We outline here the main lines of a computational research study of collective economic behavior via Agent-Based Models (ABM) or Multi-Agent System (MAS), where each agent would be endowed with specific cognitive and behavioral biases known to the field of neuroeconomics, and at the same time autonomously implement rational quantitative financial strategies updated by machine learning. We postulate that such ABMs would offer a whole new range of realism.\"}, {\"paperId\": \"bb80fbae335935438fc0cd4c197a0b05186d763f\", \"abstract\": \"We compare person-to-person service encounters with those in which the service provider is an information system to identify the capabilities needed to personalize a service encounter. We suggest \\u201csubstituting information for interaction\\u201d as a principle that unifies these different types of encounters whenever the information needed to create value in a service system accumulates incrementally through human or automated customer interactions. We review research and practice in computer science, artificial intelligence, data mining, machine learning, and information systems design to bring an interdisciplinary robustness to our conceptual proposal. Human service providers and automated service systems both need (1) a service model manager that stores information about how a customer requests a service; (2) a customer model manager that stores information about customers and preferences; (3) a recommendation system manager that uses service models, customer models, and contextual information to adapt the service at delivery time; (4) a learning system that analyzes previous service encounters to refine service and customer models, and a (5) service monitoring system that monitors the status of service delivery. The substitution concepts and mechanisms we propose highlight the range of design choices and help managers evaluate whether a human interaction or information exchange creates or undermines value in a service system.\"}, {\"paperId\": \"c6758dd6d31038c6fa1186762de8c997b2b12972\", \"abstract\": \"There is a suite of methods developed primarily in computer science and artificial intelligence which may be useful in genome-enabled animal breeding. These methods are a response to a growing need for extracting knowledge from complex and massive data, such as molecular markers or transcriptional profiles (Bishop (2006), Hastie et al. (2009)). Some of the procedures cannot be casted in a probabilistic framework, but statisticians have begun to explore their properties (Tibshirani, 1994). This approach is often referred to as \\u201cstatistical learning\\u201d or \\u201cmachine learning\\u201d; the latter term underscores computer intensiveness. Predominant aims are data mining, pattern recognition and prediction, as opposed to inference. Here, we review our experience in Wisconsin when applying several such methods to animal breeding, focusing on genome-enabled prediction of outcomes.\"}, {\"paperId\": \"d657bbc207c2cf1762d63ff7c9d7400723dfaa0e\", \"abstract\": null}, {\"paperId\": \"3d35a669e5888e25c1daf3a5fdd87299e0cd1f9b\", \"abstract\": \"The science of extracting useful information from large data sets or databases is named as data mining. Though data mining concepts have an extensive history, the term \\u201cData Mining\\u201c, is introduced relatively new, in mid 90\\u2019s. Data mining covers areas of statistics, machine learning, data management and databases, pattern recognition, artificial intelligence, and other areas. All of these are concerned with certain aspects of data analysis, so they have much in common but each also has its own distinct problems and types of solution. The fundamental motivation behind data mining is autonomously extracting useful information or knowledge from large data stores or sets. The goal of building computer systems that can adapt to special situations and learn from their experience has attracted researchers from many fields, including computer science, engineering, mathematics, physics, neuroscience and cognitive science.\"}, {\"paperId\": \"566ae4f804ef5317d32411a9c3287f6434c3863b\", \"abstract\": null}, {\"paperId\": \"85a6b67baac75c9d585aab2ebe630de3ddddcf15\", \"abstract\": null}, {\"paperId\": \"e3cd4a68c746c6b1eff61d2e6dd42707a5ebb426\", \"abstract\": \"One of the fundamental objectives of Computer Science is to reduce the menial, repetitive and mundane tasks. It might be arithmetic calculations or maintaining huge amount of data. With the advent of Artificial Intelligence and Machine Learning, we are a step ahead. We not only make the machine perform these tasks but also those which would require the intelligence and the reasoning of a human brain. This involves decision making, interpretation and prediction. The project makes an attempt to minimize human intervention in a wide range of domains. The tool that is built is capable of having human-like conversations with a person. Currently, the conversational model built is capable of having casual conversations in a variety of topics which involve no domain specific knowledge. The same can be replicated in a domain specific environment by including the previous conversations or data of that specific domain. The concepts of Machine Learning and Artificial Intelligence i.e., Recurrent Neural Networks along with LSTMs (Long Short Term Memory) are used to train the neural network.\"}, {\"paperId\": \"2cdcb0892969404f4ae4c4ff1783dbbd8bfc7f03\", \"abstract\": \"With billions of users and hundreds of billions of tweets and posts every year, social media has brought big data to social science. It has also opened an unprecedented opportunity to use artificial intelligence (AI) to glean meaning from the mass of human communications. The University of Pennsylvania9s Positive Psychology Center, for example, uses machine learning and natural language processing to sift through gobs of data to gauge the public9s emotional and physical health, including levels of depression and trust, and several personality traits. That9s traditionally done with surveys. But social media data is cheap and abundant. It is also messy, but AI offers a powerful way to reveal patterns.\"}, {\"paperId\": \"9bc67379bf927f5a905ae7d1f6b447159d413436\", \"abstract\": \"To meet current keen demand for producing next-generation workforce equipped with skills and expertise in big-data analytics, we developed an innovative systematic pedagogy for integrated research-education (INSPIRE model) that is centered around two great challenges: (1) Transforming multidisciplinary STEM training so that it enhances emerging problem-solving capacity and (2) Training STEM students how to have a bigger hand in performing large-scale scientific work. To help strengthen the problem-solving skills and leadership abilities of STEM graduates, we reform the current STEM research training in Bioinformatics and CIS (Computer and Information Sciences) so that it helps us reach the goal of catalyzing science and research training. Our main research hypothesis is that critical improvement in the way big-data scientists are trained comes not solely from large-scale data mining but, in addition, comes from developing useful machine learning and artificial intelligence techniques that automate intelligent learning derived from big-data. The INSPIRE model was built by enablers in the scientific community, and indeed, by the community at large, to help resolve the scarcity of those Professionally Skilled / Trained in Big Data analytics (PSTBD) issue by equipping students with a versatile cross-disciplinary skill set. There is a dire need for those of us in the scientific and academic community to be able to transfer our own successes into perfecting the feedback-based machine learning - cognitive science INSPIRE model, one that places a heavy emphasis on providing individualized training to individuals from all walks of life, including large populations of minorities and women, so that all efforts are made as collaboratively as possible, and the benefits of the sewn seeds may be reaped by everyone. We integrate our secure privacy preserving and causal genetic alteration research at single-cell resolution to demonstrate the model. On an even grander scale, we enhance the PSTBD research by developing the INSPIRE model so that broader social impacts can be made by such newly created fields as Systems Genomics at single-cell level and fields fostered by creative cross-disciplinary genomic big-data analytics (http://americancse.org/events/csce2017/keynotes_lectures/yang_talk) with catalyzed learning-research synergies\"}, {\"paperId\": \"d11553bcb48e30a254a695c06cee7609b171caf2\", \"abstract\": \"Computational & Mathematical Organization Theory provides an international forum for interdisciplinary research that combines computation, organizations and society. The goal is to advance the state of science in formal reasoning, analysis, and system building drawing on and encouraging advances in areas at the confluence of social networks, artificial intelligence, complexity, machine learning, sociology, business, political science, economics, and operations research. The papers in this journal will lead to the development of newtheories that explain and predict the behaviour of complex adaptive systems, new computational models and technologies that are responsible to society, business, policy, and law, new methods for integrating data, computational models, analysis and visualization techniques.  Various types of papers and underlying research are welcome. Papers presenting, validating, or applying models and/or computational techniques, new algorithms, dynamic metrics for networks and complex systems and papers comparing, contrasting and docking computational models are strongly encouraged. Both applied and theoretical work is strongly encouraged. The editors encourage theoretical research on fundamental principles of social behaviour such as coordination, cooperation, evolution, and destabilization. The editors encourage applied research representing actual organizational or policy problems that can be addressed using computational tools. Work related to fundamental concepts, corporate, military or intelligence issues are welcome.  The journal publishes a number of special issues on focused topics, including organizations of intelligent agents, counter-terrorism, computational statistics for networks, and organizations in crises. In addition, tutorial papers, such as how to check the robustness of a simulation, or system details such as algorithm descriptions are also welcome. The audience is international in scope.\"}, {\"paperId\": \"2eec67a62c3b95c2ada77c58d435b14e9a93c9b0\", \"abstract\": null}, {\"paperId\": \"ab6934d2bafede3c7d7509d8188e4e9376768349\", \"abstract\": \"Processing of natural language is branch of linguistics, artificial intelligence & computer science and its purpose is to have interaction among natural language of human beings and computers. We can say it is related to field of computer-human interaction. There are different challenges in this field like understanding of natural language i.e. allowing machines to have understanding from natural language of human beings. Mostly available tasks of natural language processing are: analysis of discourse, morphological separation, machine translation, generation and understanding of natural language, recognition of named entities, part of speech tagging, recognition of optical characters, recognition of speech and analysis of sentiments etc. Current research in NLP is showing more interest on learning algorithms which are either unsupervised or semi-supervised in nature. These techniques of learning can perform this task of learning from data which is not annotated manually with required answers or by applying mixture of non-annotated & annotated data. Normally, this job is very hard as compared to learning which is supervised & usually shows little correct results for particular amount of data as input. But there is large quantity of data is available which is non annotated in nature i.e. whole contents available on world wide web and it normally produces less accurate results. This paper discusses about a survey of different techniques of natural language processing.\"}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 600, \"next\": 700, \"data\": [{\"paperId\": \"ab6934d2bafede3c7d7509d8188e4e9376768349\", \"abstract\": \"Processing of natural language is branch of linguistics, artificial intelligence & computer science and its purpose is to have interaction among natural language of human beings and computers. We can say it is related to field of computer-human interaction. There are different challenges in this field like understanding of natural language i.e. allowing machines to have understanding from natural language of human beings. Mostly available tasks of natural language processing are: analysis of discourse, morphological separation, machine translation, generation and understanding of natural language, recognition of named entities, part of speech tagging, recognition of optical characters, recognition of speech and analysis of sentiments etc. Current research in NLP is showing more interest on learning algorithms which are either unsupervised or semi-supervised in nature. These techniques of learning can perform this task of learning from data which is not annotated manually with required answers or by applying mixture of non-annotated & annotated data. Normally, this job is very hard as compared to learning which is supervised & usually shows little correct results for particular amount of data as input. But there is large quantity of data is available which is non annotated in nature i.e. whole contents available on world wide web and it normally produces less accurate results. This paper discusses about a survey of different techniques of natural language processing.\"}, {\"paperId\": \"abb655be5ad564bbdc729bc5fa652f4c47679219\", \"abstract\": \"The Association for the Advancement of Artificial Intelligence, in cooperation with Stanford University\\u2019s Department of Computer Science, presented the 2018 Spring Symposium Series, held Monday through Wednesday, March 26\\u201328, 2018, on the campus of Stanford University. The seven symposia held were AI and Society: Ethics, Safety and Trustworthiness in Intelligent Agents; Artificial Intelligence for the Internet of Everything; Beyond Machine Intelligence: Understanding Cognitive Bias and Humanity for Well-Being AI; Data Efficient Reinforcement Learning; The Design of the User Experience for Artificial Intelligence (the UX of AI); Integrated Representation, Reasoning, and Learning in Robotics; Learning, Inference, and Control of Multi-Agent Systems. This report, compiled from organizers of the symposia, summarizes the research of five of the symposia that took place.\"}, {\"paperId\": \"aafed333f0192f1e87c22d7524a1a782eda505cf\", \"abstract\": \"The latest trends in Information Technology represent a new intellectual paradigm for scientific exploration and visualization of scientific phenomena. The present treatise covers almost all the emerging technologies in the field. Academicians, engineers, industralists, scientists and researchers engaged in teaching, research and development of Computer Science and Information Technology will find the book useful for their future academic and research work. The present treatise comprising 225 articles broadly covers the following topics exhaustively. 01. Advance Networking and Security/Wireless Networking/Cyber Laws 02. Advance Software Computing 03. Artificial Intelligence/Natural Language Processing/ Neural Networks 04. Bioinformatics/Biometrics 05. Data Mining/E-Commerce/E-Learning 06. Image Processing, Content Based Image Retrieval, Medical and Bio-Medical Imaging, Wavelets 07. Information Processing/Audio and Text Processing/Cryptology, Steganography and Digital Watermarking 08. Pattern Recognition/Machine Vision/Image Motion, Video Processing 09. Signal Processing and Communication/Remote Sensing 10. Speech Processing & Recognition, Human Computer Interaction 11. Information and Communication Technology\"}, {\"paperId\": \"8922ac88fd26f66a15b66fbad18b9da9ad3a67bb\", \"abstract\": \"A recent report developed by the National Research Council (NRC) for the Defense Modeling and Simulation Office (DMSO) encourages the use of real world, war-gaming, and laboratory data in support of the development and validation of human behavioral models for military simulations. Also encouraged in this report is the use of interdisciplinary teams embracing the disciplines of the psychological, computer, and military sciences to create such models. This paper describes the use of an artificial intelligence modeling framework, observational learning, to support these objectives. This framework combines the research methods of experimental psychology with the machine learning methods of computer science to develop behavioral models from data generated by military experts participating in live and/or simulated exercises. To date, research has demonstrated that behavioral models developed through this framework can be integrated into popular Semi-Automated Force (SAF) systems to enhance their performance. However, there has been no known investigation as to what the benefits of this approach are with respect to behavioral model fidelity. This paper introduces the interdisciplinary nature of observational learning by briefly surveying its history with respect to computer science and psychology and by illustrating how it can be used in conjunction with military experts. Next, this paper examines experimental evidence to determine whether a significant difference exists between SAF performance and human performance for a low-level, skill task. Finally, this paper demonstrates how behavioral models developed through human performance data generated by military SMEs can be used in conventional SAF systems to make SAF performance more \\\"human-like\\\".\"}, {\"paperId\": \"bc4cecc6d3c7681bd303540f987b5b0e0601ff10\", \"abstract\": \"Tom M. Mitchell is author of the textbook \\\"Machine Learning\\\" (McGraw Hill, 1997), President of the American Association for Artificial Intelligence and a member of the National Research Council's Computer Science and Telecommunications Board. He is Vice President and Chief Scientist at WhizBang Labs and is currently on a two-year leave of absence from Carnegie Mellon University where he is the Fredkin Professor of Learning and AI in the School of Computer Science and founding Director of CMU's Center for Automated Learning and Discovery. Mitchell's research interests span many areas of Machine Learning theory and practice. His current work at WhizBang Labs involves developing machine learning methods for extracting information from text. For example, WhizBang has developed the world's largest database of job openings by training its software to automatically locate and extract detailed information from job postings on corporate web sites (see www.flipdog.com).\"}, {\"paperId\": \"6acc8d4eec98603888ffd9d0c5653e6d0de9707b\", \"abstract\": \"Entity resolution, the process of determining if two or more references correspond to the same entity, is an emerging area of study in computer science. While entity resolution models leverage artificial intelligence, machine learning, and data mining techniques, relationships between various models remain ill-specified. Despite growth in both research and literature, investigations are scattered across communities with minimal communication. This paper introduces a conceptual framework, called ENRES, for explicit and formal entity resolution model definition. Through ENRES, we illustrate how several models solve related, though distinctly different, variants of entity resolution. In addition, we prove the existence of entity resolution challenges yet to be addressed by past or current research.\"}, {\"paperId\": \"476e34089adab5a7b25feffc94cbd79ce8ad1c81\", \"abstract\": \"Now a days huge amount of data is gathered and accumulated very fast in large databases. However, there is a growing gap between the generation of data and people understanding of it. The science of extracting useful information from large data sets or databases is known as Data Mining, a new discipline lying at the intersection of several areas, among them, Artificial Intelligence and Machine Learning (ML). In this work we concentrate on the use of symbolic supervised ML algorithms as part of the Data Mining process. The term symbolic indicates that the classifiers induced by these ML algorithms should be human readable, such as if-then rules, and comprehensible. An important issue is related to the quality and interestingness of the rules induced by these algorithms. But, especially in Data Mining, the amount of knowledge-rules induced by ML algorithms may be very large, making difficult the selection of good rules by humans. In this work we present and discuss several rule measures that can provide support for ranking and selecting good rules. We also present the RuleSystem , a prototype computational system that implements several functions concerned with the use of Machine Learning in the Data Miming process. The RuleSystemuses as input a common file format for data sets, features description and if-then knowledge rules. One of the functions in the RuleSystem , implemented through the Rule Analysis Module, is related to rule measures. The Rule Analysis Module proposed in this work implements several rule evaluation and interestingness measures. Through this module the RuleSystem is able to do a qualitative as well as a quantitative analysis of the set of rules induced by one or more symbolic Machine Learning algorithms. Furthermore, as the system has been implemented in Prolog it is query-centered, allowing the user to specify any constraints on the result of a query in order to narrow the search for good rules.\"}, {\"paperId\": \"3b750e38452b3a6a555e3534178984973e69ddf9\", \"abstract\": \"Artificial intelligence, especially machine learning (ML) and deep learning (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional methods. Addressing this challenge, ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model. Recent breakthroughs in ML techniques have created vast opportunities for not only overcoming long-standing mechanics problems but also for developing unprecedented materials design strategies. In this review, we first present a brief introduction of state-of-the-art ML models, algorithms and structures. Then, we discuss the importance of data collection, generation and preprocessing. The applications in mechanical property prediction, materials design and computational methods using ML-based approaches are summarized, followed by perspectives on opportunities and open challenges in this emerging and exciting field.\"}, {\"paperId\": \"8a38f8fef4e5772c1310d974c8f9c238f2b05d63\", \"abstract\": \"Machine learning and artificial intelligence approaches have revolutionized multiple disciplines, including toxicology. This review summarizes representative recent applications of machine learning and artificial intelligence approaches in different areas of toxicology, including physiologically based pharmacokinetic (PBPK) modeling, quantitative structure-activity relationship modeling for toxicity prediction, adverse outcome pathway analysis, high-throughput screening, toxicogenomics, big data and toxicological databases. By leveraging machine learning and artificial intelligence approaches, now it is possible to develop PBPK models for hundreds of chemicals efficiently, to create in silico models to predict toxicity for a large number of chemicals with similar accuracies compared to in vivo animal experiments, and to analyze a large amount of different types of data (toxicogenomics, high-content image data, etc.) to generate new insights into toxicity mechanisms rapidly, which was impossible by manual approaches in the past. To continue advancing the field of toxicological sciences, several challenges should be considered: (1) not all machine learning models are equally useful for a particular type of toxicology data, and thus it is important to test different methods to determine the optimal approach; (2) current toxicity prediction is mainly on bioactivity classification (yes/no), so additional studies are needed to predict the intensity of effect or dose-response relationship; (3) as more data become available, it is crucial to perform rigorous data quality check and develop infrastructure to store, share, analyze, evaluate, and manage big data; and (4) it is important to convert machine learning models to user-friendly interfaces to facilitate their applications by both computational and bench scientists.\"}, {\"paperId\": \"5bb7daf0c83bb781cb755dc0212fa56d98c3b589\", \"abstract\": \"Increasingly, groups external to educational systems are offering time, expertise and products, creating an intricate web of educational governance where entities outside of formal education contribute to state-funded education systems. While this involvement and its motivations have been considered in the literature, it has been less common to explore these interactions between school systems and outside organizations as they relate to the transition from the knowledge economy to the intelligent economy. Such research is important to understand the numerous inputs to education, which can then inform future decision-making. This study traces scripts around the commodification of knowledge, which connects education to individual employability or the economy and cyborg dialectic, or the mutual relationship between humans and technology. These scripts intersect to contribute to the perpetuation of data creation and usage as part of the educational intelligent economy. The scripts traced here originate from Battelle, a primarily a Ohio-based research and development organization, also focused on classroom teaching and learning, The Educational Intelligent Economy: Big Data, Artificial Intelligence, Machine Learning and the Internet of Things in Education International Perspectives on Education and Society, Volume 38, 161\\u2013177 Copyright \\u00a9 2020 by Emerald Publishing Limited All rights of reproduction in any form reserved ISSN: 1479-3679/doi:10.1108/S1479-367920190000038010 162 PETRINA M. DAVIDSON ET AL. specifically in STEM (Science, Technology, Engineering and Mathematics) education. Mapping scripts related to the commodification of knowledge and the cyborg dialectic indicates promotion of the intelligent economy broadly and individually for Battelle itself across Ohio and beyond, through investments in educators, students and policy-makers but also Battelle\\u2019s potential employees and collaborators. This data-focus creates an educational intelligence not only in students, teachers and policy-makers but also in Battelle itself, legitimating it as an actor in education.\"}, {\"paperId\": \"6578b54d1b7475805d0f8557a149908dfa1fb602\", \"abstract\": \"Artificial intelligence and machine learning techniques have proved fertile methods for attacking difficult problems in medicine and public health. These techniques have garnered strong interest for the analysis of the large, multi-domain open science datasets that are increasingly available in health research. Discovery science in large datasets is challenging given the unconstrained nature of the learning environment where there may be a large number of potential predictors and appropriate ranges for model hyperparameters are unknown. As well, it is likely that explainability is at a premium in order to engage in future hypothesis generation or analysis. Here, we present a novel method that addresses these challenges by exploiting evolutionary algorithms to optimize machine learning discovery science while exploring a large solution space and minimizing bias. We demonstrate that our approach, called integrated evolutionary learning (IEL), provides an automated, adaptive method for jointly learning features and hyperparameters while furnishing explainable models where the original features used to make predictions may be obtained even with artificial neural networks. In IEL the machine learning algorithm of choice is nested inside an evolutionary algorithm which selects features and hyperparameters over generations on the basis of an information function to converge on an optimal solution. We apply IEL to three gold standard machine learning algorithms in challenging, heterogenous biobehavioral data: deep learning with artificial neural networks, decision tree-based techniques and baseline linear models. Using our novel IEL approach, artificial neural networks achieved \\u2265 95% accuracy, sensitivity and specificity and 45\\u201373% R2 in classification and substantial gains over default settings. IEL may be applied to a wide range of less- or unconstrained discovery science problems where the practitioner wishes to jointly learn features and hyperparameters in an adaptive, principled manner within the same algorithmic process. This approach offers significant flexibility, enlarges the solution space and mitigates bias that may arise from manual or semi-manual hyperparameter tuning and feature selection and presents the opportunity to select the inner machine learning algorithm based on the results of optimized learning for the problem at hand.\"}, {\"paperId\": \"4a5a8864a162b9615cdfdfcce025c8483d9de21d\", \"abstract\": \"With the National Synchrotron Light Source II (NSLS-II) coming online in 2015 as the brightest source in the world, the imminent up-grades at the Advanced Photon Source, Advanced Light Source, and Linear Coherent Light Source, and advances in detector technology, the data generation rates at the U.S. Department of Energy (DOE) Basic Energy Sciences\\u2019 X-ray light sources are skyrocketing. At NSLS-II, over 1 petabyte of raw data was produced last year, and that rate is expected to increase as the facility matures [1]. Despite such huge data generation rates, approaches to both experimental control and data analysis have not kept pace. Consequently, data collected in seconds to minutes may take weeks to months of analysis to understand. Due to such limita-tions, knowledge extraction is often divorced from the measurement process. The lack of real-time feedback forces users into flying blind at the beamline, leading to missed opportunities, mistakes, and inefficient use of beamtime as a resource\\u2014as all beamlines are oversubscribed. This is a challenge facing nearly all users of light sources. One promising path forward to solve this challenge\\u2014both during data collection and post-experiment analysis\\u2014is the use of artificial intelligence (AI) and machine learning (ML) methods [1, 2]. In this contribution, we review recent developments employing AI/ML methods at the NSLS-II, tackling the\"}, {\"paperId\": \"e2a01f5318b1313778726c2417b44816515ce64f\", \"abstract\": \"\\nPurpose\\nThe purpose of this study is to investigate how artificial intelligence (AI), as well as machine learning (ML) techniques, are being applied and implemented within supply chains (SC) and to develop future research directions from thereof.\\n\\n\\nDesign/methodology/approach\\nUsing a systematic literature review methodology, this study analyzes the publications available on Web of Science, Scopus and Google Scholar that linked both AI and supply chain from one side and ML and supply chain from another side. A total of 388 research studies have been identified through the before said three database searches which are further screened, sorted and finalized with 50 studies. The research thoroughly reviews and analyzes the final lists of 50 studies that were found relevant and significant to the theme of AI and ML in supply chain management (SCM).\\n\\n\\nFindings\\nAI and ML applications are still at the infant stage and the opportunity for them to elevate supply chain performance is very promising. Some researchers developed AI and ML-related models which were tested and proved to be effective in optimizing SC, and therefore, the application of AI and ML in supply chain networks creates competitive advantages for firms. Other researchers claim that AI and ML are both currently adding value while many other researchers believe that they are still not fully exploited and their tools and techniques can leverage the supply chain\\u2019s total value. The research found that adoption of AI and ML have the ability to reduce the bullwhip effect, and therefore, further supports the performance of supply chain efficiency and responsiveness.\\n\\n\\nResearch limitations/implications\\nThis research was limited in terms of scope as it covered AI and ML applications in the supply chain while there are other dimensions that could be investigated such as big data and robotics but it was found too lengthy to include these additional dimensions, and therefore, left for future research studies that other researchers could explore and pursue.\\n\\n\\nPractical implications\\nThis study opens the door wide for other researchers to explore how AI and ML can be adopted in SCM and what are the models that are already tested and proven to be viable. In addition, the paper also identified a group of research studies that confirmed the unexploited avenues of AI and ML which could be of high interest to other researchers to explore.\\n\\n\\nOriginality/value\\nAlthough few earlier research studies touch based on the AI applications within manufacturing and transportation, this study is different and makes a unique contribution by offering a holistic view on the AI and ML implications within SC as a whole. The research carefully reviews a number of highly cited papers classifying them into three main themes and recommends future direction.\\n\"}, {\"paperId\": \"35d1523f32f1439494f5425a8a7b5d625826d2ce\", \"abstract\": \"Ion channels are linked to important cellular processes. For more than half a century, we have been learning various structural and functional aspects of ion channels using biological, physiological, biochemical, and biophysical principles and techniques. In recent days, bioinformaticians and biophysicists having the necessary expertise and interests in computer science techniques including versatile algorithms have started covering a multitude of physiological aspects including especially evolution, mutations, and genomics of functional channels and channel subunits. In these focused research areas, the use of artificial intelligence (AI), machine learning (ML), and deep learning (DL) algorithms and associated models have been found very popular. With the help of available articles and information, this review provide an introduction to this novel research trend. Ion channel understanding is usually made considering the structural and functional perspectives, gating mechanisms, transport properties, channel protein mutations, etc. Focused research on ion channels and related findings over many decades accumulated huge data which may be utilized in a specialized scientific manner to fast conclude pinpointed aspects of channels. AI, ML, and DL techniques and models may appear as helping tools. This review aims at explaining the ways we may use the bioinformatics techniques and thus draw a few lines across the avenue to let the ion channel features appear clearer.\"}, {\"paperId\": \"24edc97faf558e3479f646b04a522d3cd4e22a8b\", \"abstract\": \"Artificial intelligence AI or machine learning has proven to be a potential activity in the health and biomedical sciences. Previous research it has found that AI can learn new data and transform it into the useful knowledge. In the field of pharmacology, the aim is to design more efficient and novel vaccines using this method which are also cost effective. The underlying fact is to predict the molecular mechanism and structure for increased likelihood of developing new drugs. Clinical, electronic and high-resolution imaging datasets can be used as inputs to aid the drug development niche. Moreover, the use of comprehensive target activity has been performed for repurposing a drug molecule by extending target profiles of drugs which also include off targets with therapeutic potential providing a new indication.\"}, {\"paperId\": \"746867b3419eaf45ebcd86f6cd7a22e7442eb972\", \"abstract\": null}, {\"paperId\": \"fb82234f79e732665c042ca5aa3b18096749ec3c\", \"abstract\": \"Artificial intelligence (AI)-based applications have found widespread applications in many fields of science, technology, and medicine. The use of enhanced computing power of machines in clinical medicine and diagnostics has been under exploration since the 1960s. More recently, with the advent of advances in computing, algorithms enabling machine learning, especially deep learning networks that mimic the human brain in function, there has been renewed interest to use them in clinical medicine. In cardiovascular medicine, AI-based systems have found new applications in cardiovascular imaging, cardiovascular risk prediction, and newer drug targets. This article aims to describe different AI applications including machine learning and deep learning and their applications in cardiovascular medicine. AI-based applications have enhanced our understanding of different phenotypes of heart failure and congenital heart disease. These applications have led to newer treatment strategies for different types of cardiovascular diseases, newer approach to cardiovascular drug therapy and postmarketing survey of prescription drugs. However, there are several challenges in the clinical use of AI-based applications and interpretation of the results including data privacy, poorly selected/outdated data, selection bias, and unintentional continuance of historical biases/stereotypes in the data which can lead to erroneous conclusions. Still, AI is a transformative technology and has immense potential in health care.\"}, {\"paperId\": \"354cf4eade232f93ce445cf010e7bc77ced7da61\", \"abstract\": null}, {\"paperId\": \"9b622cb37b2d7e7a5de12cdfade79066b61baea2\", \"abstract\": null}, {\"paperId\": \"88efbf84d0f6ac1c1ee09fc054c6192c9cbce441\", \"abstract\": null}, {\"paperId\": \"4d93d1875faabc18c0fc05b75d9fc477a95b5ca1\", \"abstract\": \"Internet of Things (IoT) has become one of the mainstream advancements and a supreme domain of research for the technical as well as the scientific world, and financially appealing for the business world. It supports the interconnection of different gadgets and the connection of gadgets to people. IoT requires a distributed computing set up to deal with the rigorous data processing and training; and simultaneously, it requires artificial intelligence (AI) and machine learning (ML) to analyze the information stored on various cloud frameworks and make extremely quick and smart decisions w.r.t to data. Moreover, the continuous developments in these three areas of IT present a strong opportunity to collect real-time data about every activity of a business. Artificial Intelligence (AI) and Machine Learning are assuming a supportive part in applications and use cases offered by the Internet of Things, a shift evident in the behavior of enterprises trying to adopt this paradigm shift around the world. Small as well as large-scale organizations across the globe are leveraging these applications to develop the latest offers of services and products that will present a new set of business opportunities and direct new developments in the technical landscape. The following transformation will also present another opportunity for various industries to run their operations and connect with their users through the power of AI, ML, and IoT combined. Moreover, there is still huge scope for those who can convert raw information into valuable business insights, and the way ahead to do as such lies in viable data analytics. Organizations are presently looking further into the data streams to identify new and inventive approaches to elevate proficiency and effectiveness in the technical as well as business landscape. Organizations are taking on bigger, more exhaustive research approaches with the assistance of continuous progress being made in science and technology, especially in machine learning and artificial intelligence. If companies want to understand the valuable capacity of this innovation, they are required to integrate their IoT frameworks with persuasive AI and ML algorithms that allow \\u2019smart devices/gadgets\\u2019 to imitate behavioral patterns of humans and be able to take wise decisions just like humans without much of an intervention. Integrating both artificial intelligence and machine learning with IoT networks is proving to be a challenging task for the accomplishment of the present IoT-based digital ecosystems. Hence, organizations should direct the necessary course of action to identify how they will drive value from intersecting AI, ML, and IoT to maintain a satisfactory position in the business in years to come. In this review, we will also discuss the progress of IoT so far and what role AI and ML can play in accomplishing new heights for businesses in the future. Later the paper will discuss the opportunities and challenges faced during the implementation of this hybrid model.\"}, {\"paperId\": \"b4837db484f1858709aa0ffc516b4dbd3fdd842d\", \"abstract\": null}, {\"paperId\": \"55ca3517f0349a65653c26402c9129cdce0a21f6\", \"abstract\": \"As the world population ages, it is estimated that the population worldwide above the age of 65 years old will increase from 420 million in 2000 to almost 1 billion by 2030. Dementia, with Alzheimer\\u2019s disease (AD) as the leading cause, is expected to rise in tandem. AD accounts for 60%\\u201380% of all dementia cases, with an estimated 5\\u20137 million new cases diagnosed each year. Despite intensive research, the diagnosis of AD is currently made through a combination of clinical assessment, neuroimaging and detection of biomarkers from positron emission tomography or cerebrospinal fluid examination, with patients facing issues including high costs, invasiveness of the procedures. Hence, alternative identification of AD without the use of costly or invasive tests remains a challenge that is difficult to surmount. To date, the healthcare has experienced a significant shift towards early accurate detection as well as early prevention. This importance is highlighted by the screening and surveillance of prevalent diseases such as diabetic retinopathy, breast cancer and dementia. While some of these programmes have been very successful in significantly reducing morbidity and mortality, significant amount of manpower, time and training is required for their successful execution. 10 This has lent greater weight to the adoption of healthcare technology in order to optimise the accuracy and efficiency of such programmes. Artificial intelligence (AI), through the combination of digitised big data and computational power, has emerged at the forefront of healthcare. It appears to be wellsuited to address the needs of the healthcare system: fast and accurate predictive, diagnostic and possibly therapeutic algorithms. Machine learning is able to process large amounts of digitised datasets beyond the limits of human capability, and analyse and convert these data into useful clinical insights for the physician. It is a natural fit for conditions or medical specialties that have a large reserve of labelled digitised datasets. At present, convoluted neural networks (CNN), designed to receive two or threedimensional shaped inputs, is one of the most commonly applied deep learning models in medical imaging analysis. Through the utilisation of CNN deep learning, several landmark studies extracting data from retinal images have shown a high degree of accuracy compared with human graders. 13 In order to achieve this, AI generally still requires large, welllabelled and highquality datasets. This places significant barriers to the development of successful image analysis models. The retina is one of the few select organs where image collection is easily accessible and abundant through the use of ocular imaging technologies. As the microvascular properties and neuronal structures of the retina, such as ganglion cellinner plexiform layer (GCIPL) and retinal nerve fibre layer (RNFL), 15 resemble the intracranial neuronal structure and vasculature, it provides a direct visualisation of potential intracranial changes. This combination of unique attributes can potentially provide a low cost, noninvasive analysis of the brain without the patient having to undergo costly neuroimaging to reach a diagnosis. This has attracted increasing research interest, especially in the field of AD where accurate early detection and diagnostic models still remain elusive. The pathogenesis associating retinal changes with AD is currently still unclear. While some evidence suggest the presence of amyloid beta plaques and tau neurofibrillary tangles in the retina of patients with AD, 20 the significance remains debatable. On the other hand, recent studies examining structural changes of the retina in patients with AD have indicated extensive changes including loss of macula volume, 23 thinning of the GCIPL 25 and RNFL thickness, and subfoveal choroidal thickness. In the peripheral retina, notable changes include increased drusen formation and reduced retinal vascularity. Optical coherence tomography angiogram (OCTA), which allows for noninvasive assessment of retinal vascularity, has also shown decreased vessel density, perfusion density and increased foveal avascular zone in patients with AD. 29 It is thus apparent that retinal changes, especially retinal thinning and reduction in vascularity, could potentially indicate the development of AD. In the article by Wisely et al the authors have trained an AI model with multiple imaging modalities using CNN deeplearning with the best performing models achieving an area under curve (AUC) between 0.830 and 0.841. A total of 284 eyes from 159 subjects, of which 36 were clinically diagnosed with AD by experienced neurologists, were analysed in this study. Patient data, optical coherence tomography (OCT) and OCTA quantitative data, ultrawide field retinal photography as well as retinal autofluorescence images were used in the training, validation and testing of the models. Used in isolation, the GCIPL thickness as an indicator of AD had the highest predictive value with an AUC of 0.809. When used in combination, the model that was trained with a combination of GCIPL, OCTA quantitative data and patient data had the highest AUC of 0.841. This paper represents the first attempt as well as a proof of concept at developing a CNN to detect AD using multimodal retinal images. The relatively high predictive ability of the different combination models not only serves as an important foundation for future deep learning models in predicting AD, but also proves the validity of this approach. By testing a varied combination of different imaging modalities available in an ophthalmic clinic, Wisely et al are able to determine the datasets that will provide the greatest yield in accurate prediction of AD. This could contribute to the future development of a robust screening or predictive platform that uses parameters that are costeffective and simple in acquisition. Future research and AI models are likely to expand on the use of retinal imaging and combine with clinical neurological Cataract and Comprehensive, Singapore National Eye Centre, Singapore Ophthalmology and Visual Sciences, The Chinese University of Hong Kong, Hong Kong, Hong Kong Neuroophthalmology Department, Singapore National Eye Centre, Singapore Vitreoretinal Department, Singapore National Eye Centre, Singapore\"}, {\"paperId\": \"ece0e27044eefcf061345c472c59687e458a550b\", \"abstract\": null}, {\"paperId\": \"12b41ba740b7786e8bbb688a3485c0fc7330327a\", \"abstract\": \"Artificial intelligence vs Machine learning | Microsoft AzureArtificial Intelligence/Machine Learning (AI/ML)-Based.:Jf AI (Artificial Intelligence) Machine Learning Project Artificial intelligence and machine learning: How to Artificial Intelligence: How to make Machine Learning Artificial Intelligence (AI) vs. Machine Learning Artificial Intelligence | SAP Intelligent TechnologiesDifference between Artificial intelligence and Machine Artificial Intelligence (AI) & Machine Learning Artificial Intelligence and Machine Learning Northrop Artificial Intelligence | Science News2021 Conference on Artificial Intelligence, Machine Artificial Intelligence and Machine Learning | Computer ARTIFICIAL INTELLIGENCE-2022 | 2nd International Congress Artificial Intelligence & Machine Learning | School of 18CSL76 Artificial Intelligence Machine Learning The Machine Learning and Artificial Intelligence Bundle Learn artificial intelligence, machine learning, Python What Is Artificial Intelligence | AccentureThe History of Artificial Intelligence Science in the NewsLecturer in Artificial Intelligence and Machine Learning B.Tech Artificial Intelligence and Machine Learning Course Artificial Intelligence vs. Machine LearningHow Artificial Intelligence And Machine Learning Are Difference Between Artificial Intelligence vs Machine Machine Learning Artificial Intelligence Questions and What Is The Difference Between Artificial Intelligence And The Difference Between AI, Machine Learning, and Deep BTech Artificial Intelligence & Machine Learning: Courses Artificial Intelligence and Machine Learning in Software Artificial intelligence | MIT Technology ReviewProfessional Certificate Program in Machine Learning Global Conversational Artificial Intelligence and Voice Cheat Sheets for AI, Neural Networks, Machine Learning Artificial Intelligence, Intelligent Systems, Machine Artificial Intelligence (AI) \\u2013 What it is and why it Artificial intelligence and machine learning in financial Artificial Intelligence & Machine Learning Company | ONPASSIVEMachine Learning and Artificial Intelligence | Department Data Science vs Machine Learning and Artificial IntelligenceDifference between Machine learning and Artificial\"}, {\"paperId\": \"b4626c692a40ed23ba80c0c83f22ad75740a73a6\", \"abstract\": \"Research background: With increasing evidence of cognitive technologies progressively integrating themselves at all levels of the manufacturing enterprises, there is an instrumental need for comprehending how cognitive manufacturing systems can provide increased value and precision in complex operational processes.\\nPurpose of the article: In this research, prior findings were cumulated proving that cognitive manufacturing integrates artificial intelligence-based decision-making algorithms, real-time big data analytics, sustainable industrial value creation, and digitized mass production.\\nMethods: Throughout April and June 2022, by employing Preferred Reporting Items for Systematic Reviews and Meta-analysis (PRISMA) guidelines, a quantitative literature review of ProQuest, Scopus, and the Web of Science databases was performed, with search terms including ?cognitive Industrial Internet of Things?, ?cognitive automation?, ?cognitive manufacturing systems?, ?cognitively-enhanced machine?, ?cognitive technology-driven automation?, ?cognitive computing technologies,? and ?cognitive technologies.? The Systematic Review Data Repository (SRDR) was leveraged, a software program for the collecting, processing, and analysis of data for our research. The quality of the selected scholarly sources was evaluated by harnessing the Mixed Method Appraisal Tool (MMAT). AMSTAR (Assessing the Methodological Quality of Systematic Reviews) deployed artificial intelligence and intelligent workflows, and Dedoose was used for mixed methods research. VOSviewer layout algorithms and Dimensions bibliometric mapping served as data visualization tools.\\nFindings & value added: Cognitive manufacturing systems is developed on sustainable product lifecycle management, Internet of Things-based real-time production logistics, and deep learning-assisted smart process planning, optimizing value creation capabilities and artificial intelligence-based decision-making algorithms. Subsequent interest should be oriented to how predictive maintenance can assist in cognitive manufacturing by use of artificial intelligence-based decision-making algorithms, real-time big data analytics, sustainable industrial value creation, and digitized mass production.\"}, {\"paperId\": \"6614f186ab0b392c5e9247b0708cd3f18c651905\", \"abstract\": \"Machine learning (ML) approaches have been increasingly adopted for computer-assisted drug discovery in the recent years. This rapid progress is mostly due to the large-scale data sets available for ML model training, along with the development of deep learning (DL) and other artificial intelligence (AI) approaches that enable an integrated use of heterogeneous compound and target data, ranging from chemical structure to biochemical, in vitro, in vivo, and clinical endpoints for treatment response modeling. Exciting opportunities to apply ML and AI methods occur in all stages of drug discovery and development, including target identification and validation, prediction of molecular structure and function, and compound screening and optimization [1]. Based on high-quality training data, supervised learning and feature selection offers a systematic means for identification of predictive biomarkers for precision medicine; for instance, using clinical data, such predictive approaches may provide real-word evidence of treatment responses for designing marker-based trials and improving patient selection. In particular, cancer research and precision oncology has witnessed in the past few years an enormous progress in the application of ML to cancer detection and diagnosis, identification of new targets for anticancer drug discovery and repurposing, along with prediction of treatment outcomes for cancer patients [2]. However, as was recently demonstrated in the fast pace of development and application of ML models for COVID-19 detection and prognostication, the eventual health-care impact of the new models may remain rather limited unless more attention will be paid to the rigorous and often lengthy process of model validation and potential biases in the training data sets. Strikingly, a recent report showed that none of the ML models developed using chest radiographs and CT scans so far are of potential clinical use for COVID-19 management, mainly due to methodological flaws and high or unclear risk of bias in the imaging datasets used for model training [3]. Another important application of ML in the fight against COVID-19 is drug screening and repurposing. Since developing new drugs is costly and time-consuming, several efforts have been directed toward exploring existing drugs for COVID-19 treatment. However, after the first year of the COVID-19 pandemic, drug repurposing had not produced new treatment solutions [4]. This does not imply the failure of drug repurposing concept per se, rather that the development of clinic-ready treatments takes time, even with new and accelerated approaches. Indeed, increasing number repurposed drugs continue to be considered in clinical trials, along with several novel candidates. To address these challenges, a number of recent community efforts have set guidelines for improving the quality and applicability of ML models, for instance, their benchmarking in well-curated external validation data, against established standards and quality metrics, implemented as community-wide recommendations for reporting ML-based studies [5], along with standards and best practices for code publication and workflow automation [6]. It is evident that a broad adoption of these recommendations by the ML developers and life science journals will improve future assessment and reproducibility of ML-based studies. Equally important to making the codes and data available for others is that the ML models will also be implemented as easy-to-use web-applications to promote their wider use in the community, also among researchers without programming skills or bioinformatics support, while ensuring that the modeling standards are maintained. This is especially important for the complex DL algorithms, as the lack of user-friendly solutions may hinder their use in practice by the drug discovery and repurposing experts. Another timely issue for which there are some research solutions available but that are not yet made widely available in drug discovery pipelines is how to effectively integrate multi-modal and multi-scale data for guiding the drug discovery process and for predicting drug responses both in preclinical models and in patients. A wide range of learning approaches have been developed for systematic identification of drug repurposing leads based on select big data resources, such as drug structure and target profiles combined with multi-omics data from cell-based models [7], but what is currently lacking are web-based platforms that integrate all these data into an easy-to-access summary for the drug discovery\"}, {\"paperId\": \"932801a34898a6349a37d51110bc28c06a1eab2b\", \"abstract\": \"The promise of highly personalized oncology care using artificial intelligence (AI) technologies has been forecasted since the emergence of the field. Cumulative advances across the science are bringing this promise to realization, including refinement of machine learning- and deep learning algorithms; expansion in the depth and variety of databases, including multiomics; and the decreased cost of massively parallelized computational power. Examples of successful clinical applications of AI can be found throughout the cancer continuum and in multidisciplinary practice, with computer vision-assisted image analysis in particular having several U.S. Food and Drug Administration-approved uses. Techniques with emerging clinical utility include whole blood multicancer detection from deep sequencing, virtual biopsies, natural language processing to infer health trajectories from medical notes, and advanced clinical decision support systems that combine genomics and clinomics. Substantial issues have delayed broad adoption, with data transparency and interpretability suffering from AI's \\\"black box\\\" mechanism, and intrinsic bias against underrepresented persons limiting the reproducibility of AI models and perpetuating health care disparities. Midfuture projections of AI maturation involve increasing a model's complexity by using multimodal data elements to better approximate an organic system. Far-future positing includes living databases that accumulate all aspects of a person's health into discrete data elements; this will fuel highly convoluted modeling that can tailor treatment selection, dose determination, surveillance modality and schedule, and more. The field of AI has had a historical dichotomy between its proponents and detractors. The successful development of recent applications, and continued investment in prospective validation that defines their impact on multilevel outcomes, has established a momentum of accelerated progress.\"}, {\"paperId\": \"eba7c0cecafefa8325aa7d8da70ef2a030d401eb\", \"abstract\": \"The exponential increase in published articles makes a thorough and expedient review of literature increasingly challenging. This review delineated automated tools and platforms that employ artificial intelligence (AI) approaches and evaluated the reported benefits and challenges in using such methods. A search was conducted in 4 databases (Medline, Embase, CDSR, and Epistemonikos) up to April 2021 for systematic reviews and other related reviews implementing AI methods. To be included, the review must use any form of AI method, including machine learning, deep learning, neural network, or any other applications used to enable the full or semi\\u2010autonomous performance of one or more stages in the development of evidence synthesis. Twelve reviews were included, using nine different tools to implement 15 different AI methods. Eleven methods were used in the screening stages of the review (73%). The rest were divided: two in data extraction (13%) and two in risk of bias assessment (13%). The ambiguous benefits of the data extractions, combined with the reported advantages from 10 reviews, indicating that AI platforms have taken hold with varying success in evidence synthesis. However, the results are qualified by the reliance on the self\\u2010reporting of the review authors. Extensive human validation still appears required at this stage in implementing AI methods, though further evaluation is required to define the overall contribution of such platforms in enhancing efficiency and quality in evidence synthesis.\"}, {\"paperId\": \"d6073a7d2ade4910e1ec34abd14371defa4ab78d\", \"abstract\": \"Artificial intelligence (AI) and machine learning have promised to revolutionize the way we live and work, and one of the particularly promising areas for AI is image analysis. Nevertheless, many current AI applications focus on the post-processing of data, while in both materials sciences and medicine, it is often critical to respond to the data acquired on the fly. Here we demonstrate an artificial intelligence atomic force microscope (AI-AFM) that is capable of not only pattern recognition and feature identification in ferroelectric materials and electrochemical systems, but can also respond to classification via adaptive experimentation with additional probing at critical domain walls and grain boundaries, all in real time on the fly without human interference. Key to our success is a highly efficient machine learning strategy based on a support vector machine (SVM) algorithm capable of high fidelity pixel-by-pixel recognition instead of relying on the data from full mapping, making real time classification and control possible during scanning, with which complex electromechanical couplings at the nanoscale in different material systems can be resolved by AI. For AFM experiments that are often tedious, elusive, and heavily rely on human insight for execution and analysis, this is a major disruption in methodology, and we believe that such a strategy empowered by machine learning is applicable to a wide range of instrumentations and broader physical machineries.\"}, {\"paperId\": \"9658aa0df919d0ad0cb9d7ad7f6648373d832ae4\", \"abstract\": \"The need for advanced automation and artificial intelligence (AI) in various fields, including text classification, has dramatically increased in the last decade, leaving us critically dependent on their performance and reliability. Yet, as we increasingly rely more on AI applications, their algorithms are becoming more nuanced, more complex, and less understandable precisely at a time we need to understand them better and trust them to perform as expected. Text classification in the medical and cybersecurity domains is a good example of a task where we may wish to keep the human in the loop. Human experts lack the capacity to deal with the high volume and velocity of data that needs to be classified, and ML techniques are often unexplainable and lack the ability to capture the required context needed to make the right decision and take action. We propose a new abstract configuration of Human-Machine Learning (HML) that focuses on reciprocal learning, where the human and the AI are collaborating partners. We employ design-science research (DSR) to learn and design an application of the HML configuration, which incorporates software to support combining human and artificial intelligences. We define the HML configuration by its conceptual components and their function. We then describe the development of a system called Fusion that supports human-machine reciprocal learning. Using two case studies of text classification from the cyber domain, we evaluate Fusion and the proposed HML approach, demonstrating benefits and challenges. Our results show a clear ability of domain experts to improve the ML classification performance over time, while both human and machine, collaboratively, develop their conceptualization, i.e., their knowledge of classification. We generalize our insights from the DSR process as actionable principles for researchers and designers of 'human in the learning loop' systems. We conclude the paper by discussing HML configurations and the challenge of capturing and representing knowledge gained jointly by human and machine, an area we feel has great potential.\"}, {\"paperId\": \"5cb7e8e15faa52c9c6c360bf9d6a17a4e34de467\", \"abstract\": \"Abstract Data mining involves the use of mathematical sciences, statistics, artificial intelligence, and machine learning to determine the relationships between variables from a large sample of data. It has previously been shown that data mining can improve the prediction and diagnostic precision of type 2 diabetes mellitus. A few studies have applied machine learning to assess hypertension and metabolic syndrome-related biomarkers, as well as refine the assessment of cardiovascular disease risk. Machine learning methods have also been applied to assess new biomarkers and survival outcomes in patients with renal diseases to predict the development of chronic kidney disease, disease progression, and renal graft survival. In the latter, random forest methods were found to be the best for the prediction of chronic kidney disease. Some studies have investigated the prognosis of nonalcoholic fatty liver disease and acute liver failure, as well as therapy response prediction in patients with viral disorders, using decision tree models. Machine learning techniques, such as Sparse High-Order Interaction Model with Rejection Option, have been used for diagnosing Alzheimer's disease. Data mining techniques have also been applied to identify the risk factors for serious mental illness, such as depression and dementia, and help to diagnose and predict the quality of life of such patients. In relation to child health, some studies have determined the best algorithms for predicting obesity and malnutrition. Machine learning has determined the important risk factors for preterm birth and low birth weight. Published studies of patients with cancer and bacterial diseases are limited and should perhaps be addressed more comprehensively in future studies. Herein, we provide an in-depth review of studies in which biochemical biomarker data were analyzed using machine learning methods to assess the risk of several common diseases, in order to summarize the potential applications of data mining methods in clinical diagnosis. Data mining techniques have now been increasingly applied to clinical diagnostics, and they have the potential to support this field.\"}, {\"paperId\": \"be739789c1bc89e28a9b6a7a867e254784de8dda\", \"abstract\": null}, {\"paperId\": \"855987cb820e80f5b31b1edefd628d1930ee9ddf\", \"abstract\": \"Based on current trends in artificial intelligence (AI) and machine learning (ML), we provide an overview of novel algorithms intended to address Army-specific needs for increased operational tempo and autonomy for ground robots in unexplored, dynamic, cluttered, contested, and sparse data environments. This paper discusses some of the motivating factors behind US Army Research in AI and ML and provides a survey of a subset of the US Army Research Laboratory\\u2019s (ARL) Computational and Information Sciences Directorate\\u2019s (CISD) recent research in online, nonparametric learning that quickly adapts to variable underlying distributions in sparse exemplar environments, as well as a technique for unsupervised semantic scene labeling that continuously learns and adapts semantic models discovered within a data stream. We also look at a newly developed algorithm that leverages human input to help intelligent agents learn more rapidly and a novel research study working to discover foundational knowledge that is required for humans and robots to communicate via natural language. Finally we discuss a method for finding chains of reasoning with incomplete information using semantic vectors. The specific research exemplars provide approaches for overcoming the specific shortcomings of commercial AI and ML methods as well as the brittleness of current commercial techniques such that these methods can be enhanced and adapted so as to be applicable to army relevant scenarios.\"}, {\"paperId\": \"eb7078a1e21ab05a8ee5677b3a26a6830d4f6733\", \"abstract\": \"Heart failure is the most common cause of death in both males and females around the world. Cardiovascular diseases (CVDs), in particular, are the main cause of death worldwide, accounting for 30% of all fatalities in the United States and 45% in Europe. Artificial intelligence (AI) approaches such as machine learning (ML) and deep learning (DL) models are playing an important role in the advancement of heart failure therapy. The main objective of this study was to perform a network meta-analysis of patients with heart failure, stroke, hypertension, and diabetes by comparing the ML and DL models. A comprehensive search of five electronic databases was performed using ScienceDirect, EMBASE, PubMed, Web of Science, and IEEE Xplore. The search strategy was performed according to the Preferred Reporting Items for Systematic Reviews and Meta-Analysis (PRISMA) statement. The methodological quality of studies was assessed by following the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2) guidelines. The random-effects network meta-analysis forest plot with categorical data was used, as were subgroups testing for all four types of treatments and calculating odds ratio (OR) with a 95% confidence interval (CI). Pooled network forest, funnel plots, and the league table, which show the best algorithms for each outcome, were analyzed. Seventeen studies, with a total of 285,213 patients with CVDs, were included in the network meta-analysis. The statistical evidence indicated that the DL algorithms performed well in the prediction of heart failure with AUC of 0.843 and CI [0.840\\u20130.845], while in the ML algorithm, the gradient boosting machine (GBM) achieved an average accuracy of 91.10% in predicting heart failure. An artificial neural network (ANN) performed well in the prediction of diabetes with an OR and CI of 0.0905 [0.0489; 0.1673]. Support vector machine (SVM) performed better for the prediction of stroke with OR and CI of 25.0801 [11.4824; 54.7803]. Random forest (RF) results performed well in the prediction of hypertension with OR and CI of 10.8527 [4.7434; 24.8305]. The findings of this work suggest that the DL models can effectively advance the prediction of and knowledge about heart failure, but there is a lack of literature regarding DL methods in the field of CVDs. As a result, more DL models should be applied in this field. To confirm our findings, more meta-analysis (e.g., Bayesian network) and thorough research with a larger number of patients are encouraged.\"}, {\"paperId\": \"979468ea28b6aade5f9e1aa2a980c2fd08bef72d\", \"abstract\": \"The global soil carbon pool has been estimated to exceed the amount of carbon stored in the atmosphere and vegetation, though uncertainties to quantify below-ground carbon and soil carbon fluxes accurately still exist. Modeling soil carbon using artificial intelligence (AI) - machine learning (ML) and deep learning (DL) algorithms - has emerged as a powerful force in the carbon science community. These AI soil carbon models have shown improved performance to predict soil organic carbon (SOC) storage, soil respiration (Rs), and other properties of the global carbon cycle when compared to other modeling approaches. AI systems have advanced abilities to optimize fits between inputs (geospatial environmental covariates) and outputs (e.g., SOC or Rs) through advanced pattern recognition, learning algorithms, latent variables, hyperparameters, hyperplanes, weighting factors, or multiple stacked processing (e.g., convolution and pooling). These machine-oriented applications have shifted focus from knowledge discovery and understanding of ecosystem processes, carbon pools and cycling toward data-driven applications that compute digital soil carbon outputs. The purpose of this review paper is to explore the emergence, applications, and progress of AI-ML and AI-DL algorithms to model soil carbon storage and Rs at regional and global scales. A critical discussion of the power, potentials, and perils of AI soil carbon modeling is provided. The paradigm shift toward AI modeling raises questions how we study soil carbon dynamics and what conclusions we draw which impacts carbon science research and education, carbon management, carbon policies, carbon markets and economies, and soil health.\"}, {\"paperId\": \"275b695c764f697c5311e8c9ba8aafab60258f0f\", \"abstract\": \"The unparalleled success of artificial intelligence (AI) in the technology sector has catalyzed an enormous amount of research in the scientific community. It has proven to be a powerful tool, but as with any rapidly developing field, the deluge of information can be overwhelming, confusing, and sometimes misleading. This can make it easy to become lost in the same hype cycles that have historically ended in the periods of scarce funding and depleted expectations known as AI winters. Furthermore, although the importance of innovative, high-risk research cannot be overstated, it is also imperative to understand the fundamental limits of available techniques, especially in young fields where the rules appear to be constantly rewritten and as the likelihood of application to high-stakes scenarios increases. In this article, we highlight the guiding principles of data-driven modeling, how these principles imbue models with almost magical predictive power, and how they also impose limitations on the scope of problems they can address. Particularly, understanding when not to use data-driven techniques, such as machine learning, is not something commonly explored, but is just as important as knowing how to apply the techniques properly. We hope that the discussion to follow provides researchers throughout the sciences with a better understanding of when said techniques are appropriate, the pitfalls to watch for, and most importantly, the confidence to leverage the power they can provide.\"}, {\"paperId\": \"ab624f69f69aebee35a003f046ee3f0c41fe503c\", \"abstract\": \"Artificial intelligence (AI) is transforming many domains, including finance, agriculture, defense, and biomedicine. In this paper, we focus on the role of AI in clinical and translational research (CTR), including preclinical research (T1), clinical research (T2), clinical implementation (T3), and public (or population) health (T4). Given the rapid evolution of AI in CTR, we present three complementary perspectives: (1) scoping literature review, (2) survey, and (3) analysis of federally funded projects. For each CTR phase, we addressed challenges, successes, failures, and opportunities for AI. We surveyed Clinical and Translational Science Award (CTSA) hubs regarding AI projects at their institutions. Nineteen of 63 CTSA hubs (30%) responded to the survey. The most common funding source (48.5%) was the federal government. The most common translational phase was T2 (clinical research, 40.2%). Clinicians were the intended users in 44.6% of projects and researchers in 32.3% of projects. The most common computational approaches were supervised machine learning (38.6%) and deep learning (34.2%). The number of projects steadily increased from 2012 to 2020. Finally, we analyzed 2604 AI projects at CTSA hubs using the National Institutes of Health Research Portfolio Online Reporting Tools (RePORTER) database for 2011\\u20132019. We mapped available abstracts to medical subject headings and found that nervous system (16.3%) and mental disorders (16.2) were the most common topics addressed. From a computational perspective, big data (32.3%) and deep learning (30.0%) were most common. This work represents a snapshot in time of the role of AI in the CTSA program.\"}, {\"paperId\": \"9b41d1ff5c2cae23264da43e6e34d53ac8e1270e\", \"abstract\": \"Machine learning and other artificial intelligence methods are gaining increasing prominence in chemistry and materials sciences, especially for materials design and discovery, and in data analysis of results generated by sensors and biosensors. In this paper, we present a perspective on this current use of machine learning, and discuss the prospects of the future impact of extending the use of machine learning to encompass knowledge discovery as an essential step towards a new paradigm of machine-generated knowledge. The reasons why results so far have been limited are given with a discussion of the limitations of machine learning in tasks requiring interpretation. Also discussed is the need to adapt the training of students and scientists in chemistry and materials sciences, to better explore the potential of artificial intelligence capabilities.\"}, {\"paperId\": \"4c489e49b8e99b885056e38a3450606fe1112f1f\", \"abstract\": null}, {\"paperId\": \"1552ddd24fb2f89e57dfa475c5c42788b5bb594e\", \"abstract\": \"Nuclear materials are often demanded to function for extended time in extreme environments, including high radiation fluxes with associated transmutations, high temperature and temperature gradients, mechanical stresses, and corrosive coolants. They also have a wide range of microstructural and chemical makeups, resulting in multifaceted and often out-of-equilibrium interactions. Machine learning (ML) is increasingly being used to tackle these complex time-dependent interactions and aid researchers in developing models and making pre- dictions, sometimes with better accuracy than traditional modeling that focuses on one or two parameters at a time. Conventional practices of acquiring new experimental data in nuclear materials research are often slow and expensive, limiting the opportunity for data-centric ML, but new methods are changing that paradigm. Here we review high-throughput computational and experimental data approaches, especially robotic experimentation and active learning that is based on Gaussian process and Bayesian optimization. We show ML examples in structural materials (e.g., reactor pressure vessel (RPV) alloys and radiation detecting scintillating materials) and highlight new techniques of high-throughput sample preparation and characterizations, and automated radia-tion/environmental exposures and real-time online diagnostics. This review suggests that ML models of material constitutive relations in plasticity, damage, and even electronic and optical responses to radiation are likely to become powerful tools as they develop. Finally, we speculate on how the recent trends of using natural language processing (NLP) to aid the collection and analysis of literature data, interpretable artificial intelligence (AI), and the use of streamlined scripting, database, workflow management, and cloud computing platforms that will soon make the utilization of ML techniques as commonplace as the spreadsheet curve-fitting practices of today.\"}, {\"paperId\": \"d0efd261e12c316a0a4f50c2f673af04a04b6200\", \"abstract\": \"Brain is the controlling center of our body. With the advent of time, newer and newer brain diseases are being discovered. Thus, because of the variability of brain diseases, existing diagnosis or detection systems are becoming challenging and are still an open problem for research. Detection of brain diseases at an early stage can make a huge difference in attempting to cure them. In recent years, the use of artificial intelligence (AI) is surging through all spheres of science, and no doubt, it is revolutionizing the field of neurology. Application of AI in medical science has made brain disease prediction and detection more accurate and precise. In this study, we present a review on recent machine learning and deep learning approaches in detecting four brain diseases such as Alzheimer\\u2019s disease (AD), brain tumor, epilepsy, and Parkinson\\u2019s disease. 147 recent articles on four brain diseases are reviewed considering diverse machine learning and deep learning approaches, modalities, datasets etc. Twenty-two datasets are discussed which are used most frequently in the reviewed articles as a primary source of brain disease data. Moreover, a brief overview of different feature extraction techniques that are used in diagnosing brain diseases is provided. Finally, key findings from the reviewed articles are summarized and a number of major issues related to machine learning/deep learning-based brain disease diagnostic approaches are discussed. Through this study, we aim at finding the most accurate technique for detecting different brain diseases which can be employed for future betterment.\"}, {\"paperId\": \"3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf\", \"abstract\": \"\\n Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.\"}, {\"paperId\": \"9588deb74e3442743397231285951b82a1a54314\", \"abstract\": \"ABSTRACT \\n \\nIn the traditional management accounting information processing method, the method used to solve the problem is often fixed due to excessive assumptions. In order to improve its operating efficiency, combined with artificial intelligence information technology, this paper uses data mining algorithms to conduct data acquisition and rule exploration. Moreover, this paper uses statistics, machine learning and other techniques to analyze the correlation between attribute values and transform data into knowledge needed for decision-making. In addition, this paper combines machine learning algorithms to build an intelligent management accounting information system and realizes the close connection between corporate finance and business, which helps to form a closed-loop management between financial analysis, risk management, performance management, and decision-making. Finally, this paper designs experiments to verify the performance of the model. The research results show that the system constructed in this paper satisfies the intelligent demand of accounting information. \\nREFERENCES \\n \\n \\n[1] Jie Cai, Jiawei Luo, Shulin Wang, and Sheng Yang. Feature selection in machine learning: A new perspective. Neurocomputing, 300:70\\u201379, 2018. \\n[2] J. N. Goetz, A Brenning, H Petschko, and P. Leopold. Evaluating machine learning and statistical prediction techniques for landslide susceptibility modeling. Computers and Geosciences, 81:1\\u201311, 2015. \\n[3] Hamid Darabi, Bahram Choubin, Omid Rahmati, Ali Torabi Haghighi, Biswajeet Pradhan, and Bjorn Klove. Urban flood risk mapping using the GARP and QUEST models: A comparative study of machine learning techniques. Journal of Hydrology, 569:142\\u2013154, 2019. \\n[4] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. Machine learning in medicine. England Journal of Medicine, 380(14):1347\\u201358, apr 2019. \\n[5] Yang Xin, Lingshuang Kong, Zhi Liu, Yuling Chen, Yanmiao Li, Hongliang Zhu, Mingcheng Gao, Haixia Hou, and Chunhua Wang. Machine Learning and Deep Learning Methods for Cybersecurity. IEEE Access, 6:35365\\u201335381, 2018. \\n[6] Logan Ward, Ankit Agrawal, Alok Choudhary, and Christopher Wolverton. A general-purpose machinelearning framework for predicting properties of inorganic materials. npj Computational Materials, 2, 2016. \\n[7] Puyu Feng, Bin Wang, De Li Liu, Cathy Waters, and Qiang Yu. Incorporating machine learning with biophysical model can improve the evaluation of climate extremes impacts on wheat yield in south-eastern Australia. Agricultural and Forest Meteorology, 275:100\\u2013113, 2019. \\n[8] Konstantina Kourou, Themis P. Exarchos, Konstantinos P. Exarchos, Michalis V. Karamouzis, and DimitriosI. Fotiadis. Machine learning applications in cancer prognosis and prediction, 2015. \\n[9] Saleema Amershi, Maya Cakmak, W. Bradley Knox, and Todd Kulesza. Power to the people: The role of humans in interactive machine learning. AI Magazine, 35(4):105\\u2013120, 2014. \\n[10] V Rodriguez-Galiano, M. Sanchez-Castillo, M. Chica- Olmo, and M. Chica-Rivas. Machine learning predictive models for mineral prospectivity: An evaluation of neural networks, random forest, regression trees and support vector machines. Ore Geology Reviews, 71:804\\u2013818, 2015. \\n[11] ConnorW Coley, Regina Barzilay, Tommi S Jaakkola, William H Green, and Klavs F Jensen. Prediction of Organic Reaction Outcomes Using Machine Learning. ACS Central Science, 3(5):434\\u2013443, may 2017. \\n[12] Aritra Chowdhury, Elizabeth Kautz, Bulent Yener, and Daniel Lewis. Image driven machine learning methods for microstructure recognition. Computational MaterialsScience, 123:176\\u2013187, 2016.\"}, {\"paperId\": \"6b6809992dd36f8eea8f7b4bc87383db28edfc98\", \"abstract\": \"Transition-metal complexes are attractive targets for the design of catalysts and functional materials. The behavior of the metal-organic bond, while very tunable for achieving target properties, is challenging to predict and necessitates searching a wide and complex space to identify needles in haystacks for target applications. This review will focus on the techniques that make high-throughput search of transition-metal chemical space feasible for the discovery of complexes with desirable properties. The review will cover the development, promise, and limitations of \\\"traditional\\\" computational chemistry (i.e., force field, semiempirical, and density functional theory methods) as it pertains to data generation for inorganic molecular discovery. The review will also discuss the opportunities and limitations in leveraging experimental data sources. We will focus on how advances in statistical modeling, artificial intelligence, multiobjective optimization, and automation accelerate discovery of lead compounds and design rules. The overall objective of this review is to showcase how bringing together advances from diverse areas of computational chemistry and computer science have enabled the rapid uncovering of structure-property relationships in transition-metal chemistry. We aim to highlight how unique considerations in motifs of metal-organic bonding (e.g., variable spin and oxidation state, and bonding strength/nature) set them and their discovery apart from more commonly considered organic molecules. We will also highlight how uncertainty and relative data scarcity in transition-metal chemistry motivate specific developments in machine learning representations, model training, and in computational chemistry. Finally, we will conclude with an outlook of areas of opportunity for the accelerated discovery of transition-metal complexes.\"}, {\"paperId\": \"6948e00a1e92f3f636c51f950674a5d81d5a4c4d\", \"abstract\": \"Background The use of artificial intelligence has revolutionized every area of life such as business and trade, social and electronic media, education and learning, manufacturing industries, medicine and sciences, and every other sector. The new reforms and advanced technologies of artificial intelligence have enabled data analysts to transmute raw data generated by these sectors into meaningful insights for an effective decision-making process. Health care is one of the integral sectors where a large amount of data is generated daily, and making effective decisions based on these data is therefore a challenge. In this study, cases related to childbirth either by the traditional method of vaginal delivery or cesarean delivery were investigated. Cesarean delivery is performed to save both the mother and the fetus when complications related to vaginal birth arise. Objective The aim of this study was to develop reliable prediction models for a maternity care decision support system to predict the mode of delivery before childbirth. Methods This study was conducted in 2 parts for identifying the mode of childbirth: first, the existing data set was enriched and second, previous medical records about the mode of delivery were investigated using machine learning algorithms and by extracting meaningful insights from unseen cases. Several prediction models were trained to achieve this objective, such as decision tree, random forest, AdaBoostM1, bagging, and k-nearest neighbor, based on original and enriched data sets. Results The prediction models based on enriched data performed well in terms of accuracy, sensitivity, specificity, F-measure, and receiver operating characteristic curves in the outcomes. Specifically, the accuracy of k-nearest neighbor was 84.38%, that of bagging was 83.75%, that of random forest was 83.13%, that of decision tree was 81.25%, and that of AdaBoostM1 was 80.63%. Enrichment of the data set had a good impact on improving the accuracy of the prediction process, which supports maternity care practitioners in making decisions in critical cases. Conclusions Our study shows that enriching the data set improves the accuracy of the prediction process, thereby supporting maternity care practitioners in making informed decisions in critical cases. The enriched data set used in this study yields good results, but this data set can become even better if the records are increased with real clinical data.\"}, {\"paperId\": \"2326997ee1d85fcb31f51c4488cdfaa40639d131\", \"abstract\": \"Abstract Nanophotonics inverse design is a rapidly expanding research field whose goal is to focus users on defining complex, high-level optical functionalities while leveraging machines to search for the required material and geometry configurations in sub-wavelength structures. The journey of inverse design begins with traditional optimization tools such as topology optimization and heuristics methods, including simulated annealing, swarm optimization, and genetic algorithms. Recently, the blossoming of deep learning in various areas of data-driven science and engineering has begun to permeate nanophotonics inverse design intensely. This review discusses state-of-the-art optimizations methods, deep learning, and more recent hybrid techniques, analyzing the advantages, challenges, and perspectives of inverse design both as a science and an engineering.\"}, {\"paperId\": \"2ffb6b147794ad3f12160a000f6c106742205212\", \"abstract\": \"Cancer is defined as a large group of diseases that is associated with abnormal cell growth, uncontrollable cell division, and may tend to impinge on other tissues of the body by different mechanisms through metastasis. What makes cancer so important is that the cancer incidence rate is growing worldwide which can have major health, economic, and even social impacts on both patients and the governments. Thereby, the early cancer prognosis, diagnosis, and treatment can play a crucial role at the front line of combating cancer. The onset and progression of cancer can occur under the influence of complicated mechanisms and some alterations in the level of genome, proteome, transcriptome, metabolome etc. Consequently, the advent of omics science and its broad research branches (such as genomics, proteomics, transcriptomics, metabolomics, and so forth) as revolutionary biological approaches have opened new doors to the comprehensive perception of the cancer landscape. Due to the complexities of the formation and development of cancer, the study of mechanisms underlying cancer has gone beyond just one field of the omics arena. Therefore, making a connection between the resultant data from different branches of omics science and examining them in a multi-omics field can pave the way for facilitating the discovery of novel prognostic, diagnostic, and therapeutic approaches. As the volume and complexity of data from the omics studies in cancer are increasing dramatically, the use of leading-edge technologies such as machine learning can have a promising role in the assessments of cancer research resultant data. Machine learning is categorized as a subset of artificial intelligence which aims to data parsing, classification, and data pattern identification by applying statistical methods and algorithms. This acquired knowledge subsequently allows computers to learn and improve accurate predictions through experiences from data processing. In this context, the application of machine learning, as a novel computational technology offers new opportunities for achieving in-depth knowledge of cancer by analysis of resultant data from multi-omics studies. Therefore, it can be concluded that the use of artificial intelligence technologies such as machine learning can have revolutionary roles in the fight against cancer.\"}, {\"paperId\": \"5513f1f1407c17c25cf3868000b88195f1df3506\", \"abstract\": \"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subir\\u00f3n 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world \\u2013 and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (M\\u00e9haut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as \\u201chypothetical constructs\\u201d (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n\"}, {\"paperId\": \"6f5b87e38cfb4a47e3ac7988cca5612ccf0a2946\", \"abstract\": \"In recent years, technology has advanced to the fourth industrial revolution (Industry 4.0), where the Internet of things (IoTs), fog computing, computer security, and cyberattacks have evolved exponentially on a large scale. The rapid development of IoT devices and networks in various forms generate enormous amounts of data which in turn demand careful authentication and security. Artificial intelligence (AI) is considered one of the most promising methods for addressing cybersecurity threats and providing security. In this study, we present a systematic literature review (SLR) that categorize, map and survey the existing literature on AI methods used to detect cybersecurity attacks in the IoT environment. The scope of this SLR includes an in-depth investigation on most AI trending techniques in cybersecurity and state-of-art solutions. A systematic search was performed on various electronic databases (SCOPUS, Science Direct, IEEE Xplore, Web of Science, ACM, and MDPI). Out of the identified records, 80 studies published between 2016 and 2021 were selected, surveyed and carefully assessed. This review has explored deep learning (DL) and machine learning (ML) techniques used in IoT security, and their effectiveness in detecting attacks. However, several studies have proposed smart intrusion detection systems (IDS) with intelligent architectural frameworks using AI to overcome the existing security and privacy challenges. It is found that support vector machines (SVM) and random forest (RF) are among the most used methods, due to high accuracy detection another reason may be efficient memory. In addition, other methods also provide better performance such as extreme gradient boosting (XGBoost), neural networks (NN) and recurrent neural networks (RNN). This analysis also provides an insight into the AI roadmap to detect threats based on attack categories. Finally, we present recommendations for potential future investigations.\"}, {\"paperId\": \"a194704ff34ae96eb0cb9a57c4afbc5adc4c4e57\", \"abstract\": null}, {\"paperId\": \"8db6ebe1490f8a8749dae6ddb788cce93cb90f4b\", \"abstract\": \"Background The purpose of this scoping review was to explore the current applications of objective gait analysis using inertial measurement units, custom algorithms and artificial intelligence algorithms in detecting neurological and musculoskeletal gait altering pathologies from healthy gait patterns. Methods Literature searches were conducted of four electronic databases (Medline, PubMed, Embase and Web of Science) to identify studies that assessed the accuracy of these custom gait analysis models with inputs derived from wearable devices. Data was collected according to the preferred reporting items for systematic reviews and meta-analysis statement guidelines. Results A total of 23 eligible studies were identified for inclusion in the present review, including 10 custom algorithms articles and 13 artificial intelligence algorithms articles. Nine studies evaluated patients with Parkinson\\u2019s disease of varying severity and subtypes. Support vector machine was the commonest adopted artificial intelligence algorithm model, followed by random forest and neural networks. Overall classification accuracy was promising for articles that use artificial intelligence algorithms, with nine articles achieving more than 90% accuracy. Conclusions Current applications of artificial intelligence algorithms are reasonably effective discrimination between pathological and non-pathological gait. Of these, machine learning algorithms demonstrate the additional capacity to handle complicated data input, when compared to other custom algorithms. Notably, there has been increasing application of machine learning algorithms for conducting gait analysis. More studies are needed with unsupervised methods and in non-clinical settings to better reflect the community and home-based usage.\"}, {\"paperId\": \"5f9c44deffb3151ccdfc2eac1f99222fbb10abe9\", \"abstract\": null}, {\"paperId\": \"b9e52a08ce92db0f19f8433560aaf1e193ffbcfc\", \"abstract\": \"The exponential increase in our ability to harness multi-dimensional biological and clinical data from experimental to real-world settings has transformed pharmaceutical research and development in recent years, with increasing applications of artificial intelligence (AI) and machine learning (ML). Patient-centered iterative forward and reverse translation is at the heart of precision medicine discovery and development across the continuum from target validation to optimization of pharmacotherapy. Integration of advanced analytics into the practice of Translational Medicine is now a fundamental enabler to fully exploit information contained in diverse sources of big data sets such as \\u201comics\\u201d data, as illustrated by deep characterizations of the genome, transcriptome, proteome, metabolome, microbiome, and exposome. In this commentary, we provide an overview of ML applications in drug discovery and development, aligned with the three strategic pillars of Translational Medicine (target, patient, dose) and offer perspectives on their potential to transform the science and practice of the discipline. Opportunities for integrating ML approaches into the discipline of Pharmacometrics are discussed and will revolutionize the practice of model-informed drug discovery and development. Finally, we posit that joint efforts of Clinical Pharmacology, Bioinformatics, and Biomarker Technology experts are vital in cross-functional team settings to realize the promise of AI/ML-enabled Translational and Precision Medicine.\"}, {\"paperId\": \"fb78e130b45b54e555ad434cd79cfb86fab4ceb0\", \"abstract\": \"As in many areas of science and technology, artificial intelligence (AI) is currently promoted with high expectations in drug discovery and medicinal chemistry. Here, AI mostly refers to machine learning (ML), which is only a part of the methodological AI spectrum. The high level of interest in AI essentially originates from deep learning using multi-layered neural network (NN) architectures. Other AI approaches entering medicinal chemistry include expert systems and (laboratory) robotics. However, deep learning is clearly predominant. Of note, ML already has a long history in chemoinformatics and medicinal chemistry. For more than two decades, ML methods have been extensively applied for compound property predictions. In medicinal chemistry, properties of interest for computational studies include, first and foremost, biological activities of small molecules, but also physiochemical (e.g., solubility) or in vivo properties (such as metabolic stability or toxicity). Predictions of such properties aim to support the key task in the practice of medicinal chemistry: deciding which compound(s) to synthesize next. Over the years, NNs \\u2013 which were popular early on for property predictions \\u2013 were for the most part replaced by other ML methods such as support vector machines, random forests or Bayesian modeling. This was largely due to the tendency of NNs to overfit models to training data and also to the black box character of their predictions (the black box also applies to other \\u2013 but not all \\u2013 ML methods). In medicinal chemistry, chemical intuition continues to play a major role and black box predictions that cannot be explained in chemical terms work against the acceptance of ML for practical applications. Recently, NNs have been experiencing a renaissance in medicinal chemistry with the advent of deep neural networks (DNNs) and high expectations associated with deep ML. These expectations have primarily originated from other fields such as computer vision (image analysis), natural language processing or network science (including social networks).\"}, {\"paperId\": \"3201c9dea99875eb656337e763392acad6bc34e6\", \"abstract\": \"The application of deep machine learning, a subfield of artificial intelligence, has become a growing area of interest in predictive medicine in recent years. The deep machine learning approach has been used to analyze imaging and radiomics and to develop models that have the potential to assist the clinicians to make an informed and guided decision that can assist to improve patient outcomes. Improved prognostication of oral squamous cell carcinoma (OSCC) will greatly benefit the clinical management of oral cancer patients. This review examines the recent development in the field of deep learning for OSCC prognostication. The search was carried out using five different databases\\u2014PubMed, Scopus, OvidMedline, Web of Science, and Institute of Electrical and Electronic Engineers (IEEE). The search was carried time from inception until 15 May 2021. There were 34 studies that have used deep machine learning for the prognostication of OSCC. The majority of these studies used a convolutional neural network (CNN). This review showed that a range of novel imaging modalities such as computed tomography (or enhanced computed tomography) images and spectra data have shown significant applicability to improve OSCC outcomes. The average specificity, sensitivity, area under receiving operating characteristics curve [AUC]), and accuracy for studies that used spectra data were 0.97, 0.99, 0.96, and 96.6%, respectively. Conversely, the corresponding average values for these parameters for computed tomography images were 0.84, 0.81, 0.967, and 81.8%, respectively. Ethical concerns such as privacy and confidentiality, data and model bias, peer disagreement, responsibility gap, patient-clinician relationship, and patient autonomy have limited the widespread adoption of these models in daily clinical practices. The accumulated evidence indicates that deep machine learning models have great potential in the prognostication of OSCC. This approach offers a more generic model that requires less data engineering with improved accuracy.\"}, {\"paperId\": \"2e895113eeed00d2f3ab818f6afbeaf5e51e2a76\", \"abstract\": null}, {\"paperId\": \"f9cfab83f78b7fcaf9396ad8c1685763a6d719ae\", \"abstract\": \"In the same way mathematics is regarded by many mathematicians as the study of patterns in numbers (Hardy, 1940), the earth sciences can be thought of usefully as the study of patterns in the physical, chemical and biotic constituents of the Earth in both time and space. The documentation, definition and, ultimately, the explanation of how natural processes produce these patterns is referred to implicitly in the more standard definitions of these fields, where physics is construed as the study of matter and energy, chemistry the study of the composition of matter, and biology the study of life. It is the patterns existing in these domains that provide the subject matter for physical, chemical and biological study as well as providing the evidence that deterministic physical, chemical and biological processes exist. Moreover, it is these patterns that give nature its structure and organization. If such patterns did not exist\\u2013if everything on or in the Earth simply graded continuously and insensibly into everything else\\u2013it would not only be impossible to conduct a truly scientific investigation into any aspect of our planet, it would be pointless. Fortunately, humans are rather good at recognizing certain types of patterns. Throughout the evolutionary history of our lineage each of our five senses has been focused on, and honed by, natural selection for the purpose of identifying patterns in our surroundings. Our ancestors used these abilities to find food, shelter from extremes of weather, avoid predators, and care for one another. Of these senses, our visual sense is particularly acute (Isbell, 2011). Human intelligence developed to a higher degree than in any other known species, in part to accomplish the task of integrating the sense information collected by our eyes, ears, nose, tongue and skin. But even more importantly, this same intelligence allowed us to create tools whereby the performance levels of our bodies, as well as the acuity of our senses, can be enhanced well beyond their natural limits. As a result, our species has been able to survive in habitats no other similar creature could tolerate, double or treble its average lifespan over that of its evolutionary progenitors, and diversify to a point where it is now a force of planet-wide change whose influence and effects are becoming as great as those of any of the forces that have shaped the Earth\\u2019s surface in its geological past (e.g., volcanism, tectonism, erosion, glaciation, sea-level change). All of this has been accomplished, for the most part, using the pattern-recognition senses every human child is born with, augmented by tools the vast bulk of whose invention has taken place over the last 100\\u2013150 years. This is a record of progress to be proud of. Yet, we remain far from having reached the end of the path our scientific and technological forbearers have trod. Despite our innate abilities and technological advances, in many contexts our realized abilities to recognize and identify patterns in nature consistently and accurately are little better than those of our distant ancestors. Take, for example, the seemingly humble and straightforward task of identifying the living constituents of an aliquot of common seawater. In 2014 several colleagues and I published results of a blind test of identification accuracies exhibited by expert zooplankton taxonomists (Culverhouse et al., 2014). Twenty-one taxonomists were provided with the same set of six modern plankton samples (c. 700 specimens in total) and asked to place these samples\\u2019 living constituents into ten categories (e.g., Appendicularia, Chaetognatha, Copepoda, Euphausiacea, Ostracoda). Results reported by these experts were then compared. As Figure 1 shows, the tally count data collected from these expert taxonomists exhibits several interesting patterns, These include expected consistencies among experts who were members of the same laboratory, but also disturbing differences among laboratories. Even expert self consistencies (generally regarded as the mark of true expertise) were discouragingly variable, ranging from 99.7% to 68.2% for tally counts and 99.0% to 0.4% for identifications. While this investigation remains the most comprehensive evaluation of expert-level taxonomic identification performance published to date, its results are consistent with those found in a number of previous studies that address this issue. Based on the empirical evidence published to date, taxonomic experts can be relied upon to deliver results that are 70%-80% accurate in terms of their levels Artificial Intelligence & Machine Learning in the Earth Sciences\"}, {\"paperId\": \"49885611fc76df2995e4e9dc1fa388367ac7af9d\", \"abstract\": \"Advances in nextgeneration sequencing (NGS) platforms are allowing researchers to routinely collate large genomewide data sets to address a variety of ecological questions. However, with this big data comes big analytical challenges that are increasingly addressed using machine learning (for a review, see Schrider & Kern, 2018). Machine learning is a subfield of artificial intelligence and represents a conglomeration of methods where predictive accuracy is the primary goal (e.g., Belcaid & Toonen, 2015; Breiman, 2001; Elith et al., 2008; Lucas, 2020). Machine learning assumes that the datagenerating process is unknown and complex and finds the dominant patterns by learning the relationships between inputs and responses (Elith et al., 2008). Broadly, machine learning differs from other statistical approaches in two important ways. The first is that predictive performance drives model formulation rather than model selection or expert opinion, and the second is there is less emphasis on model selection ( Breiman, 2001; Lucas, 2020). For these reasons, machine learning has the reputation for being less interpretable and difficult to apply rigorously (Elith et al., 2008; Lucas, 2020; Molnar, 2018). However, in parallel with the revolution of sequencing techniques, there has also been a revolution in data science in terms of predictive performance and techniques to interpret machine learning models (FountainJones et al., 2019; Lucas, 2020; Molnar, 2018). There are now streamlined R and Python packages that make the robust use of algorithms from support vector machines (SVMs) to neural networks readily achievable (e.g., Abadi et al., 2015; Kuhn & Wickham, 2020, see Text Box 1 for some important machine learning terminology). Moreover, other statistical paradigms such as approximate Bayesian computation (ABC) are being applied sidebyside or within machine learning frameworks to enhance the utility of these approaches (e.g., Carlson, 2020; Raynal et al., 2019). The ability of machine learning algorithms to build powerful predictive models that capture complex nonlinear responses with minimal statistical assumptions has been harnessed by most molecular ecology subdisciplines for decades. For example, machine learning models were developed before the turn of the millennium to classify normal or cancerous tissue based on transcription profiles (Furey et al., 2000). Not long after gradient boosting models (GBMs) were developed (e.g., Hastie et al., 2009), researchers were applying the approach to classify population genetics models based on a suite of summary statistics such as Tajima's \\u03b8\\u03c0 (Lin et al., 2011). In addition, extensions of the popular random forest algorithm have been utilized in ecological genetics to untangle the drivers of climate adaptation (Fitzpatrick & Keller, 2015). Generally, however, advances in computer science and machine learning are slow to filter down to ecologists (Belcaid & Toonen, 2015; Elith & Hastie, 2008; FountainJones et al., 2019), partly through unfamiliarity with these types of approaches but also because of the rapid rate of advance in the data science field. This Special Issue aims to help expand the use of machine learning approaches and to help bring advances in data science to the toolkits of molecular ecologists. This issue comprises 17 papers grouped into four sections covering a diverse variety of molecular ecology subdisciplines. The first section covers how machine learning can be applied to make inferences about population demography. We further group these papers algorithmically with four papers utilizing random forest architecture and the remaining four using neural networks. The second section highlights how machine learning can detect signatures of selection across loci. The third section highlights how these methods can be applied to untangle the complex ecological drivers of genomic change (\\u2018ecological genomics\\u2019) and species community dynamics. The last section explores how advances in machine learning can provide insights into species limits and contribute to biodiversity monitoring.\"}, {\"paperId\": \"5427f0f44bb871124fcfc2193f53a51f88d2a57f\", \"abstract\": \"Machine learning and artificial intelligence (AI) have arrived in medicine and the healthcare community is experiencing significant growth in their adoption across numerous patient care settings. There are countless applications for machine learning and AI in medicine ranging from patient outcome prediction, to clinical decision support, to predicting future patient therapeutic setpoints. This commentary discusses a recent application leveraging machine learning to predict one\\u2010year patient survival following orthotopic heart transplantation. This modeling approach has significant implications in terms of improving clinical decision\\u2010making, patient counseling, and ultimately organ allocation and has been shown to significantly outperform pre\\u2010existing algorithms. This commentary also discusses how adoption and advancement of this modeling approach in the future can provide increased personalization of patient care. The continued expansion of information systems and growth of electronic patient data sources in health care will continue to pave the way for increased use and adoption of data science in medicine. Personalized medicine has been a long\\u2010standing goal of the healthcare community and with machine learning and AI now being continually incorporated into clinical settings and practice, this technology is well on the pathway to make a considerable impact to greatly improve patient care in the near future.\"}, {\"paperId\": \"7e69fb78138e1f325055fca886b53710ec96320e\", \"abstract\": \"Machine learning (ML), as an artificial intelligence tool, has acquired significant progress in data-driven research in Earth sciences. Land Surface Models (LSMs) are important components of the climate models, which help to capture the water, energy, and momentum exchange between the land surface and the atmosphere, providing lower boundary conditions to the atmospheric models. The objectives of this review paper are to highlight the areas of improvement in land modeling using ML and discuss the crucial ML techniques in detail. Literature searches were conducted using the relevant key words to obtain an extensive list of articles. The bibliographic lists of these articles were also considered. To date, ML-based techniques have been able to upgrade the performance of LSMs and reduce uncertainties by improving evapotranspiration and heat fluxes estimation, parameter optimization, better crop yield prediction, and model benchmarking. Widely used ML techniques used for these purposes include Artificial Neural Networks and Random Forests. We conclude that further improvements in land modeling are possible in terms of high-resolution data preparation, parameter calibration, uncertainty reduction, efficient model performance, and data assimilation using ML. In addition to the traditional techniques, convolutional neural networks, long short-term memory, and other deep learning methods can be implemented.\"}, {\"paperId\": \"11922790742d6c283f7068f266dbe3a59880b1e1\", \"abstract\": \"At the 2017 Annual Meeting of the American Academy for Cerebral Palsy and Developmental Medicine, we presented research illustrating the use of a machine learning algorithm (the k-means clustering algorithm) to identify diagnostic groups in the 2011 National Survey of Children\\u2019s Health (NSCH). We demonstrated how the algorithm could learn from parent-reported medical diagnoses and other characteristics, and thus identify groups of children with similar comorbidities and disabilities, yet with mixed developmental diagnostic labels. We will present follow-up research in 2018 comparing the composition of these groups to that of groups of children defined solely by diagnostic labels (cerebral palsy, intellectual disability, autism, developmental delay, etc.). As intended, the machine learning-defined groups are far more homogeneous with respect to a host of coexisting medical diagnoses and other health-related conditions than those defined by traditional labels. Cluster analysis is useful both because it can find inherent patterns in large volumes of heterogenous data, and because the discovery of such patterns acts as a data reduction technique. In our work with the NSCH data, we identified homogeneous groupings using seventeen simple indicator (yes/no) variables from data on over 80 000 children. Clustering based on these 17 indicators within a population of children with various underlying developmental diagnoses (cerebral palsy, autism spectrum disorder, etc.) reduced the data to seven highly similar, but non-overlapping, groups. This type of cluster analysis can be applied in the context of a single underlying diagnosis as well (e.g. cerebral palsy). The various clustering algorithms can perform equally well here, helping to make sense of within-group heterogeneity by further subdivision. Given limitations in data collected by the NSCH, however, clinical applicability of our results may be limited. An ideal cluster analysis for classifying children with a given developmental disability (or reclassifying children with any number of different diagnoses) would use as wide a variety of patient-specific data as possible. Examples might include health data abstracted directly from medical records, assessments of mobility (e.g. the Gross Motor Function Classification System), neuropsychological assessments, individual education plans, or anthropomorphic measurements. Cravedi et al. come close to achieving the ideal. They use a variant of cluster analysis known as ascendant hierarchical clustering to find subtypes of Gilles de la Tourette syndrome in children in France, using a wide array of clinical and condition-specific variables in their analysis. Their findings are intuitive, sensible, and potentially highly useful in defining clinically meaningful strata under the Tourette syndrome umbrella and in developing therapeutic approaches for treatment of the condition. We anticipate there will be many such applications of machine learning in developmental medicine in the years to come. We noted previously that machine learning has the potential to positively change the role of physicians in patient care. The work by Cravedi et al. reminds us that the role of researchers is likely to change significantly as well. As machine learning continues to provide new ways to think about children with developmental disabilities and their care providers, the path ahead holds many more changes for all branches of medical science. We have no illusions that every step along this path will be a positive one. But we do believe that the journey overall will lead to a better place. The latest work by Cravedi et al. adds to our optimism.\"}, {\"paperId\": \"3289a76f03fe7cbe97ad968fe1b8a957a42bedf5\", \"abstract\": \"Explainable artificial intelligence typically focuses on data-based explanations, lacking the semantic context needed to produce human-centric explanations. This is especially relevant in healthcare and life sciences where the heterogeneity in both data sources and user expertise, and the underlying complexity of the domain and applications poses serious challenges. The Semantic Web represents an unparalleled opportunity in this area: it provides large amounts of freely available data in the form of Knowledge Graphs, which link data to ontologies, and can thus act as background knowledge for building explanations closer to human conceptualizations. In particular, knowledge graphs support the computation of semantic similarity between objects, providing an understanding of why certain objects are considered similar or different. This is a basic aspect of explainability and is at the core of many machine learning applications. However, when data covers multiple domains, it may be necessary to integrate different ontologies to cover the full semantic landscape of the underlying data. We propose a methodology for semantic explanations in the biomedical domain that is based on the semantic annotation and integration of heterogenous data into a common semantic landscape that supports semantic similarity assessments. This methodology builds upon state of the art semantic web technologies and produces post-hoc explanations that are independent of the machine learning method employed.\"}, {\"paperId\": \"0d6d142dc49cf7537ece045d8d469fd014a5d3b6\", \"abstract\": \"Human space exploration beyond low Earth orbit will involve missions of significant distance and duration. To effectively mitigate myriad space health hazards, paradigm shifts in data and space health systems are necessary to enable Earth-independence, rather than Earth-reliance. Promising developments in the fields of artificial intelligence and machine learning for biology and health can address these needs. We propose an appropriately autonomous and intelligent Precision Space Health system that will monitor, aggregate, and assess biomedical statuses; analyze and predict personalized adverse health outcomes; adapt and respond to newly accumulated data; and provide preventive, actionable, and timely insights to individual deep space crew members and iterative decision support to their crew medical officer. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration, on future applications of artificial intelligence in space biology and health. In the next decade, biomonitoring technology, biomarker science, spacecraft hardware, intelligent software, and streamlined data management must mature and be woven together into a Precision Space Health system to enable humanity to thrive in deep space.\"}, {\"paperId\": \"5d8132f52b706357b4634775415c97a382c9ecb9\", \"abstract\": null}, {\"paperId\": \"be05f1219ac8136b4d35f55af45e76e22ab8c8a3\", \"abstract\": \"Simple Summary Oral cancer is characterized by high morbidity and mortality, since the disease is typically in an advanced locoregional stage at the time of diagnosis. The application of artificial intelligence (AI) techniques to oral cancer screening has recently been proposed. This scoping review analyzed the information about different machine learning tools in support of non-invasive diagnostic techniques including telemedicine, medical images, fluorescence images, exfoliative cytology and predictor variables at risk of developing oral cancer. The results suggest that such tools can make a noninvasive contribution to the early diagnosis of oral cancer and we express the gaps of the proposed questions to be improved in new investigations. Abstract The early diagnosis of cancer can facilitate subsequent clinical patient management. Artificial intelligence (AI) has been found to be promising for improving the diagnostic process. The aim of the present study is to increase the evidence on the application of AI to the early diagnosis of oral cancer through a scoping review. A search was performed in the PubMed, Web of Science, Embase and Google Scholar databases during the period from January 2000 to December 2020, referring to the early non-invasive diagnosis of oral cancer based on AI applied to screening. Only accessible full-text articles were considered. Thirty-six studies were included on the early detection of oral cancer based on images (photographs (optical imaging and enhancement technology) and cytology) with the application of AI models. These studies were characterized by their heterogeneous nature. Each publication involved a different algorithm with potential training data bias and few comparative data for AI interpretation. Artificial intelligence may play an important role in precisely predicting the development of oral cancer, though several methodological issues need to be addressed in parallel to the advances in AI techniques, in order to allow large-scale transfer of the latter to population-based detection protocols.\"}, {\"paperId\": \"33cae44bba5a9a27cb282acd418872dd29b83d6e\", \"abstract\": \"Similarity query (a.k.a. nearest neighbor query) processing has been an active research topic for several decades. It is an essential procedure in a wide range of applications (e.g., classification & regression, deduplication, image retrieval, and recommender systems). Recently, representation learning and auto-encoding methods as well as pre-trained models have gained popularity. They basically deal with dense high-dimensional data, and this trend brings new opportunities and challenges to similarity query processing. Meanwhile, new techniques have emerged to tackle this long-standing problem theoretically and empirically. This tutorial aims to provide a comprehensive review of high-dimensional similarity query processing for data science. It introduces solutions from a variety of research communities, including data mining (DM), database (DB), machine learning (ML), computer vision (CV), natural language processing (NLP), and theoretical computer science (TCS), thereby highlighting the interplay between modern computer science and artificial intelligence technologies. We first discuss the importance of high-dimensional similarity query processing in data science applications, and then review query processing algorithms such as cover tree, locality sensitive hashing, product quantization, proximity graphs, as well as recent advancements such as learned indexes. We analyze their strengths and weaknesses and discuss the selection of algorithms in various application scenarios. Moreover, we consider the selectivity estimation of high-dimensional similarity queries, and show how researchers are bringing in state-of-the-art ML techniques to address this problem. We expect that this tutorial will provide an impetus towards new technologies for data science.\"}, {\"paperId\": \"da6676b473e936d300a890fb14de89a5d9426b5a\", \"abstract\": \"Ocean-going platforms are integrating high-resolution camera feeds for observation and navigation, producing a deluge of visual data. The volume and rate of this data collection can rapidly outpace researchers\\u2019 abilities to process and analyze them. Recent advances in machine learning enable fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data set standardization, insufficient formatting, and aggregation of existing, expertly curated imagery for use by data scientists. To address this need, we have built FathomNet, a public platform that makes use of existing, expertly curated data. Initial efforts have leveraged MBARI\\u2019s Video Annotation and Reference System and annotated deep sea video database, which has more than 7M annotations, 1M frame grabs, and 5k terms in the knowledgebase, with additional contributions by National Geographic Society (NGS) and NOAA\\u2019s Office of Ocean Exploration and Research. FathomNet has over 160k localizations of 1.4k midwater and benthic classes, and contains more than 70k iconic and non-iconic views of marine animals, underwater equipment, debris, etc. We demonstrate how machine learning models trained on FathomNet data can be applied across different institutional video data, and enable automated acquisition and tracking of midwater animals using a remotely operated vehicle. As FathomNet continues to develop and incorporate more image data from other oceanographic community members, this effort will enable scientists, explorers, policymakers, storytellers, and the public to understand and care for our ocean. The Ocean is Earth\\u2019s final frontier Exploration and discovery in the ocean are crucial for sustainable management of this resource, and is vital for fisheries, mineral extraction, pharmaceutical development, and recreation. With the role that the ocean plays in modulating the Earth\\u2019s climate, and the role that its inhabitants play in carbon sequestration, accessing and understanding the life that resides within the ocean is critical. Exploring a space as vast as the ocean1, filled with life that we have yet to describe2 and numerous chemical and physical processes that we are only beginning to understand, using traditional, resource-intensive (e.g., time, person-hours, cost) sampling methodologies are limited in their ability to scale in spatiotemporal resolution and engage diverse communities3. However, with the advent of modern robotics4, low-cost observation platforms, and distributed sensing,5, 6 we are beginning to see a paradigm shift in ocean exploration and discovery. This shift is evidenced in satellite remote sensing of near-surface ocean conditions and in the global ARGO float array, where distributed platforms and open data structures are propelling the chemical and remote sensing communities to new scales of observation7\\u20139. Due to a variety of constraints, large-scale sampling of biological communities or processes below the surface waters of the ocean has largely lagged behind. Three common modalities for observing biology and biological processes in the ocean include acoustics, -omics, and imaging, and each sensing modality has their strengths and weaknesses. Acoustics allow for observations of populationand group-scale dynamics, however individual-scale observations, especially determination of animals down to lower taxonomic groups like species, are challenging tasks. Omics, particularly the promising field of eDNA, allows for identification of biological communities based on their shed DNA in the water column. While eDNA studies provide broad-scale views of biological communities with only a few discrete samples, knowing the spatial source of those DNA, how measurements relate to population sizes, and presence of confounding non-marine biological markers in samples are active areas of research that still need to be addressed. On the other hand, imaging enables the identification of animals to the species level, elucidates community structure and spatial relationships in a variety of habitats, and reveals fine-scale behavior of animal groups. However, processing visual data, particularly data with complex scenes and organisms that require expert classifications, is a resource-intensive process that cannot be scaled without significant advances in automation. Observing life in the ocean using imaging Despite the resource-intensive nature of analysing visual data, underwater imaging continues to be widely used in the marine science and technology communities. Common imaging techniques involve the use of high-definition video cameras, microscopy, macro-scale, stereo, volumetric, and multi-spectral camera systems with illumination provided by single-band or wide-band lights, LEDs, and lasers10, 11. Underwater surveys and counts of animals can be accomplished in a variety of environments due to the ease by which imaging technology can be deployed, and the number of remotely controlled and autonomous platforms that can be used12. Underwater imaging is also being used in real-time for underwater vehicle navigation and control while performing difficult tasks in complex environments. Imaging has also been used as an effective engagement tool to share marine life and the issues facing the ocean with broader communities. These efforts have resulted in quantifiable changes in public opinion, and in building more ocean-aware communities13. The value of imaging to convey understanding of ocean life is without doubt. Given all these applications of underwater imaging, a number of annotation tools have been developed to deal with the challenges associated with managing and analyzing visual data. These efforts have resulted in many capable software solutions that can either be deployed on your computer locally, in the field, or on the worldwide web. However, given the limited availability of experts, prohibitive costs to annotate footage, and the expansion of imaging as an engagement tool and sensing modality, novel methods for automated annotation of underwater visual data are desperately needed. This need is motivating deployment of artificial intelligence and data science tools in the ocean realm. Automating analysis of visual data using artificial intelligence Artificial intelligence (AI) is a broad term that encompasses advanced statistical methods, machine learning, supervised and unsupervised learning, and deep learning or convolutional neural networks (CNNs)14, 15. Statistical methods that include random forests have been successfully used in the plankton imaging community, achieving automated classification of microscale plants and animals with accuracies greater than 90%16, 17. Unsupervised learning has been used to identify regions or moments of interest in underwater video footage during vehicle deployments and post-processing annotation tasks. Although unsupervised learning can be deployed with minimal data and a priori knowledge of underwater scenes, these algorithms have limited application to automating the detection and classification of objects in underwater imagery with sufficient granularity and detail to be used for annotation. Supervised learning, or CNNs that are trained on visual data where all objects have been identified (Figure 1), has improved performance of automated annotation and classification tasks to the genus and species level. Given the potential for CNNs to automate the annotation and classification of animals to lower taxonomic levels, the underwater imaging community has called out the need for publicly available, large-scale image training sets. Currently, underwater image training\"}, {\"paperId\": \"a14181f0aac6c25e2c9c8064ec1a96e83b7290d9\", \"abstract\": \"Computational machine learning, especially self-enhancing algorithms, prove remarkable effectiveness in applications, including cardiovascular medicine. This review summarizes and cross-compares the current machine learning algorithms applied to electrocardiogram interpretation. In practice, continuous real-time monitoring of electrocardiograms is still difficult to realize. Furthermore, automated ECG interpretation by implementing specific artificial intelligence algorithms is even more challenging. By collecting large datasets from one individual, computational approaches can assure an efficient personalized treatment strategy, such as a correct prediction on patient-specific disease progression, therapeutic success rate and limitations of certain interventions, thus reducing the hospitalization costs and physicians\\u2019 workload. Clearly such aims can be achieved by a perfect symbiosis of a multidisciplinary team involving clinicians, researchers and computer scientists. Summarizing, continuous cross-examination between machine intelligence and human intelligence is a combination of precision, rationale and high-throughput scientific engine integrated into a challenging framework of big data science.\"}, {\"paperId\": \"7eff57959e4a2c5d50b16499fdb6dad3e1bf9e4b\", \"abstract\": \"In the current day, with the growth of technology the number of people travelling by flights has increased. Consistently a significant number of flights are delayed or crossed out because of numerous of reasons. These delays bother travelers. These delays also cost a lot to the aircraft organization. Flight delays negatively affect carriers, air terminals and travelers. There are different methodologies used to manufacture flight delays expectation models from the Data Science point of view. The key resource of a flight includes aircraft, cockpit crew and cabin crew. For purposes of dispatching resources effectively, the three resources may be distributed independently. If the initial flight of a flight plan is delayed due to bad weather or other factors, it may result in the delays of the directly downstream flights that need to await its resources. If the delays continue to spread to the lower downstream flights, it may result in large area delay propagation. The method proposed here introduces and summarizes the initiatives used to address the flight delay prediction problem, according to scope, data and computational methods, giving special attention to an increasing usage of machine learning methods.\"}, {\"paperId\": \"afdbd4b4093b8f83b225f33e6afba5a69c9a7a97\", \"abstract\": \"Andrew NG, a leading philosopher in the field of Artificial Intelligence (AI) once quoted \\u201cAI is the new electricity\\u201d which has the potential to transform and drive every industry. The most important driving factor for the AI transformation will be data. Clive Humby, a data science entrepreneur was once quoted saying \\u201cdata is the new oil\\u201d and data analytics being the \\u201ccombustion engine\\u201d will drive the AI led innovations. The rapid rise of Artificial Intelligence technologies in the past decade, has inspired industries to invest in every opportunity for integrating AI solutions to their products. Research, development, and innovation in the field of AI are shaping various industries like automobile, manufacturing, finance, retail, supply chain management, and education among others. The healthcare industry has also been adopting the ways of AI into various workflows within the domain. With the evolution in computing and processing powers coupled with hardware modernizations, the adoption of AI looks more feasible than ever. Research and Innovations are happening in almost every field of healthcare and hospital workflows with the target of making healthcare processes more efficient & accessible, increase the overall state of healthcare, reduce physician stress levels, and increase the patient satisfaction levels. The conventional ways in which healthcare and clinical workflows have been operating are now starting to see the change with the integration of many data driven AI solutions. The digital innovations are making life easy for healthcare professionals allowing them to spend more time listening to the problems of patients and consequently increasing the patient satisfaction levels. However, there are limitations and concerns on security of Protected Health Information which have to be addressed for a seamless amalgamation of AI systems into the healthcare domain. Many papers have been published which mostly talk about one particular field/problem in the healthcare domain. No publications have covered the opportunities provided by AI technologies to the entire healthcare domain. This review paper discusses in detail about the progress AI has been able to make in the healthcare domain holistically and what the future of AI looks like. The paper also discusses about the implementation opportunities various AI technologies like Machine Learning, Deep Learning, Reinforcement Learning, Natural Language Processing, Computer Vision provide in different fields of healthcare and clinical workflows and how Artificial Intelligence systems will boost the capabilities of healthcare professionals in restoring the human touch in patient-physician encounters. A physician\\u2019s intuition and judgement will always remain better suited since each case, each health condition, and each person is unique in its own way, but AI methods can help enhance the accuracy of diagnosis, assist physicians in making improved and precise clinical decisions.\"}, {\"paperId\": \"cb4a359d90d24f2a872784e98b852f3fc0e36368\", \"abstract\": \"The aim of this study is to examine the studies in the literature on the use of artificial intelligence in education in terms of its bibliometric properties. The Web of Science (WoS) database was used to collect the data. Various keywords were used to search the literature, and a total of 2,686 publications on the subject published between 2001-2021 were found. The inquiry revealed that most of the studies were carried out in the USA. According to the results, it was seen that the most frequently published journals were Computers Education and International Journal of Emerging Technologies in Learning. The study showed that the institutions of the authors were in the first place as Carnegie Mellon University, University of Memphis and Arizona State University as the most productive organizations due to the number of their publications, while Vanlehn, K. and Chen, C. \\u2013M. were the most effective and productive researchers. As a result of the analysis, it was determined that the co-authorship network structure was predominantly USA, Taiwan and United Kingdom. In addition, when the keywords mentioned together were mapped, it was seen that the words artificial intelligence, intelligent tutoring systems, machine learning, deep learning and higher education were used more frequently.\"}, {\"paperId\": \"21f76cce6f1419a631ad5b78a63d7d8c2fab0c54\", \"abstract\": null}, {\"paperId\": \"06a550424758d9c3766051d7cb68540f43879071\", \"abstract\": null}, {\"paperId\": \"ec0a152504f18c60d5ae17f6c070193e8d73edc1\", \"abstract\": \"In this paper, a novel application of machine learning algorithms including Neural Network architecture is presented for the prediction of flood severity. Floods are considered natural disasters that cause wide-scale devastation to areas affected. The phenomenon of flooding is commonly caused by runoff from rivers and precipitation, specifically during periods of extremely high rainfall. Due to the concerns surrounding global warming and extreme ecological effects, flooding is considered a serious problem that has a negative impact on infrastructure and humankind. This paper attempts to address the issue of flood mitigation through the presentation of a new flood dataset, comprising 2000 annotated flood events, where the severity of the outcome is categorised according to 3 target classes, demonstrating the respective severities of floods. The paper also presents various types of machine learning algorithms for predicting flood severity and classifying outcomes into three classes, normal, abnormal, and high-risk floods. Extensive research indicates that artificial intelligence algorithms could produce enhancement when utilised for the pre-processing of flood data. These approaches helped in acquiring better accuracy in the classification techniques. Neural network architectures generally produce good outcomes in many applications, however, our experiments results illustrated that random forest classifier yields the optimal results in comparison with the benchmarked models.\"}, {\"paperId\": \"94276e745888fa90a1eb2f3c1a33da979adf8c1f\", \"abstract\": \"Artificial intelligence applications within the geo-sciences are becoming increasingly common, yet there are still many challenges involved in adapting established techniques to geoscience data sets. Applications in the realm of volcanic hazards assessment show great promise for addressing such challenges. Here, we describe a Jupyter Notebook we developed that ingests real-time GNSS data streams from the EarthCube CHORDS (Cloud-Hosted Real-time Data Services for the geosciences) portal TZVOLCANO, applies unsupervised learning algorithms to perform automated data quality control (\\\"noise reduction\\\"), and explores autonomous detection of unusual volcanic activity using a neural network. The TZVOLCANO CHORDS portal streams real-time Global Navigation Satellite System (GNSS) positioning data in 1 second intervals from the TZVOLCANO network, which monitors the active volcano Ol Doinyo Lengai in Tanzania, through UNAVCO\\u2019s real-time GNSS data services. UNAVCO\\u2019s real-time data services provide near-real-time positions processed by the Trimble Pivot system. The positioning data (latitude, longitude, and height) are imported into this Jupyter Notebook in user-defined time spans. The positioning data are then collected in sets by the Jupyter Notebook and processed to extract a useful calculated variable in preparation for the machine learning algorithms, of which we choose the vector magnitude. Unsupervised K-means and Gaussian Mixture machine learning algorithms are then utilized to locate and remove data points (\\\"filter\\\") that are likely caused by noise and unrelated to volcanic signals. We find that both the K-means and Gaussian Mixture machine learning algorithms perform well at identifying regions of high noise within tested GNSS data sets, but the Gaussian Mixtures approach performs better. The filtered data are then used to train an artificial intelligence neural network that predicts volcanic deformation. Our Jupyter Notebook has the potential to be used for detecting potentially hazardous volcanic activity in the form of rapid vertical or horizontal displacement of the Earth\\u2019s surface.\"}, {\"paperId\": \"ec0b011f08d09bfcbc6325da0b7b8ba1c5abd493\", \"abstract\": null}, {\"paperId\": \"89be2697249abd4b21d946bf30b1181002af1ad9\", \"abstract\": \"THE MYTH OF ARTIFICIAL INTELLIGENCE: Why Computers Can't Think the Way We Do by Erik J. Larson. Cambridge, MA: Belknap Press, 2021. 312 pages. Hardcover; $29.95. ISBN: 9780674983519. *The Myth of Artificial Intelligence (AI) offers a technical and philosophical introduction to AI with an emphasis on AI's limitations. Larson, a computer scientist and tech entrepreneur, keeps his central claim modest: true general AI is neither inevitable nor imminent, and if it is possible, it will require fundamentally new approaches. It is an easy read, combining references to fiction, history, and science. It lays out a bird's eye view of the origins and ideas behind current AI methods, focusing on general AI, a category of AI that would need to learn and engage with a wide variety of problems. *Separated into three parts, The Myth of AI begins with the history and algorithmic logic of AI, largely through the lens of the Turing test. Larson argues that we are not near the singularity (superintelligent computers able to create ever more intelligent machines) and that, in fact, the basic premise of the singularity is flawed. *The second part discusses inference. AI falls short of human intelligence because it can work with hard rules, but cannot make the guesses necessary to formulate new ones or handle uncertain rules. In attempts at the Turing test, AI can throw data at the problem but will always lack understanding. Achieving the understanding necessary for true intelligence will require an approach fundamentally different from recent advances made in AI, which are only effective for narrow AI (a category of AI for solving specialized problems) and not general AI. *The final, and relatively brief, part examines AI in science. According to Larson's assessment, new scientific research relies heavily on newly available computation power and big data in order to use narrow AI to its full extent. Larson claims that this approach will hinder development of new theories. He also claims that this leads to treating scientists as if they were computers as well, which causes overvaluing the system of science above people. He criticizes \\\"swarm science,\\\" which he describes as a large group of scientists approaching one problem with a variety of projects, emphasizing this collaboration over the individuals. Instead, he claims, we need our culture to continue to emphasize individual discovery and intelligence, as it is the key to innovation. *Through the discussions of the history, philosophy, and logic of AI in the first two parts of the book, Larson disentangles the hype of AI from what is actually possible with current technology. Even as he sheds light on the gap between the singularity prediction and what machine learning is truly capable of, he emphasizes the significance of the myth. \\\"The myth is an emotional lighthouse by which we navigate the AI topic\\\" (p. 76). The stories we tell through predictions and science fiction define AI in the public eye and set the goals for AI research. *Our underlying philosophy matters as much as the current state of AI research, when we consider the social role of AI and what we predict for our future. In the development of AI, we must define intelligence and explore what it means to be human. While this is not a book with overtly religious claims, it does acknowledge the spiritual claims inherent in discussions of personhood. It also frames technoscience as replacing philosophy and religion and as the oversimplified understanding of humanity and the precursor to expectations of the singularity. *Beyond the stated goal of disenchanting the reader of the inevitability of AI, the book highlights the significance of stories to both society and science and emphasizes the importance of understanding for both humans and AI. We need to understand not only the technical aspects of the technology we build but also the philosophy that defines our goals. *While I found the first two sections of the book to be an engaging and accurate discussion of the tension between the science and hopes of AI, I had concerns about the warnings of \\\"swarm science\\\" in the third. Larson is placing a strong emphasis on individual genius in science; however, science has never been a truly independent endeavor. Many times in history, from evolution to DNA, multiple teams of scientists independently made the same discoveries at nearly the same time, based on previously published work. Though these discoveries were not inevitable, they built upon other research and relied on collaboration at least as much as individual genius. Larson focuses on a particular neuroscience project and makes some valid criticisms, but then he generalizes his observations to all of science in ways that I do not believe to be accurate. His argument that all of science is moving away from theory toward shallow observations is not as obvious as he claims, nor is it supported by the evidence offered in the book. *As a counterexample, the research that resulted in the COVID-19 vaccine could be considered \\\"swarm science\\\" and was effective. Large amounts of funding were very suddenly directed to many scientists for one goal: understand and prevent the coronavirus. Due to both new funding and established research, we developed and approved multiple vaccines in one year. I was not convinced of several of Larson's generalizations in this third section. Tension between celebrating collaboration and individual genius will persist. However, it appears that there is more collaboration in science today. This is likely due to a variety of reasons, including a scientific community connected by the internet and more contributors receiving appropriate credit for their work. *The Myth of AI is a broad view of AI that should prove valuable and comprehensible to readers with or without a technical background. The first two sections offer a clear explanation and history of AI, and the third offers food for thought on how the process of science has been shaped by advances in AI and computer technology. The first sections would be a good introduction to someone not familiar with AI or looking to think about the philosophy of AI and I\\u00ac\\u2020would recommend the book for these sections. *While the book avoids religious claims, the philosophical discussions of what it means to \\\"understand\\\" and the level of trust we place in AI are essential questions for Christians working in technology-related disciplines. The Myth of AI presents a jumping-off point for much deeper reflection about using AI responsibly and what it means to be human. *Reviewed by Elizabeth Koning, graduate student in the Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL 61801.\"}, {\"paperId\": \"e585f6e752fb2668b33f7d4b18c8af9bd5abc1a4\", \"abstract\": \"Increasingly, modern Artificial Intelligence (AI) research has become more computationally intensive. However, a growing concern is that due to unequal access to computing power, only certain firms and elite universities have advantages in modern AI research. Using a novel dataset of 171394 papers from 57 prestigious computer science conferences, we document that firms, in particular, large technology firms and elite universities have increased participation in major AI conferences since deep learning's unanticipated rise in 2012. The effect is concentrated among elite universities, which are ranked 1-50 in the QS World University Rankings. Further, we find two strategies through which firms increased their presence in AI research: first, they have increased firm-only publications; and second, firms are collaborating primarily with elite universities. Consequently, this increased presence of firms and elite universities in AI research has crowded out mid-tier (QS ranked 201-300) and lower-tier (QS ranked 301-500) universities. To provide causal evidence that deep learning's unanticipated rise resulted in this divergence, we leverage the generalized synthetic control method, a data-driven counterfactual estimator. Using machine learning based text analysis methods, we provide additional evidence that the divergence between these two groups - large firms and non-elite universities - is driven by access to computing power or compute, which we term as the \\\"compute divide\\\". This compute divide between large firms and non-elite universities increases concerns around bias and fairness within AI technology, and presents an obstacle towards \\\"democratizing\\\" AI. These results suggest that a lack of access to specialized equipment such as compute can de-democratize knowledge production.\"}, {\"paperId\": \"45c86e27c5446d273b12d2a5dc544811eff3a299\", \"abstract\": \"Artificial intelligence, machine learning, health care robots, and algorithms for clinical decision-making are currently being sought after in diverse fields of clinical medicine and bioengineering. The field of personalized medicine stands to benefit from new technologies so as to harness the omics big data, for example, to individualize and accelerate cancer diagnostics and therapeutics in particular. In this overarching context, breast cancer is one of the most common malignancies worldwide with multiple underlying molecular etiologies and each subtype displaying diverse clinical outcomes. Disease stratification for breast cancer is, therefore, vital to its effective and individualized clinical care. The support vector machine (SVM) is a rising machine learning approach that offers robust classification of high-dimensional big data into small numbers of data points (support vectors), achieving differentiation of subgroups in a short amount of time. Considering the rapid timelines required for both diagnosis and treatment of most aggressive cancers, this new machine learning technique has important clinical and public applications and implications for high-throughput data analysis and contextualization. This expert review describes and examines, first, the SVM models employed to forecast breast cancer subtypes using diverse systems science data, including transcriptomics, epigenetics, proteomics, and radiomics, as well as biological pathway, clinical, pathological, and biochemical data. Then, we compare the performance of the present SVM and other diagnostic and therapeutic prediction models across the data types. We conclude by emphasizing that data integration is a critical bottleneck in systems science, cancer research and development, and health care innovation and that SVM and machine learning approaches offer new solutions and ways forward in biomedical, bioengineering, and clinical applications.\"}, {\"paperId\": \"adb01374d2f7167761b7a752964c01c3a80c1b62\", \"abstract\": \"Artificial intelligence (AI) and machine learning have promised to revolutionize the way we live and work, and one of particularly promising areas for AI is image analysis. Nevertheless, many current AI applications focus on post-processing of data, while in both materials sciences and medicines, it is often critical to respond to the data acquired on the fly. Here we demonstrate an artificial intelligent atomic force microscope (AI-AFM) that is capable of not only pattern recognition and feature identification in ferroelectric materials and electrochemical systems, but can also respond to classification via adaptive experimentation with additional probing at critical domain walls and grain boundaries, all in real time on the fly without human interference. We believe such a strategy empowered by machine learning is applicable to a wide range of instrumentations and broader physical machineries.\"}, {\"paperId\": \"99b32e402447ad13ca0da231b60885ff906f2221\", \"abstract\": \"Plant disease automation in agriculture science is the primary concern for every country, as the food demand is increasing at a fast rate due to an increase in population. Moreover, the increased use of technology today has increased the efficacy and accuracy of detecting diseases in plants and animals. The detection process marks the beginning of a series of activities to fight the diseases and reduce their spread. Some diseases are also transmitted between animals and human beings, making it hard to fight them. For many years, scientists have researched how to deal with the common diseases that affect humans and plants. However, there are still many parts of the detection and discovery process that have not been completed. The technology used in medical procedures has not been adequate to detect all diseases on time, and that is why some diseases turn out to become pandemics because they are hard to detect on time. Our focus is to clarify the details about the diseases and how to detect them promptly with artificial intelligence. We discuss the use of machine learning and deep learning to detect diseases in plants automatically. Our study also focuses on how machine learning methods have been moved from conventional machine learning to deep learning in the last five years. Furthermore, different data sets related to plant diseases are discussed in detail. The challenges and problems associated with the existing systems are also presented.\"}, {\"paperId\": \"820cf78e7113511ea340f5cec477734dca84f644\", \"abstract\": \"Recent evolution in the field of data science has revealed the potential utility of machine learning (ML) applied to criminal justice. Hence, the literature focused on finding better techniques to predict criminal recidivism risk is rapidly flourishing. However, it is difficult to make a state of the art for the application of ML in recidivism prediction. In this systematic review, out of 79 studies from Scopus and PubMed online databases we selected, 12 studies that guarantee the replicability of the models across different datasets and their applicability to recidivism prediction. The different datasets and ML techniques used in each of the 12 studies have been compared using the two selected metrics. This study shows how each method applied achieves good performance, with an average score of 0.81 for ACC and 0.74 for AUC. This systematic review highlights key points that could allow criminal justice professionals to routinely exploit predictions of recidivism risk based on ML techniques. These include the presence of performance metrics, the use of transparent algorithms or explainable artificial intelligence (XAI) techniques, as well as the high quality of input data.\"}, {\"paperId\": \"d24f80089bb837324724105e928638f90312f082\", \"abstract\": \"Artificial intelligence (AI) is a subdiscipline of computer science that has made substantial progress in medicine and there is a growing body of AI research in dentistry. Dentists should have an understanding of the foundational concepts and the ability to critically evaluate dental research in AI. Machine learning (ML) is a subfield of AI that most dental AI research is dedicated to. The most prolific area of ML research is automated interpretation of dental imaging. Other areas include providing treatment recommendations, predicting future disease and treatment outcomes. The research impact is limited by small datasets that do not harness the positive correlation between very large datasets and ML performance. There is also a need to standardise research methodologies and utilise performance metrics that are appropriate for the clinical context. In addition to research challenges, this article discusses the ethical, legal and logistical considerations associated with implementation in clinical practice. This includes explainable AI, model bias, data privacy and security. The future implications of AI in dentistry involve a promise for a novel form of practicing dentistry however, the effect of AI on patient outcomes is yet to be determined.\"}, {\"paperId\": \"2801497b342869054322ed4b4167ccbda94235e6\", \"abstract\": null}, {\"paperId\": \"d226eae365188cb1d82a942d83b816ca24b853e0\", \"abstract\": \"230 During the last decade, enhanced computing power and the availability of large amounts of data have prompted the practical use of artificial intelligence in health care. Health and medical journals now commonly include reports on machine learning and big data, and descriptions of the risks posed by, and the governance required to manage, this technology. Machine learning algorithms are used to make diagnoses, identify treatments and analyse public health threats, and these systems can learn and improve continuously in response to new data. The tension between risks and concerns on one hand versus potential and opportunity on the other has shaped this issue of the Bulletin of the World Health Organization on the new ethical challenges of artificial intelligence in public health. Data-driven discovery and analysis in health care can increase knowledge and efficiency as well as challenge social values related to privacy, data control and the monetization of personal information. In India, for example, the adoption of a system for assigning all citizens a unique identification number, linking it to individual health records and several health-related schemes, raises ethical, legal and social issues, and the need for an appropriate ethical framework and data governance.1 These issues might be particularly challenging in lowand middle-income countries. Trust is perhaps the overarching theme of the contributions to this issue, and it is indeed one of the central values in digital health. One article explores opportunities for a human-centric ethical and regulatory environment to support the evolution of trust-based artificial intelligence with special regard to health insurance.2 Likewise, trust plays a role along with empathy and compassion in the humane side of care, the importance of which must be preserved in exploring the kind of health care society ought to promote.3 Similarly, European Union guidance might be too context-specific and as such leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally.4 In the context of population health research, researchers propose a post-research review model for ethics governance of research using artificial intelligence.5 For mobile health research in behavioural science, machine learning tools pose novel challenges for transparency, privacy, consent and the management of adverse events, all of which point to the need for consensusbased guidelines.6 As use of artificial intelligence systems expands, accountability for harm to patients and responsibility for their safety entail the need for human control and understanding of these systems.7 Other safeguards will require deliberate investments in data quality, access to care and processes to minimize bias, all in the service of trustworthiness.8 Success in integrating artificial intelligence into everyday patient care, as for instance in the United Kingdom of Great Britain and Northern Ireland\\u2019s National Health Service, is dependent on transparency, accountability and trust.9 In addition to trust, the values of fairness, justice and equity are seen as posing challenges even if other ethical duties are met. If artificial intelligence systems can explicitly improve equity, it is also a requirement that they do not worsen inequity.10 Thus, the case of neglected tropical diseases in low-resource settings illustrates opportunities for improved public health, as well as new challenges.11 Globally, the potential to help address some shortages and unmet needs in public health and care services might be realized by artificial intelligence-controlled conversational agents or chatbots that give health advice. However, realizing this potential will require the collaborative establishment of best practices and international ethics guidelines for technologies that replace humans.12 The field of bioethics emerged and grew in response to the development of new technologies and, sometimes, related wrongdoing. Ensuring adequate education, governance and ongoing ethical scrutiny will be essential if we are to realize the benefits and minimize the risks of this new technology. Questions of artificial intelligence accountability, equity and inclusiveness remain. The field is quickly evolving, and more artificial intelligence-based applications and services are becoming available in high-income countries. Identifying better tools for benefit-sharing and, simultaneously, evidence-based safeguards and criteria for appropriate uses and users to benefit everyone, including those in middleand lowincome countries, is essential. The World Health Organization (WHO) has made a commitment to addressing ethics, governance and regulation of artificial intelligence for health. In late 2019, WHO established an expert group to help develop a global framework for ethics and governance in artificial intelligence. The goal of this initiative is to ensure that these technologies are aligned with the overarching aims of promoting fair and equitable global health, meeting human rights standards and supporting Member States\\u2019 commitments to achieve universal health coverage. \\u25a0 Balancing risks and benefits of artificial intelligence in the health sector Kenneth Goodman, Diana Zandi, Andreas Reis & Effy Vayena\"}, {\"paperId\": \"dab040238280354a35b648cde5aff1e449e5be76\", \"abstract\": \"Widespread adoption of electronic health records (EHRs) has resulted in the collection of massive amounts of clinical data. In ophthalmology in particular, the volume range of data captured in EHR systems has been growing rapidly. Yet making effective secondary use of this EHR data for improving patient care and facilitating clinical decision-making has remained challenging due to the complexity and heterogeneity of these data. Artificial intelligence (AI) techniques present a promising way to analyze these multimodal data sets. While AI techniques have been extensively applied to imaging data, there are a limited number of studies employing AI techniques with clinical data from the EHR. The objective of this review is to provide an overview of different AI methods applied to EHR data in the field of ophthalmology. This literature review highlights that the secondary use of EHR data has focused on glaucoma, diabetic retinopathy, age-related macular degeneration, and cataracts with the use of AI techniques. These techniques have been used to improve ocular disease diagnosis, risk assessment, and progression prediction. Techniques such as supervised machine learning, deep learning, and natural language processing were most commonly used in the articles reviewed.\"}, {\"paperId\": \"65daaf915c0b9c6a7bcad1e40b4b46ff5551405c\", \"abstract\": \"The impact parameter is one of the crucial physical quantities of heavy-ion collisions (HICs), and can affect obviously many observables at the final state, such as the multifragmentation and the collective flow. Usually, it cannot be measured directly in experiments but might be inferred from observables at the final state. Artificial intelligence has had great success in learning complex representations of data, which enables novel modeling and data processing approaches in physical sciences. In this article, we employ two of commonly used algorithms in the field of artificial intelligence, the Convolutional Neural Networks (CNN) and Light Gradient Boosting Machine (LightGBM), to improve the accuracy of determining impact parameter by analyzing the proton spectra in transverse momentum and rapidity on the event-by-event basis. Au+Au collisions with the impact parameter of 0$\\\\leq$$b$$\\\\leq$10 fm at intermediate energies ($E_{\\\\rm lab}$=$0.2$-$1.0$ GeV$/$nucleon) are simulated with the ultrarelativistic quantum molecular dynamics (UrQMD) model to generate the proton spectra data. It is found that the average difference between the true impact parameter and the estimated one can be smaller than 0.1 fm. The LightGBM algorithm shows an improved performance with respect to the CNN on the task in this work. By using the LightGBM's visualization algorithm, one can obtain the important feature map of the distribution of transverse momentum and rapidity, which may be helpful in inferring the impact parameter or centrality in heavy-ion experiments.\"}, {\"paperId\": \"c0bff32c9b92b6cf76acc426a0be4eb0d930a300\", \"abstract\": \"Medical practitioners bring a diverse set of cognitive capabilities to bear on their work. Clinical history-taking, for example, requires that providers start with a range of previous probabilities across diseases and ask a series of dynamic questions that efficiently refine these possibilities, with each question building on an evaluation of previous answers. Decisions are also guided by past experiences, and practitioners intuitively develop causal models predicting responses to specific scenarios such as how patients might react to being told that they need a surgical procedure. As part of learning, the provider compares the actual response to expectation and refines the model to guide the next set of actions. The pursuit of artificial intelligence, in its original conception, involved training machines to replicate all these capabilities.1 Artificial intelligence, as so defined, remains in its infancy, more suited to science fiction than actual practice. But in some areas relevant to medicine\\u2014specifically pattern recognition tasks\\u2014machines have begun to narrow the gap with humans. In this issue, Huang and colleagues2 use a class of machine learning models, multilayer convolutional neural networks, to carry out a specific task: detecting wall motion abnormalities (WMAs) in echocardiogram videos. They use 3 different types of models to (1) identify the viewpoint of the video (eg, parasternal long axis); (2) \\u201csegment\\u201d or delineate the images into component regions; and (3) detect WMAs in these regions. They compare model performances to that of human readers as the gold standard and, crucially, replicate these results with data from a second institution. Just 10 years ago, the idea of a machine performing these tasks would have seemed the stuff of science fiction (albeit perhaps not the most riveting plot). Today, this represents an eminently feasible application of algorithms that have been used to automatically generate descriptive captions for YouTube videos3 or transform a modern photograph into an Impressionist rendering of the same scene.4 The common thread across these applications enables us to use this article as a starting point to describe the recent evolution of computer vision as a discipline, its application to medicine, and whether, more soberingly, such applications are likely ever to be used in practice. Five years ago, I lamented how many of the most popular machine learning algorithms have been available for decades and yet have made almost no inroads into clinical practice.5 It is worth considering whether this statement should be revised today and, if not, speculating on what is still needed. Before the late 2000s, much of machine learning focused on combining handengineered features in models to perform tasks such as image classification (eg, differentiating images of an apple from those of a banana) or regression (estimating a numeric parameter such as humidity). Hand-engineered features are \\u00a9 2020 American Heart Association, Inc.\"}, {\"paperId\": \"942b36ad49400d65f94837c4c8a29ca7a2282632\", \"abstract\": \"Advances in Machine Learning and availability of state-of-the-art computational resources, along with digitized healthcare data, have set the stage for extensive application of artificial intelligence in the realm of diagnosis, prognosis, clinical decision support, personalized treatment options, drug development, and the field of biomedicine. Here, we discuss the application of Machine Learning algorithms in patient healthcare and dermatological domains along with the ethical complexities that are involved. In scientific studies, ethical challenges were initially not addressed proportionally (as assessed by keyword counts in PubMed) and just more recently (since 2016) this has started to improve. Few pioneering countries have created regulatory guidelines around how to respect matters of (1) privacy, (2) fairness, (3) accountability, (4) transparency and (5) conflict of interest when developing novel medical Machine Learning applications. While there is a strong promise of emerging medical applications to ultimately benefit both the patients and the medical practitioners, it is important to raise awareness on the five key ethical issues and incorporate them into medical practice in the near future.\"}, {\"paperId\": \"b7dd79f2091febaa57b469f845fef8dad0efad2f\", \"abstract\": \"Ubiquitous learning (u-learning) refers to anytime and anywhere learning. U-learning has progressed to be considered a conventional teaching and learning approach in schools and is adopted to continue with the school curriculum when learners cannot attend schools for face-to-face lessons. Computer Science, namely the field of Artificial Intelligence (AI) presents tools and techniques to support the growth of u-learning and provide recommendations and insights to academic practitioners and AI researchers. Aim: The aim of this study was to conduct a meta-analysis of Artificial Intelligence works in ubiquitous learning environments and technologies to present state from the plethora of research. Method: The mining of related articles was devised according to the technique of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). The complement of included research articles was sourced from the broadly used databases, namely, Science Direct, Springer Link, Semantic Scholar, Academia, and IEEE. Results: A total of 16 scientific research publications were shortlisted for this study from 330 articles identified through database searching. Using random-effects model, the estimated pooled estimate of artificial intelligence works in ubiquitous learning environments and technologies reported was 10% (95% CI: 3%, 22%; I2 = 99.46%, P = 0.00) which indicates the presence of considerable heterogeneity. Conclusion: It can be concluded based on the experimental results from the sub group analysis that machine learning studies [18% (95% CI: 11%, 25%), I2 = 99.83%] was considerably more heterogeneous (I2 = 99.83%) than intelligent decision support systems, intelligent systems and educational data mining. However, this does not mean that intelligent decision support systems, intelligent systems and educational data mining is not efficient.\"}, {\"paperId\": \"2c6ca228a0f8edf333e012df875db7405367de16\", \"abstract\": \"I have a confession to make: everyone is talking about artificial intelligence (AI), but I had no idea what it actually meant. So I set out to find out. My research tells me AI and machine learning or machine intelligence are terms used in computer science for \\u2018intelligence\\u2019 demonstrated by machines like computers, as opposed to the \\u2018natural intelligence\\u2019 of humans and some other animals and birds (Fig. 1). The concept of intelligent machines was considered and described by science fiction writers like Isaac Asimov before they became real. Mary Shelley, daughter of feminist Mary Wollstonecraft and wife of Percy Bysshe Shelley, started writing Frankenstein, the story of an artificial being, in 1815 when she was only 18. Douglas Adams invented an Electric Monk which believed things for you in order to save you the bother of believing everything the world expected you to believe. The word intelligent comes from the Latin intelligere, meaning to understand. Is a computer that beats a chess grandmaster actually intelligent or just well programmed? Alan Turing, the brilliant English mathematician who worked at Bletchley Park during World War II breaking German ciphers, whose work led researchers to consider the possibility of building an electronic brain, suggested a better question was not whether a machine was intelligent, but whether machines could exhibit intelligent behaviour. Incidentally, despite being a war hero, Turing was prosecuted for \\u2018gross indecency\\u2019 for the then crime of being homosexual and was \\u2018chemically castrated\\u2019. In 1954, at age 41, Turing died after taking cyanide. In 2009, the British Parliament pardoned Turing and apologised to him for having persecuted him. In 1956, a young mathematician John McCarthy organised a workshop on thinking machines at Dartmouth College, New Hampshire. He proposed a 2-month, 10-man study of \\u2018artificial intelligence\\u2019, the first ever use of the term. People came and went at the workshop. Rather wonderfully, McCarthy lost his list of attendees. Enthusiasm for AI waxed and waned. The late 1970s when funding was withdrawn were called the \\u2018AI winter\\u2019, but that winter of discontent has matured into a burgeoning springtime of AI use, including in multiple areas of health care. AI is more than just computerising tasks; it builds on and develops computing capabilities. AI is about how machines \\u2018learn\\u2019 the associations between things by being repeatedly exposed to data. They are agnostic as to whether or not those associations make a priori sense or are logical. AI is increasingly being used in diagnosis, treatment and research. In dermatology, AI systems get feedback data from scanned skin biopsies and gradually improve their diagnostic accuracy based on the data, although systems are not yet sufficiently reliable to be used as the exclusive determinant of which skin lesions need biopsy. Computer technology is being used in pharmacies to assist with drug dosing and even with choice of drugs. One use in research is to aid in sample-size determination. On a positive note, if reliable, AI applications have enormous potential to improve information use in resource poor settings. A literature search for AI yields lots of articles on robots. Although there is no universally agreed definition of AI, most experts agree that a robot that simply automates a task is not AI, because AI involves significant learning. Current robotic surgery has a master\\u2013slave basis; true artificially intelligent robotic surgery is still a way off. Surgeons are rapidly training themselves in using robot-assisted (minimally invasive) surgery to operate on adults with proven or suspected malignancy, gynaecological problems including tubal sterilisation, and prostatic hypertrophy. There is less use of robots in children, although there are reports of paediatric robot-assisted surgery being used to perform pyeloplasty in the USA and to treat solid tumours in the UK, albeit with little or no evaluation as yet of the advantages and disadvantages. The names are great. The main US robotic surgical system is called the da Vinci Surgical System, a fitting tribute to the 16th century polymath, vegetarian, liberator of caged birds, so beloved of paediatricians (Fig. 2). A robot soothingly named Calmer is better at reducing pain in neonates than \\u2018facilitated tucking\\u2019. Robots are increasingly being used for the rehabilitation and education of children with disability, although research seems to Fig. 1 Artwork showing Jjaibot, an artificial intelligence robot capable of detecting human emotions such as anger, joy and happiness. doi:10.1111/jpc.14828\"}, {\"paperId\": \"1a62dcd2614f1422d274f2337551620f8c61d4b6\", \"abstract\": \"Coupling artificial intelligence with high-throughput experimentation accelerates discovery of amorphous alloys. With more than a hundred elements in the periodic table, a large number of potential new materials exist to address the technological and societal challenges we face today; however, without some guidance, searching through this vast combinatorial space is frustratingly slow and expensive, especially for materials strongly influenced by processing. We train a machine learning (ML) model on previously reported observations, parameters from physiochemical theories, and make it synthesis method\\u2013dependent to guide high-throughput (HiTp) experiments to find a new system of metallic glasses in the Co-V-Zr ternary. Experimental observations are in good agreement with the predictions of the model, but there are quantitative discrepancies in the precise compositions predicted. We use these discrepancies to retrain the ML model. The refined model has significantly improved accuracy not only for the Co-V-Zr system but also across all other available validation data. We then use the refined model to guide the discovery of metallic glasses in two additional previously unreported ternaries. Although our approach of iterative use of ML and HiTp experiments has guided us to rapid discovery of three new glass-forming systems, it has also provided us with a quantitatively accurate, synthesis method\\u2013sensitive predictor for metallic glasses that improves performance with use and thus promises to greatly accelerate discovery of many new metallic glasses. We believe that this discovery paradigm is applicable to a wider range of materials and should prove equally powerful for other materials and properties that are synthesis path\\u2013dependent and that current physiochemical theories find challenging to predict.\"}, {\"paperId\": \"4a2ef0d59e78687488f59f85addc3178ea8439bb\", \"abstract\": null}, {\"paperId\": \"5e699360578d6ca55770376cb97ad021bcbad9e6\", \"abstract\": \"Introduction Nowadays artificial intelligence (AI) is bringing tremendous new opportunities and challenges to geospatial research. Its fast development is powered by theoretical advancement, big data, computer hardware (e.g, the graphics processing unit, GPU) and high-performance computing platforms that support the development, training, and deployment of AI models within reasonable amount of time. Recent years have witnessed significant advances in Geospatial Artificial Intelligence (GeoAI), which is the integration of geospatial studies and AI, especially machine learning and deep learning methods and latest AI technologies in both academia and industry. GeoAI can be regarded as a study subject to develop intelligent computer programs to mimic the processes of human perception, spatial reasoning, and discovery about geographical phenomena and dynamics, to advance our knowledge, and to solve problems in human environmental systems and their interactions with a focus on spatial contexts and roots in geography or geographic information science (GIScience). Thus, it would require the knowledge of AI theory, programming and computation practices as well as geographic domain knowledge to be competent in GeoAI research. There have already been increasingly collaborative GeoAI studies for GIScience, remote sensing, physical environment and human society. It is a good time to provide a key reference list for educators, students, researchers, and practitioners to keep up with the latest GeoAI research topics. This bibliographical entry will first review the historical roots for AI in geography and GIScience and then list up to 10 selective recent works with annotations that briefly describe their importance for each topic of interest in the GeoAI landscape, ranging from fundamental spatial representation learning to spatial predictions and to various advancements in cartography, earth observation, social sensing and geospatial semantics.\"}, {\"paperId\": \"9e0f8080f50aab65574d68e9bd435a841c66da8c\", \"abstract\": \"T he concept of \\u2018\\u2018machine learning\\u2019\\u2019 and \\u2018\\u2018artificial intelligence (AI)\\u2019\\u2019 within medicine may elicit a different response based on the individual. For some, machine learning and AI in medicine may be associated with technology taking the place of humanphysician led medical expertise. For others, the idea of machine learning in medicine is seen as an example of technology advancement. Regardless, it is a concept that is gaining traction and attention because of the abundance of possibilities related to this technology. A search in PubMed.gov garners over 45,000 search results for \\u2018\\u2018machine learning,\\u2019\\u2019 the majority being published within the last 5 years. If you use an internet search engine, you will obtain even more search results for this term. So what is machine learning? Machine learning is based on algorithms that utilize statistics to identify patterns. It provides a system that can automatically learn from experience \\u2013 but experience based on data and not based on human experience or reaction. This technology is heavily employed in many day-to-day services, including video and music streaming sites, social media sites, and voice assistant services. In these instances, data are collected and machine learning predicts what you will want next. So how can machine learning be applied to pediatric gastroenterology? In the January 2020 issue of Journal of Pediatric Gastroenterology and Nutrition (JPGN), Patel et al provided an in-depth review of AI and machine learning applied to gastrointestinal diagnostics. This review discussed important terminology within the practice of machine learning that is essential to understanding the science behind this technology (1). As discussed within this review, AI has applications within gastrointestinal endoscopy (polyp detection, celiac disease, IBD), endomicroscopy, video capsule endoscopy, and radiographic imaging interpretation (1). In this month\\u2019s issue of JPGN, Dhaliwal et al explore one such possible application of machine learning within pediatric inflammatory bowel disease (IBD) \\u2013 differentiation of colonic IBD. Differentiation of ulcerative colitis (UC) from Crohn disease is a difficult process and has been a topic of multiple publications itself, including a 2007 clinical report from NASPGHAN and CCFA (2), and more recently, diagnostic criteria published by the Pediatric IBD (PIBD) Porto group of ESPGHAN (3). As mentioned by Dhaliwal et al, the PIBD classification algorithm focuses on features pointing away from a diagnosis of UC. In their machine-learning model, the authors evaluated 73 patients and 28 specific disease features. Their model not only incorporated features from the PIBD algorithm but also included disease features typical of UC (primarily histologic features\\u2014see Table 1 of publication). Through machine-learning analysis of disease features, they were able to identify 7 features (3 histologic and 4 endoscopic\\u2014see Table 2 of publication) that accurately distinguished between UC and colonic CD. As my music streaming service often reminds me, interaction (thus submitting more data) within the service improves algorithm accuracy. Similar could be said for this study, which represented a specific patient population from a single center. The power of this publication may not be its immediate generalization to pediatric IBD in its entirety. The concept and potential performance of an AIbased diagnostic algorithm within pediatric IBD is, however, promising. AI and machine learning have the potential to analyze large data sets, provide meaningful data-driven algorithms free of human bias, and could contribute to data-driven standard care practices. Variation in care is a known issue throughout medicine, including pediatric IBD (4). Utilizing models, such as this may help us in our quest to become better physicians, making informed diagnoses and decisions that lead to improved patient outcomes. The study by Dhaliwal et al is a step in this direction. Ideas and models such as this are likely to shape the practice of medicine in the future. Human emotion, reaction, and experience should never be completely removed from the art of medicine but utilization of technology can improve the science of medicine and further our ability to care for our patients. Change is on the horizon; count me in the camp that views AI and machine learning as important technology innovation.\"}, {\"paperId\": \"302709061a86e34ced9a744db6b922fba2866059\", \"abstract\": null}, {\"paperId\": \"4cbaa4cd3f5b5203c022b0c2751aa60d5c9defba\", \"abstract\": \"Artificial intelligence (A.I.) is moving into every aspect of our lives: selecting the online advertisements that we see, recognizing our voices, and providing automated answers when we speak to customer service helpline chatbots. Machine learning, a form of A.I. that involves computers learning and improving without being explicitly programmed, is now considered a fully mature technology and is being used across a wide variety of industries. For example, the banking industry uses machine-learning algorithms to comb through vast amounts of transactional data to combat credit card fraud, while the automotive industry uses machine learning to predict when parts might fail and when servicing will be required.1 Medicine is traditionally slower than other industries to develop and adopt new technologies, but A.I. is beginning to enter clinical practice to complement the expertise of health care practitioners in fields such as radiology.2 A.I. and machine learning bear similar promise in anesthesiology, but many challenges remain in incorporating machine learning into daily clinical practice.3 To date, most A.I. projects published in the medical literature have focused on the development and validation of machine-learning models and algorithms. A small minority of these reports describe a project that has been integrated successfully into daily clinical workflow, and few, if any A.I. efforts have demonstrated measurable improvements in patient outcomes. On January 30, 2020, a PubMed search of peerreviewed publications yielded 48 original investigation articles when using combinations of the Medical Subject Headings (MeSH) and free-text terms \\u201canesthesia,\\u201d \\u201cartificial intelligence,\\u201d and \\u201cmachine learning\\u201d for 2015 onwards, sifted for relevance. Only 13 of these articles were published in clinical journals, while the remaining 35 articles were found in engineering and basic science publications. In contrast, there were 18 editorials and narrative reviews in anesthesiology journals alone (Supplemental Digital Content, Table 1, http://links.lww.com/AA/D53, lists the details of the literature search). This imbalance of original and secondary articles would suggest that A.I. in anesthesia may be approaching the Peak of Inflated Expectations described by the Gartner Hype Cycle (Figure).4 The cycle describes the phases of public interest recognized during the development of new technology. A breakthrough Innovation Trigger leads to proofs-of-concept and significant publicity, but initially, no usable or commercially viable products exist. As publicity continues, development does not yet meet the expectations of the wider field, attitudes begin to harden, and general interest wanes in a Trough of Disillusionment. Slowly, the technology evolves along the Slope of Enlightenment and a greater number of examples of benefit emerge and are understood. Eventually, mainstream adoption takes off until the Plateau of Productivity is reached. At this point, broad applicability and relevance are evident Artificial Intelligence in Anesthesiology: Hype, Hope, and Hurdles\"}, {\"paperId\": \"78a0158a5bb51786142f79f494648f51070da327\", \"abstract\": \"Background Significant investments and advances in health care technologies and practices have created a need for digital and data-literate health care providers. Artificial intelligence (AI) algorithms transform the analysis, diagnosis, and treatment of medical conditions. Complex and massive data sets are informing significant health care decisions and clinical practices. The ability to read, manage, and interpret large data sets to provide data-driven care and to protect patient privacy are increasingly critical skills for today\\u2019s health care providers. Objective The aim of this study is to accelerate the appropriate adoption of data-driven and AI-enhanced care by focusing on the mindsets, skillsets, and toolsets of point-of-care health providers and their leaders in the health system. Methods To accelerate the adoption of AI and the need for organizational change at a national level, our multistepped approach includes creating awareness and capacity building, learning through innovation and adoption, developing appropriate and strategic partnerships, and building effective knowledge exchange initiatives. Education interventions designed to adapt knowledge to the local context and address any challenges to knowledge use include engagement activities to increase awareness, educational curricula for health care providers and leaders, and the development of a coaching and practice-based innovation hub. Framed by the Knowledge-to-Action framework, we are currently in the knowledge creation stage to inform the curricula for each deliverable. An environmental scan and scoping review were conducted to understand the current state of AI education programs as reported in the academic literature. Results The environmental scan identified 24 AI-accredited programs specific to health providers, of which 11 were from the United States, 6 from Canada, 4 from the United Kingdom, and 3 from Asian countries. The most common curriculum topics across the environmental scan and scoping review included AI fundamentals, applications of AI, applied machine learning in health care, ethics, data science, and challenges to and opportunities for using AI. Conclusions Technologies are advancing more rapidly than organizations, and professionals can adopt and adapt to them. To help shape AI practices, health care providers must have the skills and abilities to initiate change and shape the future of their discipline and practices for advancing high-quality care within the digital ecosystem. International Registered Report Identifier (IRRID) PRR1-10.2196/30940\"}, {\"paperId\": \"6ce6efbc3e2b5ac86cb2f51881c7c2a88493b939\", \"abstract\": \"1 CAS Key Laboratory of Mental Health, Institute of Psychology, Chinese Academy of Sciences, Beijing 100101, China 2 Department of Psychology, University of Chinese Academy of Sciences, Beijing 100049, China 3 School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen 518000, Guangdong, China  Electroencephalogram (EEG) is an important technique for measuring population\\u2010level electrical activity arising from the human brain. Due to its exquisite temporal sensitivity and implementation simplicity, EEG has been widely applied to dynamically evaluate the function of the brain. Being responded to a specific sensory, cognitive, or motor event, the changes of EEG signals give rise to evoked potentials (EPs) and event\\u2010related potentials (ERPs), which are highly associated with different brain functions, e.g., perception, emotion, and cognition. These advances make the EEG technique popularly used in various basic and clinical applications. To make full use of the EEG technique, signal processing and machine learning methods are crucial in the extraction of information for better understan\\u2010 ding the cerebral functioning. Particularly, in this age of artificial intelligence (AI), rapidly developed AI methods, such as convolutional neural networks and recurrent neural networks, have been applied to EEG signals and have achieved promising performance in many real applications. As a consequence, the field of EEG signal processing has undergone significant growth in the last few years, and the scope and range of practical applications of EEG, such as brain\\u2013computer interface (BCI), are steadily increasing. For this reason, the special issue aims to provide a collection of papers discussing the conceptual and methodological innovations as well as practical applications of the EEG techniques. This special session has included seven review papers contributed by experts in this interdisciplinary field, and all authors have worked in the fields of EEG processing methods and applications for many years. First of all, Li [1] shared his insightful and constructive thoughts on EEG signal analysis and classification. Specifically, he focused on several important and emerging topics in EEG processing, such as brain connectivity, tensor decomposition, multi\\u2010modality, deep learning, big data, and naturalistic experiments. These topics, particularly those AI\\u2010related topics, are both crucial and promising for the future advancement of EEG signal analysis and classification. Next, this special issue presented several papers concerning the applications of EEG in psychology, emotion recognition, and BCI. One important and conventional application field of EEG is psychology, in which EEG has been extensively used to decode the psychological  Address correspondence to Li Hu, huli@psych.ac.cn; and Zhiguo Zhang, zgzhang@szu.edu.cn\"}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 700, \"next\": 800, \"data\": [{\"paperId\": \"3390c4c6f31ca43063a8ed3ea857d8da89114789\", \"abstract\": \"Despite efforts to integrate research across different subdisciplines of biology, the scale of integration remains limited. We hypothesize that future generations of Artificial Intelligence (AI) technologies specifically adapted for biological sciences will help enable the reintegration of biology. AI technologies will allow us not only to collect, connect and analyze data at unprecedented scales, but also to build comprehensive predictive models that span various subdisciplines. They will make possible both targeted (testing specific hypotheses) and untargeted discoveries. AI for biology will be the cross-cutting technology that will enhance our ability to do biological research at every scale. We expect AI to revolutionize biology in the 21st century much like statistics transformed biology in the 20th century. The difficulties, however, are many, including data curation and assembly, development of new science in the form of theories that connect the subdisciplines, and new predictive and interpretable AI models that are more suited to biology than existing machine learning and AI techniques. Development efforts will require strong collaborations between biological and computational scientists. This white paper provides a vision for AI for Biology and highlights some challenges.\"}, {\"paperId\": \"c6eac00d7b32a5eaf1203a2ac7b05a0ad1c77831\", \"abstract\": null}, {\"paperId\": \"803e277aa57f45bbe94a0b42d6c82875b20220ef\", \"abstract\": \"\\nEducation data scientists, learning engineers and precision education specialists are new experts in knowledge production in educational research. By bringing together data science methodologies and advanced artificial intelligence (AI) systems with disciplinary expertise from the psychological, biological and brain sciences, they are building a new field of AI-based learning science. This article presents an examination of how education research is being remade as an experimental data-intensive science. AI is combining with learning science in new \\u2018digital laboratories\\u2019 where ownership over data, and power and authority over educational knowledge production, are being redistributed to research assemblages of computational machines and scientific expertise.\"}, {\"paperId\": \"4328d31ebdc4964972d331fba6e37b482ef8dc25\", \"abstract\": \"Artificial intelligence (AI) is a field of data science pertaining to advanced computing machines capable of learning from data and interacting with the human world. Early diagnosis and diagnostics, self\\u2010care, prevention and wellness, clinical decision support, care delivery, and chronic care management have been identified within the healthcare areas that could benefit from introducing AI. In pediatric allergy research, the recent developments in AI approach provided new perspectives for characterizing the heterogeneity of allergic diseases among patients. Moreover, the increasing use of electronic health records and personal healthcare records highlighted the relevance of AI in improving data quality and processing and setting\\u2010up advanced algorithms to interpret the data. This review aimed to summarize current knowledge about AI and discuss its impact on the diagnostic framework of pediatric allergic diseases such as eczema, food allergy, and respiratory allergy, along with the future opportunities that AI research can offer in this medical area.\"}, {\"paperId\": \"b5bf68553e1eecf03d0a0e6f13c3f65d073417f3\", \"abstract\": \"Most current affect scales and sentiment analysis on written text focus on quantifying valence/sentiment, the primary dimension of emotion. Distinguishing broader, more complex negative emotions of similar valence is key to evaluating mental health. We propose a semi-supervised machine learning model, DASentimental, to extract depression, anxiety, and stress from written text. We trained DASentimental to identify how N = 200 sequences of recalled emotional words correlate with recallers\\u2019 depression, anxiety, and stress from the Depression Anxiety Stress Scale (DASS-21). Using cognitive network science, we modeled every recall list as a bag-of-words (BOW) vector and as a walk over a network representation of semantic memory\\u2014in this case, free associations. This weights BOW entries according to their centrality (degree) in semantic memory and informs recalls using semantic network distances, thus embedding recalls in a cognitive representation. This embedding translated into state-of-the-art, cross-validated predictions for depression (R = 0.7), anxiety (R = 0.44), and stress (R = 0.52), equivalent to previous results employing additional human data. Powered by a multilayer perceptron neural network, DASentimental opens the door to probing the semantic organizations of emotional distress. We found that semantic distances between recalls (i.e., walk coverage), was key for estimating depression levels but redundant for anxiety and stress levels. Semantic distances from \\u201cfear\\u201d boosted anxiety predictions but were redundant when the \\u201csad\\u2013happy\\u201d dyad was considered. We applied DASentimental to a clinical dataset of 142 suicide notes and found that the predicted depression and anxiety levels (high/low) corresponded to differences in valence and arousal as expected from a circumplex model of affect. We discuss key directions for future research enabled by artificial intelligence detecting stress, anxiety, and depression in texts.\"}, {\"paperId\": \"66e70d7d6af0c51b9b907d9c4a8de0dfca70b1a7\", \"abstract\": \"There have been tremendous advances in artificial intelligence (AI) and machine learning (ML) within the past decade, especially in the application of deep learning to various challenges. These include advanced competitive games (such as Chess and Go), self-driving cars, speech recognition, and intelligent personal assistants. Rapid advances in computer vision for recognition of objects in pictures have led some individuals, including computer science experts and health care system experts in machine learning, to make predictions that ML algorithms will soon lead to the replacement of the radiologist. However, there are complex technological, regulatory, and medicolegal obstacles facing the implementation of machine learning in radiology that will definitely preclude replacement of the radiologist by these algorithms within the next two decades and beyond. While not a comprehensive review of machine learning, this article is intended to highlight specific features of machine learning which face significant technological and health care systems challenges. Rather than replacing radiologists, machine learning will provide quantitative tools that will increase the value of diagnostic imaging as a biomarker, increase image quality with decreased acquisition times, and improve workflow, communication, and patient safety. In the foreseeable future, we predict that today's generation of radiologists will be replaced not by ML algorithms, but by a new breed of data science-savvy radiologists who have embraced and harnessed the incredible potential that machine learning has to advance our ability to care for our patients. In this way, radiology will remain a viable medical specialty for years to come.\"}, {\"paperId\": \"9f1804fb51570686108d2d94badb3baf84f8052d\", \"abstract\": \"The use of big data and AI to help guide clinical decisions is a central aspect of precision medicine initiatives. Yet, buzz words like big data and AI mystify many clinicians and biomedical researchers. Their widespread use in other industries and initial clinical applications can serve as a guide to a clearer understanding of what they are and are not, their promise, and their potential peril. Many common terms mean different things in different contexts. AI typically refers to a machine with human capabilities; machine learning (ML) may refer either to a set of computational and statistical tools for identifying relationships in data or to the use of such tools to make predictions based on data; deep neural networks are a particular type of ML whose success at tasks, such as image recognition, has led to them being referred to as AI or deep learning. Developing most AI, ML, and deep neural network tools requires access to big data\\u2014another concept with multiple meanings. For data scientists, it implies using more data than one computer can handle with significant attendant analytical and computational challenges and opportunities; for clinicians and biomedical researchers, it refers to complex datasets with numerous structured and unstructured data fields, such as those typically found in electronic health records. Figure 1 illustrates the relationship between AI, ML, deep neural networks, and big data. Reinforcement learning is a notable exception to the use of big data to train AI. It is an approach to building AI tools based only on feedback. For example, DeepMind program AlphaGo Zero became the most powerful Go program in the world solely by playing against itself. Thus far, reinforcement learning in health care has been developed using historical data representing decisions and feedback. If (when) AI starts to make and test clinical decisions, algorithms will have the capacity to learn on their own. The widespread use of AI by companies such as Amazon and Google offers lessons for health care. Such industries have applied AI to enable smart electronic assistants, facial recognition software, and self-driving cars; the use and misuse of such applications have raised concerns about safety, fairness, and privacy. For example, data are increasingly being used to predict consumer behavior and preferences. Do I want the computers at a department store to know that I am pregnant before my family does? Will I be charged more for plane tickets if I buy them using an iPhone? There are numerous proof-of-concept examples of ML in health care. It has been applied to clinical risk prediction and to learning from the large volume of data generated by electronic health records and other large datasets. Notable recent examples include the classification of a picture of a nevus as malignant or benign, of a retinal fundus image to predict the risk of cardiovascular disease, and of using histopathology specimens to predict prognosis in lung cancer. In everyday clinical cardiology, ML has been used for interpreting automated ECG, determine left ventricular ejection fractions from echocardiography, and scoring coronary artery calcium scans. A form of AI known as computer vision is being developed to prevent clinical errors and improve patient safety. Although relatively few of these methods have been implemented in routine clinical practice, and none on a large scale, they demonstrate the promise of AI in clinical medicine and biomedical research. AI can be used to automate and simplify tasks too onerous or time-consuming for a single person or team to perform. What if we could use voice recognition software as a medical scribe to allow more doctoring and less documenting in patient exam rooms? What if we could aggregate a personalized cohort to ask simple, clinically relevant questions with a few clicks of our mouse? Research teams at Stanford University have developed a variety of approaches to leveraging data found in the electronic health record to help clinicians make decisions based on a patient\\u2019s unique characteristics. One such approach allows a physician considering a decision to call for an informatics consult. This triggers an ML algorithm that identifies patients similar to the one being considered and presents their treatments and outcomes. Similar tools could be used to quickly identify patients who meet criteria for entry into randomized clinical trials, dramatically cutting enrollment and recruitment costs. In translational research, ML can efficiently identify candidate molecules for drug development. ML can better risk stratify populations underrepresented in our available evidence base and identify patterns and relationships not captured by traditional statistical methods. The opinions expressed in this article are not necessarily those of the editors or of the American Heart Association. From the Division of Cardiovascular Medicine, Cardiovascular Institute (F.R., R.A.H.), Department of Medicine (F.R., R.A.H.), and Department of Management Science and Engineering (D.S.), Stanford University, CA. Correspondence to Fatima Rodriguez, MD, MPH, Division of Cardiovascular Medicine, Stanford University School of Medicine, 870 Quarry Rd, Falk CVRC, Stanford, CA 94305\\u20135406. Email frodrigu@ stanford.edu Viewpoints\"}, {\"paperId\": \"49e8918f6c5a5943bf52efd9fefe96eae0182f2f\", \"abstract\": \"The technological progress in the field of artificial intelligence (AI) has brought new opportunities and challenges to the intelligent development and innovative research in geospatial related fields. Geospatial artificial intelligence (GeoAI) refers to the interdisciplinary research direction that combines geography, earth science and artificial intelligence, and seeks to solve major scientific and engineering problems in human-environmental interaction systems through the research and development of spatial intelligence in machines to improve the dynamic perception, intelligent reasoning and knowledge discovery of geographic phenomena and earth science processes. This paper briefly summarizes the historical origins of GeoAI development, introduces spatially explicit and implicit AI models, reviews recent GeoAI research and applications(including spatial representation learning, spatiotemporal prediction and spatial interpolation, monitoring of geographic resources and environment, cartography, and geo-text data semantic analysis), and identifies several potential research challenges and directions for the future development of GeoAI.\"}, {\"paperId\": \"d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f\", \"abstract\": \"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth\\u2019s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth\\u2019s behavior and by the inaccessibility of nearly all of Earth\\u2019s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.\"}, {\"paperId\": \"079c848e7c08e2c58e8c6f5ad98b587964e97f3b\", \"abstract\": \"International mass digitization efforts through infrastructures like the European Distributed System of Scientific Collections (DiSSCo), the US resource for Digitization of Biodiversity Collections (iDigBio), the National Specimen Information Infrastructure (NSII) of China, and Australia\\u2019s digitization of National Research Collections (NRCA Digital) make geo- and biodiversity specimen data freely, fully and directly accessible. \\n Complementary, overarching infrastructure initiatives like the European Open Science Cloud (EOSC) were established to enable mutual integration, interoperability and reusability of multidisciplinary data streams including biodiversity, Earth system and life sciences (De Smedt et al. 2020). \\n Natural Science Collections (NSC) are of particular importance for such multidisciplinary and internationally linked infrastructures, since they provide hard scientific evidence by allowing direct traceability of derived data (e.g., images, sequences, measurements) to physical specimens and material samples in NSC. \\n To open up the large amounts of trait and habitat data and to link these data to digital resources like sequence databases (e.g., ENA), taxonomic infrastructures (e.g., GBIF) or environmental repositories (e.g., PANGAEA), proper annotation of specimen data with rich (meta)data early in the digitization process is required, next to bridging technologies to facilitate the reuse of these data. \\n This was addressed in recent studies (Younis et al. 2018, Younis et al. 2020), where we employed computational image processing and artificial intelligence technologies (Deep Learning) for the classification and extraction of features like organs and morphological traits from digitized collection data (with a focus on herbarium sheets).\\n However, such applications of artificial intelligence are rarely\\u2014this applies both for (sub-symbolic) machine learning and (symbolic) ontology-based annotations\\u2014integrated in the workflows of NSC\\u2019s management systems, which are the essential repositories for the aforementioned integration of data streams.\\n This was the motivation for the development of a Deep Learning-based trait extraction and coherent Digital Specimen (DS) annotation service providing \\u201cMachine learning as a Service\\u201d (MLaaS) with a special focus on interoperability with the core services of DiSSCo, notably the DS Repository (nsidr.org) and the Specimen Data Refinery (Walton et al. 2020), as well as reusability within the data fabric of EOSC. \\n Taking up the use case to detect and classify regions of interest (ROI) on herbarium scans, we demonstrate a MLaaS prototype for DiSSCo involving the digital object framework, Cordra, for the management of DS as well as instant annotation of digital objects with extracted trait features (and ROIs) based on the DS specification openDS (Islam et al. 2020).\\n Source code available at: https://github.com/jgrieb/plant-detection-service\"}, {\"paperId\": \"9c2f58bd781bd0ae5d329b10cb47bfbc81f8ab23\", \"abstract\": null}, {\"paperId\": \"2d4cdce1f45c28393955460727904756e7a079ca\", \"abstract\": \"Purpose of review Artificial intelligence has pervasively transformed many industries and is beginning to shape medical practice. New use cases are being identified in subspecialty domains of medicine and, in particular, application of artificial intelligence has found its way to the practice of allergy-immunology. Here, we summarize recent developments, emerging applications and obstacles to realizing full potential. Recent findings Artificial/augmented intelligence and machine learning are being used to reduce dimensional complexity, understand cellular interactions and advance vaccine work in the basic sciences. In genomics, bioinformatic methods are critical for variant calling and classification. For clinical work, artificial intelligence is enabling disease detection, risk profiling and decision support. These approaches are just beginning to have impact upon the field of clinical immunology and much opportunity exists for further advancement. Summary This review highlights use of computational methods for analysis of large datasets across the spectrum of research and clinical care for patients with immunological disorders. Here, we discuss how big data methods are presently being used across the field clinical immunology.\"}, {\"paperId\": \"79dcd93bc8909a30511aceaaf0cb7a7b781ea4cd\", \"abstract\": \"Summary: Medical decision-making is increasingly based on quantifiable data. From the moment patients come into contact with the health care system, their entire medical history is recorded electronically. Whether a patient is in the operating room or on the hospital ward, technological advancement has facilitated the expedient and reliable measurement of clinically relevant health metrics, all in an effort to guide care and ensure the best possible clinical outcomes. However, as the volume and complexity of biomedical data grow, it becomes challenging to effectively process \\u201cbig data\\u201d using conventional techniques. Physicians and scientists must be prepared to look beyond classic methods of data processing to extract clinically relevant information. The purpose of this article is to introduce the modern plastic surgeon to machine learning and computational interpretation of large data sets. What is machine learning? Machine learning, a subfield of artificial intelligence, can address clinically relevant problems in several domains of plastic surgery, including burn surgery; microsurgery; and craniofacial, peripheral nerve, and aesthetic surgery. This article provides a brief introduction to current research and suggests future projects that will allow plastic surgeons to explore this new frontier of surgical science.\"}, {\"paperId\": \"52564bfc0eb99dd753c023b328ee60c45f058af7\", \"abstract\": null}, {\"paperId\": \"b5d78391f9a4a6b60cc3ef68eb25be3fcd6730d7\", \"abstract\": \"Background: Language is a valuable source of clinical information in Alzheimer\\u2019s disease, as it declines concurrently with neurodegeneration. Consequently, speech and language data have been extensively studied in connection with its diagnosis. Objective: Firstly, to summarize the existing findings on the use of artificial intelligence, speech, and language processing to predict cognitive decline in the context of Alzheimer\\u2019s disease. Secondly, to detail current research procedures, highlight their limitations, and suggest strategies to address them. Methods: Systematic review of original research between 2000 and 2019, registered in PROSPERO (reference CRD42018116606). An interdisciplinary search covered six databases on engineering (ACM and IEEE), psychology (PsycINFO), medicine (PubMed and Embase), and Web of Science. Bibliographies of relevant papers were screened until December 2019. Results: From 3,654 search results, 51 articles were selected against the eligibility criteria. Four tables summarize their findings: study details (aim, population, interventions, comparisons, methods, and outcomes), data details (size, type, modalities, annotation, balance, availability, and language of study), methodology (pre-processing, feature generation, machine learning, evaluation, and results), and clinical applicability (research implications, clinical potential, risk of bias, and strengths/limitations). Conclusion: Promising results are reported across nearly all 51 studies, but very few have been implemented in clinical research or practice. The main limitations of the field are poor standardization, limited comparability of results, and a degree of disconnect between study aims and clinical applications. Active attempts to close these gaps will support translation of future research into clinical practice.\"}, {\"paperId\": \"c3c9a9c5eb3446db95a703fce3271dcba149d3d6\", \"abstract\": \"As systems evolve over time, their natural tendency is to become increasingly more complex. Studies in the field of complex systems have generated new perspectives on the application of management strategies in health systems. Much of this research appears as a natural extension of the cross-disciplinary field of systems theory. Since writing my 1st article for Managing Organizational Complexity in 2004, much has happened to further our understanding of complexity in healthcare systems. The growth of new computational methods in the fields of data science and data analytics has allowed scientists to identify signals or patterns in large complex data sets (big data) that in the past were seemingly hidden. Rather than relying on historical statistical methods to infer outcomes, these advanced methods combined with increased computer processing power allow machines to learn the structure of data and create artificial intelligence (AI). In our ongoing efforts to find solutions for complex healthcare problems, AI is becoming more and more an accepted method. The purpose of this edition of Managing Organizational Complexity is to define AI and machine learning, discuss the recent resurgence of AI, and then provide examples of how AI can provide value to healthcare with an emphasis on nursing.\"}, {\"paperId\": \"50a483975abba83f66b4d621f70f2d2128b93137\", \"abstract\": \"Artificial intelligence (AI) is a major branch of computer science that is fruitfully used for analyzing complex medical data and extracting meaningful relationships in datasets, for several clinical aims. Specifically, in the brain care domain, several innovative approaches have achieved remarkable results and open new perspectives in terms of diagnosis, planning, and outcome prediction. In this work, we present an overview of different artificial intelligent techniques used in the brain care domain, along with a review of important clinical applications. A systematic and careful literature search in major databases such as Pubmed, Scopus, and Web of Science was carried out using \\u201cartificial intelligence\\u201d and \\u201cbrain\\u201d as main keywords. Further references were integrated by cross-referencing from key articles. 155 studies out of 2696 were identified, which actually made use of AI algorithms for different purposes (diagnosis, surgical treatment, intra-operative assistance, and postoperative assessment). Artificial neural networks have risen to prominent positions among the most widely used analytical tools. Classic machine learning approaches such as support vector machine and random forest are still widely used. Task-specific algorithms are designed for solving specific problems. Brain images are one of the most used data types. AI has the possibility to improve clinicians' decision-making ability in neuroscience applications. However, major issues still need to be addressed for a better practical use of AI in the brain. To this aim, it is important to both gather comprehensive data and build explainable AI algorithms.\"}, {\"paperId\": \"eb8d90ab1ac54f08df53a57a8c5948356c3ba175\", \"abstract\": null}, {\"paperId\": \"cc7c4864f6a9bb7f5489aa17546c1b0b3de59993\", \"abstract\": \"Machine learning (ML) is a subset of artificial intelligence that enables to take decision based on data. Artificial intelligence makes possible to integrate ML capabilities into data driven modelling systems in order to bridge the gaps and lessen demands on human experts in oceanographic research .ML algorithms have proven to be a powerful tool for analysing oceanographic and climate data with high accuracy in efficient way. ML has a wide spectrum of real time applications in oceanography and Earth sciences. This study has explained in simple way the realistic uses and applications of major ML algorithms. The main application of machine learning in oceanography is prediction of ocean weather and climate, habitat modelling and distribution, species identification, coastal water monitoring, marine resources management, detection of oil spill and pollution and wave modelling.\"}, {\"paperId\": \"8503c9ec049edaaf513ba0f4a32393ae43d9650e\", \"abstract\": \"Machine learning, an important branch of artificial intelligence, is increasingly being applied in sciences such as forest ecology. Here, we review and discuss three commonly used methods of machine learning (ML) including decision-tree learning, artificial neural network, and support vector machine and their applications in four different aspects of forest ecology over the last decade. These applications include: (i) species distribution models, (ii) carbon cycles, (iii) hazard assessment and prediction, and (iv) other applications in forest management. Although ML approaches are useful for classification, modeling, and prediction in forest ecology research, further expansion of ML technologies is limited by the lack of suitable data and the relatively \\u201chigher threshold\\u201d of applications. However, the combined use of multiple algorithms and improved communication and cooperation between ecological researchers and ML developers still present major challenges and tasks for the betterment of future ecological research. We suggest that future applications of ML in ecology will become an increasingly attractive tool for ecologists in the face of \\u201cbig data\\u201d and that ecologists will gain access to more types of data such as sound and video in the near future, possibly opening new avenues of research in forest ecology.\"}, {\"paperId\": \"1c25518c7036f984f0efddc25e76cf5d926f2a44\", \"abstract\": \"Machine Learning (ML) furnishes the ability of insights on automatic recognizing patterns and determining the prediction models for the structured and unstructured data even in the absence of explicit programming instructions. Today, the impact of Artificial Intelligence (AI) has grown up to several heights, ranging from Life sciences to the Management techniques. The integration of ML led to reduce or eliminate the errors in the prediction, classification and simulation models. The objective of the paper is to represent the ML objectives, explore the various ML techniques and algorithms with its applications in the various fields.\"}, {\"paperId\": \"54e7a9d699cb9a0e768f7a8cbb114d256d213620\", \"abstract\": null}, {\"paperId\": \"c51748e245305185b89892d40d8d551b13eababf\", \"abstract\": \"ATLAS OF AI: Power, Politics, and the Planetary Costs of Artificial Intelligence by Kate Crawford. New Haven, CT: Yale University Press, 2021. 336 pages. Hardcover; $28.00. ISBN: 9780300209570. *Atlas of AI: Power, Politics, and the Planetary Costs of Artificial Intelligence is Kate Crawford's analysis of the state of the AI industry. A central idea of her book is the importance of redefining Artificial Intelligence (AI). She states, \\\"I've argued that there is much at stake in how we define AI, what its boundaries are, and who determines them: it shapes what can be seen and contested\\\" (p. 217). *My own definition of AI goes something like this: I\\u00ac\\u2020imagine a future where I'm sitting in a cafe drinking coffee with my friends, but in this future, one of my friends is a robot, who like me is trying to make a living in this world. A future where humans and robots live in harmony. Crawford views this definition as mythological: \\\"These mythologies are particularly strong in the field of artificial intelligence, where the belief that human intelligence can be formalized and reproduced by machines has been axiomatic since the mid-twentieth century\\\" (p.\\u00ac\\u20205). I do not know if my definition of artificial intelligence can come true, but I am enjoying the process of building, experimenting, and dreaming. *In her book, she asks me to consider that I may be unknowingly participating, as she states, in \\\"a material product of colonialism, with its patterns of extraction, conflict, and environmental destruction\\\" (p. 38). The book's subtitle illuminates the purpose of the book: specifically, the power, politics, and planetary costs of usurping artificial intelligence. Of course, this is not exactly Crawford's subtitle, and this is where I both agree and disagree with her. The book's subtitle is actually Power, Politics, and the Planetary Costs of Artificial Intelligence. In my opinion, AI is more the canary in the coal mine. We can use the canary to detect the poisonous gases, but we cannot blame the canary for the poisonous gas. It risks missing the point. Is AI itself to be feared? Should we no longer teach or learn AI? Or is this more about how we discern responsible use and direction for AI technology? *There is another author who speaks to similar issues. In Weapons of Math Destruction, Cathy O'Neil states it this way, \\\"If we had been clear-headed, we all would have taken a step back at this point to figure out how math had been misused ... But instead ... new mathematical techniques were hotter than ever ... A computer program could speed through thousands of resumes or loan applications in a second or two and sort them into neat lists, with the most promising candidates on top\\\" (p. 13). *Both Crawford and O'Neil point to human flaws that often lead to well-intentioned software developers creating code that results in unfair and discriminatory decisions. AI models encode unintended human biases that may not evaluate candidates as fairly as we would expect, yet there is a widespread notion that we can trust the algorithm. For example, the last time you registered an account on a website, did you click the checkbox confirming that \\\"yes, I read the disclaimer\\\" even though you did not? When we click \\\"yes\\\" we are accepting this disclaimer and placing trust in the software. Business owners place trust in software when they use it to make predictions. Engineers place trust in their algorithms when they write software without rigorous testing protocols. I\\u00ac\\u2020am just as guilty. *Crawford suggests that AI is often used in ways that are harmful. In the Atlas of AI we are given a tour of how technology is damaging our world: strip mining, labor injustice, the misuse of personal data, issues of state and power, to name a few of the concerns Crawford raises. The reality is that AI is built upon existing infrastructure. For example, Facebook, Instagram, YouTube, Amazon, TikTok have been collecting our information for profit even before AI became important to them. The data centers, CPU houses, and worldwide network infrastructure were already in place to meet consumer demand and geopolitics. But it is true that AI brings new technologies to the table, such as automated face recognition and decision tools to compare prospective employment applicants with diverse databases and employee monitoring tools that can make automatic recommendations. Governments, militaries, and intelligence agencies have taken notice. As invasion of privacy and social justice concerns emerge, Crawford calls us to consider these issues carefully. *Reading Crawford's words pricked my conscience, convicting me to reconsider my erroneous ways. For big tech to exist, to supply what we demand, it needs resources. She walks us through the many resources the technology industry needs to provide what we want, and AI is the \\\"new kid on the block.\\\" This book is not about AI, per se; it is instead about the side effects of poor business/research practices, opportunist behavior, power politics, and how these behaviors not only exploit our planet but also unjustly affect marginalized people. The AI industry is simply a new example of this reality: data mining, low wages to lower costs, foreign workers with fewer rights, strip mining, relying on coal and oil for electricity (although some tech companies have made strides to improve sustainability). This sounds more like a parable about the sins of the tech industry than a critique about the dangers of AI. *Could the machine learning community, like the inventors of dynamite who wanted to simply help railroads excavate tunnels, be unintentionally causing harm? Should we, as a community, be on the lookout for these potential harms? Do we have a moral responsibility? Maybe the technology sector needs to look more inwardly to ensure that process efficiency and cost savings are not elevated as most important. *I did not agree with everything that Crawford classified as AI, but I do agree that as a community we are responsible for our actions. If there are injustices, then this should be important to us. In particular, as people of faith, we should heed the call of Micah 6:8 to act justly in this world, and this includes how we use AI. *Reviewed by Joseph Vybihal, Professor of Computer Science, McGill University, Montreal, PQ H3A 0G4.\"}, {\"paperId\": \"518f15c4e7e357f16d2fc68e65a4c8c3bf9636ba\", \"abstract\": \"Summary Objectives: To introduce and summarize current research in the field of Public Health and Epidemiology Informatics. Methods: The 2018 literature concerning public health and epidemiology informatics was searched in PubMed and Web of Science, and the returned references were reviewed by the two section editors to select 15 candidate best papers. These papers were then peer-reviewed by external reviewers to give the editorial team an enlightened selection of the best papers. Results: Among the 805 references retrieved from PubMed and Web of Science, three were finally selected as best papers. All three papers are about surveillance using digital tools. One study is about the surveillance of flu, another about emerging animal infectious diseases and the last one is about foodborne illness. The sources of information are Google news, Twitter, and Yelp restaurant reviews. Machine learning approaches are most often used to detect signals. Conclusions: Surveillance is a central topic in public health informatics with the growing use of machine learning approaches in regards of the size and complexity of data. The evaluation of the approaches developed remains a serious challenge.\"}, {\"paperId\": \"7dc5cb781e59bcbfbf9e41e1f1bc915a821883b9\", \"abstract\": \"Background: Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the \\u201c2nd Meeting of Science and Dialysis: Artificial Intelligence,\\u201d organized in the Bellvitge University Hospital, Barcelona, Spain. Summary and Key Messages: AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation.\"}, {\"paperId\": \"0e7791a9394ea7c11b762a8969ef2e3f2b647823\", \"abstract\": null}, {\"paperId\": \"64ec1b23b2882c232cb7d3c4e59ab73f30a3214e\", \"abstract\": null}, {\"paperId\": \"be881df00a339b2c239ee75715fbd33d2612a180\", \"abstract\": \"This book is highly recommended for anyone with previous programing experience who seeks a solid, grounded introduction to basic machine learning using the R statistical software. With nearly 100 additional pages added since the first edition in 2013, this update to Brett Lantz\\u2019s excellent text is well worth the purchase, even for those who already have an earlier copy on their shelf. Clear writing, robust explanations, and compelling examples appear throughout, and most chapters explain the math underlying the methods in as simple and easy a manner as possible. I liked the first edition so much, I used it as the primary textbook for my applied machine learning class for undergraduate juniors and seniors in science and engineering. Chapter 1 provides an overview of the main concepts associated with developing and using ML models for decision making. It includes discussions of traditional topics like overfitting and emerging issues like bias and artificial intelligence (AI) ethics. The chapter structure follows the same pattern as previous editions, so knn, Naive Bayes, decision trees, four neural networks and SVMs, association rules, k-means, and performance are all covered. Chapter 12 on specialized machine learning topics is significantly updated from previous editions and now covers tidyverse, domain-specific data, and brief examinations of performance optimization techniques like parallelization, MapReduce, Hadoop, and Spark. In most chapters, there are fully reproducible examples clearly broken down into steps. Within those steps, subtasks (for example, transformation, data preparation, model specification) are also clearly specified, making it clear how to structure different types of problems. This book is excellent for beginners and others who want to use R to learn how to skillfully address ML problems using their own data.\"}, {\"paperId\": \"176e53ccd4b48b577fdd7f4949b541870ed9ca3a\", \"abstract\": \"Artificial intelligence (AI) in medicine captures the imagination with concepts such as the digital doctor, but also raises concerns such as replacing healthcare professionals, undermining trust in clinicians, and exacerbating inequalities. There is also continued uncertainty as to exactly what AI is. Given this confusion about definition, purpose, and potential, the RCGP has published an introductory report aimed at clarifying the position,1 in which a broad definition of AI as software with decision making capacity is used, ranging from software sequentially going through a series of yes/no questions (decision trees and algorithms) at the simplest, to complex software that learns from data (machine learning) where millions of datasets are used, for example, to find a pattern linking a set of symptoms and a disease.\\n\\nOften, development of AI tools for place-based care and self-care is predominately driven by a focus on the technology and a commercial need to find a market, rather than the challenges and needs of healthcare professionals, practice and community providers, and patients. Unlocking the potential for AI means we must have healthcare professionals, patients, and technology experts working together and engaging with policy makers and commissioners.\\n\\nThe development and provision of AI continues to progress rapidly. The Academic Health Science Network (AHSN), established by NHS England to spread innovation, released the results of a recent AI mapping survey,2 providing an overview of AI healthcare activity. While not exhaustive (organisations including Babylon and Livi are missing), the survey provides some useful insights. Primary and community care AI clusters around two areas. First, clinical decision making and care management, for example, symptom assessment, automating clinical coding, image recognition for dermatological conditions, triaging, and personalised self-management. Second, proactive detection, such as, analysing patient \\u2026\"}, {\"paperId\": \"0511457a48ccd886324c91085db419d799d336a8\", \"abstract\": null}, {\"paperId\": \"7d4481aa67b89fadc576ac7af777ee7476d56581\", \"abstract\": \"Recently, artificial intelligence (AI) has been highlighted in various areas including healthcare [1\\u20134]. AI can be categorized into symbolic AI such as expert systems and machine learning (ML), which includes deep learning. Technically, recently mentioned AI refers to ML or deep learning. Deep learning, which is inspired by biological neurons, is a subcategory of machine learning algorithms [5]. Machine learning (including deep learning) requires a large amount of training data to improve performance. Therefore, to implement a good healthcare AI system, we need a vast amount of healthcare data. Many people believe there is a large amount of data in hospitals based on the wide adaptation of electronic medical records (EMR). They mentioned that the adoption rate of EMR in the United States was dramatically increased to 97% after the introduction of the Health Information Technology for Economic and Clinical Health (HITECH) Act [6] and the adoption rate of EMR in Korea is more than 92%. Nearly all hospitals in Korea also use the computerized physician order entry (CPOE) system. However, the EMR adoption rate is only 58.1%, and the fully comprehensive EMR adoption rate has dropped to 11.6% [7]. This implies a lack of digitalized data for healthcare AI research in Korea. Even though there is a large amount of data, having only a large quantity of data based on big data concepts may fail to achieve an applicable healthcare AI system. We need well-curated and labeled data. For example, 54 US licensed ophthalmologists and ophthalmology senior residents have reviewed 128,175 retinal images to build a well-curated dataset [3]. Current digitalized medical records require more in-depth curation to be used for research. Moreover, to realize precision medicine with the aid of AI methods, we need many new healthcare data types including genome and wearable data. Corresponding Author: Soo-Yong Shin Department of Computer Science and Engineering, Kyung Hee University, 1732, Deogyeong-daero, Giheung-gu, Yongin-si, Gyeonggi-do 17104, Korea Tel: +82-31-201-2543 E-mail: sooyong.shin@khu.ac.kr\"}, {\"paperId\": \"3b77617f1daae4db5b0d45bd5756440cbbd447c3\", \"abstract\": \"Smart city promotes the unification of conventional urban infrastructure and information technology (IT) to improve the quality of living and sustainable urban services in the city. To accomplish this, smart cities necessitate collaboration among the public as well as private sectors to install IT platforms to collect and examinemassive quantities of data. At the same time, it is essential to design effective artificial intelligence (AI) based tools to handle healthcare crisis situations in smart cities. To offer proficient services to people during healthcare crisis time, the authorities need to look closer towards them. Sentiment analysis (SA) in social networking can provide valuable information regarding public opinion towards government actions. With this motivation, this paper presents a new AI based SA tool for healthcare crisis management (AISA-HCM) in smart cities. The AISA-HCM technique aims to determine the emotions of the people during the healthcare crisis time, such as COVID-19. The proposed AISA-HCM technique involves distinct operations such as pre-processing, feature extraction, and classification. Besides, brain stormoptimization (BSO) with deep belief network (DBN), called BSODBN model is employed for feature extraction. Moreover, beetle antenna search with extreme learning machine (BAS-ELM) method was utilized for classifying the sentiments as to various classes. The use of BSO and BAS algorithms helps to effectively modify the parameters involved in the DBN andELMmodels respectively. The performance validation of the AISA-HCM technique takes place using Twitter data and the outcomes are examined with respect to various measures. The experimental outcomes highlighted the enhanced performance of the AISA-HCM technique over the recent state of art SA approaches with the maximum precision of 0.89, recall of 0.88, Fmeasure of 0.89, and accuracy of 0.94. \\u00a9 2022 Tech Science Press. All rights reserved.\"}, {\"paperId\": \"d8cabbfe1a44d7758d67f5deb1ec7dc36fe0ee5f\", \"abstract\": \"By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it. Eliezer Yudkowsky, Machine Intelligence Research Institute1\\n\\nArtifical intelligence (AI) is the future and is already part of our everyday life. Be it voice recognition assistance with Apple\\u2019s Siri or Amazon\\u2019s Alexa, or algorithms that filter your spam emails, recommend a film to you on Netflix, screen your bank account transactions for fraud, or get your auto-pilot flight smoothly to your next holiday destination, chances are you have already experienced AI.\\n\\nMachine \\u2018deep learning\\u2019 takes place in a multi-layered \\u2018black box\\u2019 of deep neural networks, where algorithms are not defined by task-specific rules, but are able to evolve and self-learn using pattern recognition and trial and error. Thus, AI can approach problems as a doctor progressing through their training does: by learning rules from data. However, by having the capacity to analyse massive amounts of data, algorithms are able to find correlations that the human mind cannot.\\n\\nFor some, however, the use of AI in medicine remains a troubling concept. Science fiction is littered with examples of AI running amok at the expense of humanity. While the use of AI in medicine does not evoke exactly the same kind of Orwellian concerns as it might in, for example, national defence, there are still important issues such as privacy, data protection, even the straightforward need for simple human contact to consider. Doctors are expected to temper knowledge with compassion and understanding. These are characteristics which some fear may be lost in any AI driven system, where patients may find their rights are, at best, an afterthought \\u2026\"}, {\"paperId\": \"ae0d5bc3714e4f569177b5f99e0484989516756b\", \"abstract\": null}, {\"paperId\": \"98d172fe2e894aba3d7e5e849dd06c3e0696e927\", \"abstract\": null}, {\"paperId\": \"7204114fcb21c17a41133eac1aa264301a850685\", \"abstract\": null}, {\"paperId\": \"51db00b74663d9305634644b78d288980039f9b9\", \"abstract\": null}, {\"paperId\": \"afaf61d5fd943a6f665d46e3de4205148eea5179\", \"abstract\": \"\\n \\n The aim of this paper is to propose a new overarching challenge for AI: the design of imagination machines. Imagination has been defined as the capacity to mentally transcend time, place, and/or circumstance. Much of the success of AI currently comes from a revolution in data science, specifically the use of deep learning neural networks to extract structure from data. This paper argues for the development of a new field called imagination science, which extends data science beyond its current realm of learning probability distributions from samples. Numerous examples are given in the paper to illustrate that human achievements in the arts, literature, poetry, and science may lie beyond the realm of data science, because they require abilities that go beyond finding correlations: for example, generating samples from a novel probability distribution different from the one given during training; causal reasoning to uncover interpretable explanations; or analogical reasoning to generalize to novel situations (e.g., imagination in art, representing alien life in a distant galaxy, understanding a story about talking animals, or inventing representations to model the large-scale structure of the universe). We describe the key challenges in automating imagination, discuss connections between ongoing research and imagination, and outline why automation of imagination provides a powerful launching pad for transforming AI.\\n \\n\"}, {\"paperId\": \"6d1f0f9286abe336c94ecc15977734fa8a6e28e5\", \"abstract\": null}, {\"paperId\": \"80d696da9403b70d62c3eb855169876022d84f72\", \"abstract\": null}, {\"paperId\": \"149117c1e121fc8797379eda21803db7b9d9ba03\", \"abstract\": null}, {\"paperId\": \"5342abeb08fdc128b8cf1af17276ace5aa0fdd11\", \"abstract\": \"Statistics is a science of data collection. Although a science, it is actually a branch of mathematics. In statistics, data-sets are distributed in such a way that they form a certain shape, the most common of which is the bell-shaped curve known also as the bell curve, normal distribution and the Gaussian. Binomial, \\u03c72 and Poisson, among others, are other statistical distributions that the readers are already familiar with or at least, have heard of; but what of data mining and machine learning? Data mining and machine learning can be defined in many ways. Here, the authors describe data mining as a set of techniques for analysing and describing structured data; and machine learning as one for interpreting data by comparing them to models for data behaviour. In essence, data mining and machine learning are about looking for patterns and relationships in data. Perhaps an easier way to look at it is through the readers\\u2019 own experience with supermarket and social media. By means of the bonus-point programme, supermarket accumulates customers\\u2019 shopping habits and interests. Ever notice when the readers stop buying something, say a dog food, after a while there are emails (or snail mails even) of the same enticing the readers to go and buy one (Sales now on! etc., never mind that the dog has been put to sleep)? That is all data mining, popular with the customer relationship management team. Or, any kind of News Feed the readers get on Facebook? How does Facebook know the readers want to see the video of the cat first thing in the morning? Facebook algorithm essentially learns (yes, on its own) via statistical analysis what the readers like (and \\u2018Like\\u2019) and bingo! The cat\\u2019s Feed is at the top. The fact that the algorithm can do this without being told is machine learning, a form of artificial intelligence. What is this book about? One half of this book is about statistics. The other is about using statistics in astronomy and astrophysics. As the readers already know, scientists, astronomers included, are not clueless when it comes to statistics \\u2013 they have been using it for ages. If that is so, why do they need this book then? The truth is, the field of statistics has evolved by leaps and bounds, leaving astronomers stuck in the same old rut, using the same old technique that is proven to be more and more cumbersome each day in dealing with the amount of data that easily run to the terabytes. (It seems astronomers no longer count data by the numbers but by (computer) memory instead.) In a word, the book is about teaching astronomers the art of analysing very large data-sets by introducing a more efficient way of using statistics. It is a practical guide to data mining by utilising tools such as unsupervised classification, clustering, principal component analysis, locally linear embedding and the somewhat exotic-sounding, projection pursuit; and to machine learning by methods like Bayesian, supervised classification, maximum likelihood estimator and regression. (Supervised simply means something about the data is already known while unsupervised means nothing is.) The combination of the three subjects is certainly special as usually, the readers would have to get different text(s) for each. The way to read this book is by doing i.e. by practicing the computer codes to reproduce the graphs and figures as the readers go through it so the pros and cons of any particular method can be studied. To make the practice accessible to all, the authors have used AstroML1 based on Python2 in their sample analyses. Python is an open-source object-oriented language that is quickly gaining popularity in the scientific community not least because it is free hence available to everybody; and AstroML is its module for data mining and machine learning. Nothing needs to be done other than installing Python and the AstroML package to start the ball rolling. For the purpose of this book, data-sets from Sloan Digital Sky Survey are used. There is no need to worry about their large volume either as the data are maintained on GitHub,3 the online repository. Of course, the readers can also choose to use their own data and modify the codes as they see fit. Who is this book for? Obviously it is written for those already in the fields of astronomy and astrophysics that collect data by the millions like extragalactic astronomy, exoplanets, etc. Its value can only be appreciated by researches looking for ways of interpreting the astronomical amount of astronomical data (OK, pun intended) by way of statistics.\"}, {\"paperId\": \"b78888bc9dd18a1a8c36bb560ca18f9b1eeab05e\", \"abstract\": \"Welcome to this inaugural issue of Radiology: Artificial Intelligence. Our journal\\u2019s mission is to publish highquality scientific work that advances our understanding of artificial intelligence (AI) in radiology. AI has become a topic of great interest\\u2014especially the application of machine learning techniques to medical images\\u2014but AI itself is not new. The term artificial intelligence was proposed in 1956 to describe efforts to understand, simulate, and improve upon human qualities such as reasoning, learning, solving problems, understanding verbal and written language, processing visual information, and playing games like chess and poker. What is new is a resurgence of interest in AI, particularly in the use of machine learning to recognize patterns in images. And, curiously, it is game playing that has opened this new frontier\\u2014but not the games of chess, checkers, or Go. Rather, think Xbox and PlayStation. Video games require a rapidly changing three-dimensional scene to be transformed into two-dimensional images shown in real time. The need to compute images efficiently spurred the development of highly parallelized graphics processing units. These specialized processors, in turn, have powered software for increasingly complex and sophisticated \\u201cdeep\\u201d artificial neural network models. Whereas neural networks developed 10 years ago typically had three or four layers, today\\u2019s deep networks comprise hundreds of layers (1). Deep learning models have engendered both great excitement and a great deal of hyperbole. After all, if AI systems can pick out pictures of cats on the web, then surely such systems are ready to replace radiologists, right? Well, perhaps not, at least not right now. There is much work to be done to build and validate systems that can detect and characterize the thousands of imaging findings and their associated diseases that can be seen across a panoply of radiology studies. And that brings us to the quote from Shakespeare. Anyone can claim to build an AI system, but that doesn\\u2019t mean that the system will do their bidding as imagined. Our journal is here to assure that the science and applications of AI in radiology are built on thoughtful, innovative, and well-validated research. What sorts of topics will this journal publish? We will bring you the same high caliber of research that is found in RSNA\\u2019s flagship scientific journal, Radiology, but focused here on AI, machine learning, and data science in radiology. In particular, we seek to publish first-rate work that provides rigorous evaluation of AI\\u2019s applications to clinical problems in radiology. We invite manuscripts that show the impact of AI to extract information, diagnose and manage disease in patients, streamline radiology workflow, or improve health care outcomes. We\\u2019re interested in image segmentation, image reconstruction, automated detection of abnormalities, diagnostic reasoning, natural language processing, clinical workflow analysis, radiomics, and radiogenomics. We also invite manuscripts that demonstrate novel applications of AI in radiology or highlight innovative AI methodologies. Developers of publicly available sets of radiologic images, image annotations, radiology reports, or algorithms can present their work as a Data Resources report. AI and radiology do not exist in isolation: they are part of broad endeavors to advance knowledge and improve health. As such, this journal will feature articles on the ethical, legal, social, and economic implications of AI in radiology. AI is and must be a human\\u2014and humane\\u2014activity (2). We must engage in this work with an eye to how these technologies will help us care for our patients more effectively and humanely. Our goal is not to replace, but rather to extend our human abilities to provide medical care\\u2014 and to improve the lives of those we are privileged to serve. All RSNA members receive access to this online bimonthly journal. We invite all readers (RSNA members or not) to sign up for our Editor\\u2019s Blog, The Vasty Deep (https://pubs.rsna.org/page/ai/blog) and to follow us on Twitter (@Radiology_AI). These social media platforms will augment the journal and offer innovative online features. Again, welcome!\"}, {\"paperId\": \"d5ae3dce416a9fdad3763bac3078c3548ba4f9c8\", \"abstract\": \"The article considers the basic methods of machine learning applied by individual entrepreneurs within the framework of transition to digital production to improve the efficiency of data processing, classification of existing and would-be customers and their subsequent work with them. The main attention is paid to the problem of increasing the effectiveness of methods of machine learning applied for solving the current questions. Areas of application of technology are shown. The peculiarities of machine learning are briefly analyzed. The main features and prospects of the development of Machine Learning services are shown on the basis of the concept of a step-by-step combination of the methods under consideration. One of the main algorithms for working with data is analyzed; its main features, scope and procedure are described. Recommendations are given for the further use of machine learning algorithms. The role of machine learning in the development of modern science and industry is analyzed, the main tendencies of the industry development are determined, and the practical application of big data is shown. As part of the transition to Industry 4.0, the main areas of application of machine learning, big data, Artificial Intelligence and their relations with the corresponding fields of science and production are described. The article also offers a review of the application of Artificial Intelligence and machine learning in particular in the context of the transition to digitalization and the issues of individual entrepreneurship.\"}, {\"paperId\": \"a4bd137beb6e5cc20834edf72ce7b3b910697de8\", \"abstract\": null}, {\"paperId\": \"ca6088fabddecf009db863954114980ae2c0f55a\", \"abstract\": \"Prof. Vural \\u00d6zdemir (Editor-in-Chief, OMICS: A Journal of Integrative Biology): Colin, many thanks for agreeing to this interview for OMICS readership. Artificial intelligence (AI) and algorithms are becoming hot topics in systems medicine and emerging OMICS fields such as microbiome science. Therefore, please allow me to introduce the journal readership to you. OMICS is the first systems sciences and systems thinking journal with a legacy over two decades. Our approach to large-scale biology is interdisciplinary and integrative. With that, I mean an approach that is broadly focused on systems science technologies, interdisciplinary, and spans from \\u2018\\u2018cell to society.\\u2019\\u2019 Let\\u2019s start with a brief history of AI from 20th century. Have AI and its conception changed since the last century? Mr. Colin Garvey: An easy way to make sense of AI history is in terms of three paradigms, \\u2018\\u2018GOFAI\\u2019\\u2019 (1950\\u201360s), \\u2018\\u2018expert systems\\u2019\\u2019 (late 1970\\u201380s), and \\u2018\\u2018machine learning\\u2019\\u2019 (2010\\u2013present). GOFAI, short for \\u2018\\u2018good old-fashioned artificial intelligence,\\u2019\\u2019 employed symbolic logic to make \\u2018\\u2018thinking machines,\\u2019\\u2019 which basically failed. But the basic insight that clever heuristics were more important than bruteforce computation for intelligent behavior in machines lived on. Some say heuristic search, which is still used billions of times a day on the Internet, was the original AI problem. Next, the expert systems paradigm still used symbolic logic but narrowed the focus from general intelligence to human expertise in specific domains, such as chemistry and medicine. Attempts to replicate experts\\u2019 knowledge and decision-making processes led to the first major medical AI system, MYCIN, and eventually to more familiar software like TurboTax. But the expert systems paradigm has always been limited by the \\u2018\\u2018knowledge acquisition bottleneck\\u2019\\u2019\\u2014 it turns out that extracting expertise from living humans is hard, time-consuming work! The current machine learning (ML) paradigm bypasses that bottleneck to extrapolate patterns or \\u2018\\u2018learn\\u2019\\u2019 directly from data, usually through a training period of 100,000s of trial-and-error loops. This requires considerable computational power and memory, which is why the recent successes of ML algorithms in image and speech recognition, language translation, and games like Go and poker, owe as much to recent hardware innovations as clever programming. That said, ML doesn\\u2019t seem to be taking AI any closer to the longtime dream of \\u2018\\u2018general intelligence.\\u2019\\u2019 In many cases ML even makes AI less intelligible to humans. Most ML algorithms, once fully trained on a given dataset, become what you might call \\u2018\\u2018black box savants\\u2019\\u2019: accurate 98% of the time, but totally incapable of explaining why they produce the answers they do. Defense Advanced Research Projects Agency (DARPA) recently launched the \\u2018\\u2018Explainable AI\\u2019\\u2019 program specifically to address this problem. Prof. \\u00d6zdemir: AI and related tools are rapidly changing engineering, manufacturing, and industry practices. Yet, AI has not been firmly at the epicenter of medical research and life sciences compared to engineering, self-driving cars, or customer and retail services. AI might potentially be harnessed with a view to systems medicine, for example, to obtain and make sense of deep phenotypic data collected in the course of a day in community settings (instead of hospital settings) on individuals\\u2019 health. What are the key prospects that AI offers for medical research? Mr. Garvey: With the proliferation of cheap computation and data storage, high-bandwidth wireless connections, and sensors in smartphones and wearables, the infrastructure for continuous monitoring of more life processes at finer scales now exists, and the data produced thereby offer great potential in the ML paradigm. Informed by a systems perspective of the organism-in-context, AI could help to facilitate a transformation in medicine, away from the treatment of symptoms by specialists to the intergenerational maintenance and enhancement of health at individual, community, and even population levels. Of course, the promise of gathering data across the entire phenotypic expression of the human organism raises a number of serious ethical questions\\u2014but this is an area where I think the medical community has greater expertise than the technologists. For this reason, systems medicine can and should inform the development of relevant AI technologies. Prof. \\u00d6zdemir: What about the AI prospects for clinical applications? Any linkages between AI and the quantified self movement? Mr. Garvey: One positive example I like is the electronic wristband developed by Rosalind Picard of MIT\\u2019s Media Lab. It uses AI to predict epileptic seizures in the wearer by analyzing sensor data gathered via sensor contact with the skin, to prevent dangerous falls and other risks. But industry\"}, {\"paperId\": \"c62c5ee6d06654dfd0e23913e2216d1b5bbf29a3\", \"abstract\": \"The role of artificial intelligence techniques and its impact in context of cognitive radio networks has become immeasurable. Artificial intelligence redefines and empowers the decision making and logical capability of computing machines through the evolutionary process of leaning, adapting, and upgrading its knowledge bank accordingly. Significant functionalities of artificial intelligence include sensing, collaborating, learning, evolving, training, dataset, and performing tasks. Cognitive radio enables learning and evolving through contextual data perceived from its immediate surrounding. Cognitive science aims at acquiring knowledge by observing and recording externalities of environment. It allows self-programming and self-learning with added intelligence and enhanced communicational capabilities over wireless medium. Equipped with cognitive technology, the vision of artificial intelligence gets broadened towards optimizing usage of radio spectrum by accessing spectrum availability, thereby reducing channel interferences while communication among licensed and non-licensed users.\"}, {\"paperId\": \"05dc3dcc70509c2519cbf54b04eb41496da59f3c\", \"abstract\": \"Core competencies in molecular imaging and nuclear medicine include imaging with radioactive isotopes, pattern recognition, image interpretation, and report communication. In diagnostic imaging, nuclear medicine physicians communicate with referring physicians to establish indications for imaging studies, perform or supervise imaging procedures to obtain high-quality images, interpret images to compile medical reports, and communicate results to inform patients and their referring doctors. In many of these tasks, computers already are indispensable tools that enable or assist the work of physicians. How does artificial intelligence (AI) fit into this workflow (Fig. 1)? There is no universal definition of AI (1), but in the context of practical applications, AI can be considered a scientific discipline that uses computers to perform tasks usually requiring human cognition. Research topics on AI include pattern recognition, natural language processing, machine learning, problem solving, and knowledge representation. Thus, AI techniques apply to many core competencies in metabolic imaging and nuclear medicine, and this has led to an enormous interest and even hype about AI in medical imaging (2). AI applications at present dominate the technical exhibitions of international imaging conferences such as the 2019 European Congress of Radiology (https://ecronline.myesr.org/ecr2019/) and are prominently featured in the product portfolios of most commercial companies of medical imaging technology. AI applications have enabled a multibillion-dollar business model for the large Internet companies penetrating the consumer market in many ways, including Internet searches, social networks, or smartphone software. In consumer markets, value is measured by the willingness of a consumer to use or pay for a product. Therefore, the value of AI applications in consumer markets depends not on an intrinsic objective value but the perception of the customer. This value concept is well illustrated by a quote attributed to Charles Revson, the founder of a cosmetics company: \\u2018\\u2018In the factory we make cosmetics, in the store we sell hope.\\u2019\\u2019 However, in medicine the business model and the value concept of a consumer market do not apply. Hope is essential when taking care of patients, but hope cannot serve as the foundation for medical decision making and patient stratification. Several decades ago, patients with extrasystoles after a myocardial infarction were commonly treated with antiarrhythmic drugs in the hope of preventing arrhythmic death. A large, randomized multicenter trial uncovered this hope as a deadly illusion and established that the antiarrhythmic therapy in fact increased arrhythmic mortality and caused premature death in thousands of patients. From this trial and others that were able to scientifically disprove suggested or assumed benefits based on hope and hype, rigorous and often laborious scientific methods have emerged that permit establishing, grading, or disproving the value of diagnostic and therapeutic procedures. For patients, value is generated if quality of life is improved, morbidity is reduced, or preventable mortality is eliminated. It is difficult or impossible for both patients and physicians to directly assess the value of medical products, interventions, or technology. Thus, science is essential to firmly guide patient care. International guidelines compiled by professional societies assess the value of diagnostic and therapeutic methods based on scientific evidence and, for each method, clearly state the class of recommendation for a particular use with an associated level of evidence. This science-based model of grading value for patient care has been universally adopted in the medical community. Currently, there is little scientific evidence for the value of AI applications in medical imaging. Several AI applications have received Food and Drug Administration approval (3), but this does not imply that AI applications at present are relevant for medical practice or that their value has been firmly established. Most AI applications are built from 3 essential components: complex computer algorithms, extensive computing resources, and large data sets. Algorithms and computing facilities are easily available at little or no cost. Many powerful AI algorithms are published as opensource code, such as TensorFlow (https://www.tensorflow.org) and Core ML (https://developer.apple.com/machine-learning/). Extensive computing power is offered instantaneously on demand through the large Internet companies. Recently, even smartphones have been equipped with high-power computing hardware, including neural network chips that enable intensive AI applications, such as facial identification or speech recognition. In contrast, large data sets are more difficult to collect and thus are the most critical component when building AI applications. Data are more important than hardware or software in determining the success of AI applications (4). Even highly complex AI algorithms cannot compensate for incomplete, inadequate, or low-quality data collection. Thus, the characteristics and validity of data sets need to be firmly established and made fully transparent when AI applications are investigated for a proposed clinical purpose. Collaborative efforts are frequently required to compile the large data sets, which need to include many thousands of data entries. Recently, the Mozilla Common Voice project (https://voice. mozilla.org/en) has published an open-source multilanguage data set of voice recordings from 40,000 people in 18 languages to foster and enable research in natural language processing. If AI applications are to be applied successfully in medical imaging, Received Mar. 18, 2019; revision accepted Apr. 3, 2019. For correspondence or reprints contact: Gerold Porenta, Ambulatorium D\\u00f6bling, Heiligenst\\u00e4dterstrasse 62-64, 1190 Vienna, Austria. E-mail: gerold@porenta.com Published online May 3, 2019. COPYRIGHT\\u00a9 2019 by the Society of Nuclear Medicine and Molecular Imaging. DOI: 10.2967/jnumed.119.227702\"}, {\"paperId\": \"3ae47e68d15de6d39ab6cda107949a4660783b38\", \"abstract\": \"\\nPurpose\\nThe travel and tourism industry (TTI) could benefit the most from artificial intelligence (AI), which could reshape this industry. This study aims to explore the characteristics of tourism AI start-ups, the AI technological domains financed by Venture Capitalists (VCs), and the phases of the supply chain where the AI domains are in high demand.\\n\\n\\nDesign/methodology/approach\\nThis study developed a database of the European AI start-ups operating in the TTI from the Crunchbase database (2005\\u20132020). The authors used start-ups as the unit of analysis as they often foster radical change. The authors complemented quantitative and qualitative methods.\\n\\n\\nFindings\\nAI start-ups have been mainly created by male Science, Technology, Engineering and Mathematics graduates between 2015 and 2017. The number of founders and previous study experience in non-start-up companies was positively related to securing a higher amount of funding. European AI start-ups are concentrated in the capital town of major tourism destinations (France, UK and Spain). The AI technological domains that received more funding from VCs were Learning, Communication and Services (i.e. big data, machine learning and natural language processing), indicating a strong interest in AI solutions enabling marketing automation, segmentation and customisation. Furthermore, VC-backed AI solutions focus on the pre-trip and post-trip.\\n\\n\\nOriginality/value\\nTo the best of the authors\\u2019 knowledge, this is the first study focussing on digital entrepreneurship, specifically VC-backed AI start-ups operating in the TTI. The authors apply, for the first time, a mixed-method approach in the study of tourism entrepreneurship.\\n\"}, {\"paperId\": \"1e2e9cdacc66e74ce89d165a45bf1d4c88fb4e39\", \"abstract\": \"Researchers and practitioners studying best practices strive to design Machine Learning (ML) application systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. In this paper, software-engineering architecture and design (anti-)patterns for ML application systems are analyzed to bridge the gap between traditional software systems and ML application systems with respect to architecture and design. Specifically, a systematic literature review confirms that ML application systems are popular due to the promotion of artificial intelligence. We identified 32 scholarly documents and 48 gray documents out of which 38 documents discuss 33 patterns: 12 architecture patterns, 13 design patterns, and 8 anti-patterns. Additionally, a survey of developers reveals that there are 7 major architecture patterns and 5 major design patterns. Then the relationships among patterns are identified in a pattern map. THE POPULARITY OF ML techniques has increased in recent years. ML is used in many domains, including cyber security, IoT, and autonomous cars. ML techniques rely on mathematics and software engineering. The former generates algorithms, develops capabilities to learn from input data, and produces representative models. The latter is employed for implementation and performance. Although many works investigated the mathematics and computer science on which ML techniques are built, few have examined their implementation. This situation raises many concerns. The first is software complexity of ML techniques. The second is quality of the available implementations, including performance and reliability. The third is model quality, which may be negatively impacted by software bug. These concerns could be alleviated if developers could demonstrate the software quality of their implementations. Consequently, researchers and practitioners have been studying best practices to design ML application systems to address issues with software complexity and quality of ML techniques. Such practices are often formalized as architecture patterns and design patterns. These patterns encapsulate reusable solutions to commonly occurring problems within ML application systems and software design. Herein we report the results of a systematic IEEE Software Published by the IEEE Computer Society c \\u00a9 2020 IEEE 1\"}, {\"paperId\": \"2dd2acb0a79b6a6483df9d22da590b9243f57f45\", \"abstract\": \"Background Artificial intelligence\\u2013based assistive diagnostic systems imitate the deductive reasoning process of a human physician in biomedical disease diagnosis and treatment decision making. While impressive progress in this area has been reported, most of the reported successes are applications of artificial intelligence in Western medicine. The application of artificial intelligence in traditional Chinese medicine has lagged mainly because traditional Chinese medicine practitioners need to perform syndrome differentiation as well as biomedical disease diagnosis before a treatment decision can be made. Syndrome, a concept unique to traditional Chinese medicine, is an abstraction of a variety of signs and symptoms. The fact that the relationship between diseases and syndromes is not one-to-one but rather many-to-many makes it very challenging for a machine to perform syndrome predictions. So far, only a handful of artificial intelligence\\u2013based assistive traditional Chinese medicine diagnostic models have been reported, and they are limited in application to a single disease-type. Objective The objective was to develop an artificial intelligence\\u2013based assistive diagnostic system capable of diagnosing multiple types of diseases that are common in traditional Chinese medicine, given a patient\\u2019s electronic health record notes. The system was designed to simultaneously diagnose the disease and produce a list of corresponding syndromes. Methods Unstructured freestyle electronic health record notes were processed by natural language processing techniques to extract clinical information such as signs and symptoms which were represented by named entities. Natural language processing used a recurrent neural network model called bidirectional long short-term memory network\\u2013conditional random forest. A convolutional neural network was then used to predict the disease-type out of 187 diseases in traditional Chinese medicine. A novel traditional Chinese medicine syndrome prediction method\\u2014an integrated learning model\\u2014was used to produce a corresponding list of probable syndromes. By following a majority-rule voting method, the integrated learning model for syndrome prediction can take advantage of four existing prediction methods (back propagation, random forest, extreme gradient boosting, and support vector classifier) while avoiding their respective weaknesses which resulted in a consistently high prediction accuracy. Results A data set consisting of 22,984 electronic health records from Guanganmen Hospital of the China Academy of Chinese Medical Sciences that were collected between January 1, 2017 and September 7, 2018 was used. The data set contained a total of 187 diseases that are commonly diagnosed in traditional Chinese medicine. The diagnostic system was designed to be able to detect any one of the 187 disease-types. The data set was partitioned into a training set, a validation set, and a testing set in a ratio of 8:1:1. Test results suggested that the proposed system had a good diagnostic accuracy and a strong capability for generalization. The disease-type prediction accuracies of the top one, top three, and top five were 80.5%, 91.6%, and 94.2%, respectively. Conclusions The main contributions of the artificial intelligence\\u2013based traditional Chinese medicine assistive diagnostic system proposed in this paper are that 187 commonly known traditional Chinese medicine diseases can be diagnosed and a novel prediction method called an integrated learning model is demonstrated. This new prediction method outperformed all four existing methods in our preliminary experimental results. With further improvement of the algorithms and the availability of additional electronic health record data, it is expected that a wider range of traditional Chinese medicine disease-types could be diagnosed and that better diagnostic accuracies could be achieved.\"}, {\"paperId\": \"21412d1d0a8db97fa04e7ff44c65c44f89fa0680\", \"abstract\": \"Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to \\u201ctrust a \\u2018black box\\u2019\\u201d). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist. This issue of JAMA contains 2 Viewpoints on deep learning in health care. Hinton4 explains the technology underlying AI and deep learning, using clinical examples. AI is the general term for imitating human intelligence with computer systems. Early AI systems represented human reasoning with symbolic logic. As computer processing and storage became more powerful, researchers developed machine-learning techniques to imitate the way the human brain learns. The first machine learning continued to rely on human experts to label the data the system trained on (eg, the diagnosis) and to identify the significant features (eg, findings). Machine learning weighted the features from the data. With continued advances in computational power and with larger data sets, researchers began to develop deep learning techniques. The first deep learning algorithms were \\u201csupervised\\u201d in that human experts continued to label the training data, and the deep learning algorithms learned the features and weights directly from the data. The retinopathy screening algorithms are an example of supervised deep learning. Hinton4 describes continuing development of new deep learning techniques, including ones that are completely unsupervised. He also points out that it is not feasible to see the features learned by deep learning to explain how the system reaches a conclusion. Naylor5 identifies 7 factors driving adoption of AI and deep learning in health care: (1) the strengths of digital imaging over human interpretation; (2) the digitization of health-related records and data sharing; (3) the adaptability of deep learning to analysis of heterogeneous data sets; (4) the capacity of deep learning for hypothesis generation in research; (5) the promise of deep learning to streamline clinical workflows and empower patients; (6) the rapid-diffusion open-source and proprietary deep learning programs; and (7) of the adequacy of today\\u2019s basic deep learning technology to deliver improved performance as data sets get larger. Factors 3, 4, and 6 are specific to deep learning; the other factors apply to other AI techniques as well. Artificial intelligence is a family of technical techniques in the same way the radiologic imaging tool kit includes flat images, computed tomography scans, and functional imaging such as magnetic resonance imaging. Advances in computational technology, computer science, informatics, and statistics improve existing techniques and make new techniques possible. The addition of deep learning to the AI family of techniques represents an advance similar in magnitude to the addition of the computed tomography scanner to the radiology tool kit. Each AI technique has strengths and weaknesses. Symbolic logic is self-explaining but difficult to scale.6 For example, knowledge engineers extract the logic by interviewing or observing human experts. Statistical techniques such as supervised deep learning scale, but are subject to bias in the training data, and the reasoning cannot be explained. Since deep learning systems are trained on data from the past, they are not prepared to reason in the way humans do about conditions that have not been seen before. In the future, unsupervised deep learning may reduce this gap between human intelligence and AI. The potential applications of AI in health care present a range of computational difficulty. Narrow tasks, in which the context is predefined, are relatively easy. Imageprocessing tasks such as recognizing the border of an organ to suggest where to cut off a scan, or highlighting a suspicious area in an image for the radiologist or pathologist, are examples of narrow tasks. Image analysis and diagnostic prediction tasks such as the diabetic retinopathy example are broader and harder, but doable with today\\u2019s technology. Very broad data analysis and pattern prediction tasks such as analyzing heterogeneous data sets from diverse sources to suggest novel associations are feasible today because the purpose is limited to hypothesis generation. Thinking in the way humans do\\u2014reasoning, for example, from a few observations to suggest a novel scientific framework as Einstein did with the theory of relativity\\u2014is beyond technology on the horizon. Clinicians should view the output of AI programs or devices as statistical predictions. They should maintain an index of suspicion that the prediction may be wrong, just as they Viewpoint pages 1099 and 1101 Opinion\"}, {\"paperId\": \"3ad6932d2a1c695d1b8791a29dff670be4276202\", \"abstract\": \"This paper presents an in-depth literature review on the driving forces and barriers for achieving operational excellence through artificial intelligence (AI). Artificial intelligence is a technological concept spanning operational management, philosophy, humanities, statistics, mathematics, computer sciences, and social sciences. AI refers to machines mimicking human behavior in terms of cognitive functions. The evolution of new technological procedures and advancements in producing intelligence for machines creates a positive impact on decisions, operations, strategies, and management incorporated in the production process of goods and services. Businesses develop various methods and solutions to extract meaningful information, such as big data, automatic production capabilities, and systematization for business improvement. The progress in organizational competitiveness is apparent through improvements in firm\\u2019s decisions, resulting in increased operational efficiencies. Innovation with AI has enabled small businesses to reduce operating expenses and increase revenues. The focused literature review reveals the driving forces for achieving operational excellence through AI are improvement in computing abilities of machines, development of data-based AI, advancements in deep learning, cloud computing, data management, and integration of AI in operations. The barriers are mainly cultural constraints, fear of the unknown, lack of employee skills, and strategic planning for adopting AI. The current paper presents an analysis of articles focused on AI adoption in production and operations. We selected articles published between 2015 and 2020. Our study contributes to the literature reviews on operational excellence, artificial intelligence, driving forces for AI, and AI barriers in achieving operational excellence.\"}, {\"paperId\": \"4dc28be3d635705e0ab0b4d7355b036a31f015ef\", \"abstract\": \"New edition of the bestselling guide to artificial intelligence with Python, updated to Python 3.x and TensorFlow 2, with seven new chapters that cover RNNs, AI & Big Data, fundamental use cases, chatbots, and more. \\n \\n Key Features \\n \\n \\n Completely updated and revised to Python 3.x, and TensorFlow 2 \\n \\n Seven new chapters that include AI on the cloud, RNNs and DL models, feature engineering, the machine learning data pipeline, and more \\n \\n New author with 25 years of experience in artificial intelligence across multiple industries and enterprise domains \\n \\n \\n Book Description \\n \\n Artificial Intelligence with Python, Second Edition is an updated and expanded version of the bestselling guide to artificial intelligence using the latest version of Python 3.x and TensorFlow 2. Not only does it provide you an introduction to artificial intelligence, this new edition goes further by giving you the tools you need to explore the amazing world of intelligent apps and create your own applications. \\n \\n This edition also includes seven new chapters on more advanced concepts of Artificial Intelligence, including fundamental use cases of AI; machine learning data pipelines; feature selection and feature engineering; AI on the cloud; the basics of chatbots; RNNs and DL models; and AI and Big Data. \\n \\n Finally, this new edition explores various real-world scenarios and teaches you how to apply relevant AI algorithms to a wide swath of problems, starting with the most basic AI concepts and progressively building from there to solve more difficult challenges so that by the end, you will have gained a solid understanding of, and when best to use, these many artificial intelligence techniques. \\n \\n What you will learn \\n \\n \\n Understand what artificial intelligence, machine learning, and data science are \\n \\n Explore the most common artificial intelligence use cases \\n \\n Learn how to build a machine learning pipeline \\n \\n Assimilate the basics of feature selection and feature engineering \\n \\n Identify the differences between supervised and unsupervised learning \\n \\n Discover the most recent advances and tools offered for AI development in the cloud \\n \\n Develop automatic speech recognition systems and chatbots \\n \\n Understand RNNs and various DL models \\n \\n \\n Who this book is for \\n \\n The intended audience for this book is Python developers who want to build real-world Artificial Intelligence applications. Basic Python programming experience and awareness of machine learning concepts and techniques is mandatory.\"}, {\"paperId\": \"91fe28e54a27b2360f6979b484805871d29eef7d\", \"abstract\": null}, {\"paperId\": \"87bc917cea92360b47937cbd50028918699d2b05\", \"abstract\": \"Howwas your morning? Perhaps you woke up, did a little online shopping while brewing your coffee, posted some pictures on social media over breakfast, glanced over the world news, drove to work, checked your email, picked up your mail, and opened up your latest issue of ACS Central Science. Pretty unremarkable, right? Maybe, but in the few hours that you have been awake you have most likely interacted with numerous instances of machine learning algorithms ticking away just below the surface of our everyday lives. The term \\u201cmachine learning\\u201d may be defined as algorithms that allow computers to learn to perform tasks, identify relationships, and discern patterns without the need for humans to provide the underlying instructions. Conventional algorithms operate by sequentially executing a preprogrammed set of rules to achieve a particular outcome. Machine learning algorithms, by contrast, are instead provided with a set of examples by the user and train themselves to learn the rules f rom the data. This powerful idea dates back to at least the 1950s, but has only been fully realized in recent years with the advent of sufficiently large digital data sets over which to perform training\\ue0d5for example, Google photo albums, Amazon shopping lists, Netflix viewing histories\\ue0d5and sufficiently powerful computer hardware and algorithms to perform the training\\ue0d5typically powerful graphics cards developed for the computer game industry that can be hijacked to conduct machine learning. This paradigm has revolutionized multiple domains of science and technology, with different variants of machine learning dominating, and in some cases enabling, multifarious applications such as retail recommendation engines, facial detection and recognition, language translation, autonomous and assisted driving, spam filtering, and character recognition. The success of these algorithms may be largely attributed to their enormous flexibility and power to extract patterns, correlations, and structure from data. These features can be nonintuitive and complicated functions that are difficult for humans to parse, or exist as weak signals that are only discernible from large, high-dimensional data sets that defy conventional analysis techniques. There remains a fundamental difference between artificial and human intelligence\\ue0d5no machine has yet exhibited generic human cognition, and for now, the Turing Test remains intact1\\ue0d5but machine performance in certain specific tasks is unequivocally superhuman. A prominent example is provided by Google\\u2019s Go-playing computer program AlphaGo Zero. This program was provided only with the rules of the ancient board game and learned to play by playing games against itself in a form of reinforcement learning. After just 3 days of training, AlphaGo Zero roundly defeated the best previous best algorithm (AlphaGo Lee) that had itself beaten the 18-time (human) world champion Lee Sedol 100 games to 0. Remarkably, AlphaGo Zero employed previously unknown strategies of play that had never been discovered by human players over the 2500 year history of the game.\"}, {\"paperId\": \"b68763eba5dfce131d089a446e9e4ebd1dd89a7c\", \"abstract\": \"Machine learning is a sub field of artificial intelligence which allows forecasting through learning past behaviors and rules from old data. In today\\u2019s world, machine learning is being used almost in any fields such as education, medicine, veterinary, banking, telecommunication, security, and bio-medical sciences. In human health, although machine learning is generally preferred particularly in predicting diseases and identifying respective risk factors, it is obvious that there are a limited number of publications where this method was applied on veterinary or indicates whether it is correct and applicable. In this review, it was observed that the neural network, logistic regression, linear regression, multiple regression, principle component analysis and k-means methods were frequently used in examined publications and machine learning application in veterinary field upward momentum. Additionally, it was observed that recent developments in the field of machine learning (deep learning, ensemble learning, voice recognition, emotion recognition, etc.) is still new in the field of veterinary. In this review, publications are examined under clustering, classification, regression, multivariate data analysis and image processing topics. This review aims at providing basic information on machine learning and to increase the number of multidisciplinary publications on computer sciences/engineering and veterinary field.\"}, {\"paperId\": \"90506fb63be3e07c3d1514d459acee64fe759ef2\", \"abstract\": \"Public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing this collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to any penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number. Artificial Intelligence (AI) is conventionally, if loosely, defined as intelligence exhibited by machines. Operationally, it can be defined as those areas of R&D practiced by computer scientists who identify with one or more of the following academic sub-The field of Machine Learning (ML) is a foundational basis for AI. While this is not a complete list, it captures the vast majority of AI researchers. Artificial General Intelligence (AGI) is a research area within AI, small as measured by numbers of researchers or total funding, that seeks to build machines that can successfully perform any task that a human might do. Where AI is oriented around specific tasks, AGI seeks general cognitive abilities. On account of this ambitious goal, AGI has high visibility, disproportionate to its size or present level of success, among futurists, science fiction writers, and the public.\"}, {\"paperId\": \"9c7113a7760afc05cbfdde9a8f7c672f3ac3e907\", \"abstract\": \"Molecular imaging (MI) is a science that uses imaging methods to reflect the changes of molecular level in living state and conduct qualitative and quantitative studies on its biological behaviors in imaging. Optical molecular imaging (OMI) and nuclear medical imaging are two key research fields of MI. OMI technology refers to the optical information generated by the imaging target (such as tumors) due to drug intervention and other reasons. By collecting the optical information, researchers can track the motion trajectory of the imaging target at the molecular level. Owing to its high specificity and sensitivity, OMI has been widely used in preclinical research and clinical surgery. Nuclear medical imaging mainly detects ionizing radiation emitted by radioactive substances. It can provide molecular information for early diagnosis, effective treatment and basic research of diseases, which has become one of the frontiers and hot topics in the field of medicine in the world today. Both OMI and nuclear medical imaging technology require a lot of data processing and analysis. In recent years, artificial intelligence technology, especially neural network-based machine learning (ML) technology, has been widely used in MI because of its powerful data processing capability. It provides a feasible strategy to deal with large and complex data for the requirement of MI. In this review, we will focus on the applications of ML methods in OMI and nuclear medical imaging.\"}, {\"paperId\": \"c27ad9346f384e828a4cd6dc8e7e724ea54bd1a2\", \"abstract\": \"Background Research on the integration of artificial intelligence (AI) into community-based primary health care (CBPHC) has highlighted several advantages and disadvantages in practice regarding, for example, facilitating diagnosis and disease management, as well as doubts concerning the unintended harmful effects of this integration. However, there is a lack of evidence about a comprehensive knowledge synthesis that could shed light on AI systems tested or implemented in CBPHC. Objective We intended to identify and evaluate published studies that have tested or implemented AI in CBPHC settings. Methods We conducted a systematic scoping review informed by an earlier study and the Joanna Briggs Institute (JBI) scoping review framework and reported the findings according to PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analysis-Scoping Reviews) reporting guidelines. An information specialist performed a comprehensive search from the date of inception until February 2020, in seven bibliographic databases: Cochrane Library, MEDLINE, EMBASE, Web of Science, Cumulative Index to Nursing and Allied Health Literature (CINAHL), ScienceDirect, and IEEE Xplore. The selected studies considered all populations who provide and receive care in CBPHC settings, AI interventions that had been implemented, tested, or both, and assessed outcomes related to patients, health care providers, or CBPHC systems. Risk of bias was assessed using the Prediction Model Risk of Bias Assessment Tool (PROBAST). Two authors independently screened the titles and abstracts of the identified records, read the selected full texts, and extracted data from the included studies using a validated extraction form. Disagreements were resolved by consensus, and if this was not possible, the opinion of a third reviewer was sought. A third reviewer also validated all the extracted data. Results We retrieved 22,113 documents. After the removal of duplicates, 16,870 documents were screened, and 90 peer-reviewed publications met our inclusion criteria. Machine learning (ML) (41/90, 45%), natural language processing (NLP) (24/90, 27%), and expert systems (17/90, 19%) were the most commonly studied AI interventions. These were primarily implemented for diagnosis, detection, or surveillance purposes. Neural networks (ie, convolutional neural networks and abductive networks) demonstrated the highest accuracy, considering the given database for the given clinical task. The risk of bias in diagnosis or prognosis studies was the lowest in the participant category (4/49, 4%) and the highest in the outcome category (22/49, 45%). Conclusions We observed variabilities in reporting the participants, types of AI methods, analyses, and outcomes, and highlighted the large gap in the effective development and implementation of AI in CBPHC. Further studies are needed to efficiently guide the development and implementation of AI interventions in CBPHC settings.\"}, {\"paperId\": \"b430afae080f51c925da240aef5c2ec65c9ab2ae\", \"abstract\": \"Machine learning, a branch of artificial intelligence, learns from previous experience to optimize performance, which is ubiquitous in various fields such as computer sciences, financial analysis, robotics, and bioinformatics. A challenge is that machine learning with the rapidly growing \\\"big data\\\" could become intractable for classical computers. Recently, quantum machine learning algorithms [Lloyd, Mohseni, and Rebentrost, arXiv.1307.0411] were proposed which could offer an exponential speedup over classical algorithms. Here, we report the first experimental entanglement-based classification of two-, four-, and eight-dimensional vectors to different clusters using a small-scale photonic quantum computer, which are then used to implement supervised and unsupervised machine learning. The results demonstrate the working principle of using quantum computers to manipulate and classify high-dimensional vectors, the core mathematical routine in machine learning. The method can, in principle, be scaled to larger numbers of qubits, and may provide a new route to accelerate machine learning.\"}, {\"paperId\": \"0dffc6c6a019b6548841c7c7eb9ada13f812ed83\", \"abstract\": \"Evidence-driven disaster risk management (DRM) relies upon many different data types, information sources, and types of models to be effective. Tasks such as weather modelling, earthquake fault line rupture, or the development of dynamic urban exposure measures involve complex science and large amounts of data from a range of sources. Even experts can struggle to develop models that enable the understanding of the potential impacts of a hazard on the built environment and society.In this context, this guidance note explores how new approaches in machine learning can provide newways of looking into these complex relationships and provide more accurate, efficient, and useful answers. The goal of this document is to provide a concise, demystifying reference that readers, from project managers to data scientists, can use to better understand how machine learning can be applied in disaster risk management projects. There are many sources of information on this complex and evolving set of technologies. Therefore, this guidance note is aimed to be as focused as possible, providing basic information and DRM-specific case studies and directing readers to additional resources including online videos, infographics, courses, and articles for further reference. A machine learning (ML) algorithm is a type of computer program that learns to perform specific tasks based on various data inputs or rules provided by its designer. Machine learning is a subset of artificial intelligence (AI), but the two terms are often used interchangeably. For a thorough discussion of the differences and similarities of the terms ML and AI, see Section 2. As the name implies, an ML algorithm\\u2019s purpose is to \\u201clearn\\u201d from previous data and output a result that adds information and insight that was not previously known. This approach enables actions to be taken on the information gathered from the data; sometimes in near real time, like suggested web search results, and sometimes with longer term human input, like many of the DRM case studies presented in this document. Over the past few decades, there has been an enormous increase incomputational capacity and speed and available sensor data, exponentially increasing the volume of available data for analysis. This has allowed the capabilities of ML algorithms to advance to nearly ubiquitous impact on many aspects of society. Machine learning and artificial intelligencehave become household terms, crossing from academia and specialized industry applications into everyday interactions with technology\\u2014from image, sound, and voice recognition features of our smartphones to seamlessly recommending items in online shopping, from mail sorting to ranking results of a search engine. The same technology is being leveraged to answer bigger questions in society, including questions about sustainable development, humanitarian assistance, and disaster risk management.\"}, {\"paperId\": \"e4eaeef9c33c84acb56b399f307aa13ecca75fe4\", \"abstract\": \"\\u201cData science\\u201d\\u2014an interdisciplinary science encompassing statistics, mathematics, engineering, computer science, informatics, and/or decision science to elicit evidence-based informed shared decision-making rationales among parties (O\\u2019Connor, 2018; Park, 2018a, 2018b)\\u2014has grown by leaps and bounds since 2016 when Google DeepMind\\u2019s AlphaGo revealed its exceptional performance beyond human capability. Further, \\u201cdata science\\u201d is now evolving bi-exponentially all around the world. The leading Industry-University Collaboration initiatives for Artificial Intelligence (AI)-centered research include the institutions of (1) Google and University of Toronto (the Vector Institute), (2) Google DeepMind and University of Alberta, (3) IBM and Massachusetts Institute of Technology, and (4) Microsoft and University of Montreal and are expected to promote AI\\u2019s high-speed development (Bushey, 2017), improving the process of scientific discovery and the quality of decision-making given unpredictable conditions and advancing a data\\u2010driven economy (Ahalt et al., 2014; Park & Glenn, 2017). Such a consortium heralds that academic, economic, and sociological innovation/transformation would ensue sequentially and concurrently. Dare we dream how far science can go? The impact of these changes has not yet affected our everyday lives because these technological advances have not yet become an essential part of our lives. However, Virtual Reality (VR) developers are already almost finished constructing a real economy in several VR platforms (Park, 2018c). The AI-driven Autonomous Driving Car (ADC) is also expected to be released in 2020 (Business-Insider, 2016). By extension, Google\\u2019s Toronto smart city project will initiate its pilot program this summer, begin construction in 2020, and have its first residents by 2022 (Li, 2018). A drastic socio-structural change in our lives is already underway (Park, 2018c).\\u00a0 Each academic discipline is accordingly transitioning to a STEM (Science, Technology, Engineering, and Mathematics)-integrated education policy and reforming the curriculum to meet the demands of the times. Such an attempt mainly focuses on providing students a wide array of methodological expertise and hands-on experience in data science through taking additional courses delivered by STEM-related departments, upon students\\u2019 request. The course content includes combining large data sets with advanced analytics, e.g., (1) data/text/opinion/reality mining, (2) descriptive statistical analytics such as data visualization, social network analysis and cluster analysis, and (3) predictive analytics such as mathematical modeling, forecasting, simulation, machine learning, and so on (O\\u2019Connor, 2018). However, we must face an inconvenient truth about students\\u2019 preparation in statistics and mathematics, the foundational subjects of data science. Above all, before teaching students computer programming with Python or R, we first need to figure out WHAT the students want to mine from the flood of data. We also need to proactively identify whether the students understand HOW to explore their intellectual curiosity. That is, a well-established pre-knowledge about the primary specialization encompassing statistics and/or mathematics is required. For example, a decision science expert whose primary major is not nursing science has inherent limits performing data analyses appropriately reflecting the nature and value of nursing care. His/her evidence may be ineffective or even dangerous to nursing practice. No matter how perfect its scientific integrity is, knowledge without a foundation in the primary science may result in invalid policy-making in nursing practice (Park, 2018b), consequently threatening patient safety. Data science requires a totally different way of thinking and working from traditional, fixed framework-oriented, statistics-based nursing science (O\\u2019Connor, 2018; Park, 2018b), indicating that creativity and originality are highly valued in the new era of science. However, most undergraduate and even graduate nursing programs do not currently have an educational system synthesizing mathematics and computer programming such as Python or R, even though statistics is relatively well-integrated in curriculums. First, a lack of competent educators in both nursing science and data science is an urgent issue. However, insufficiency of scholarly preparation in mathematics among current nurse scientists is more urgent (Park, 2017), setting back the advancement of nursing science and going against the trend. An enormous challenge lies ahead due to the demanding nature of mathematics, with its long, high-intensity training and the wide gap between/among the viewpoints of academic disciplines\\u2014i.e., the humanities/social sciences and STEM. However, we should no longer avoid such a challenge. We nurses have an obligation to keep on doing our best to satisfy the ever-changing and complex needs of our patients in order to protect them from potential harm as well as enhance patients\\u2019 well-being. RECEIVED 25 May 2018, REVISED 25 June 2018, ACCEPTED 25 June 2018\"}, {\"paperId\": \"67d90e30f6f810e8c5ce0b06fa2801d368ec0625\", \"abstract\": \"Importance Artificial intelligence (AI) will play an increasing role in health care. In gynecologic oncology, it can advance tailored screening, precision surgery, and personalized targeted therapies. Objective The aim of this study was to review the role of AI in gynecologic oncology. Evidence Acquisition Artificial intelligence publications in gynecologic oncology were identified by searching \\u201cgynecologic oncology AND artificial intelligence\\u201d in the PubMed database. A review of the literature was performed on the history of AI, its fundamentals, and current applications as related to diagnosis and treatment of cervical, uterine, and ovarian cancers. Results A PubMed literature search since the year 2000 showed a significant increase in oncology publications related to AI and oncology. Early studies focused on using AI to interrogate electronic health records in order to improve clinical outcome and facilitate clinical research. In cervical cancer, AI algorithms can enhance image analysis of cytology and visual inspection with acetic acid or colposcopy. In uterine cancers, AI can improve the diagnostic accuracies of radiologic imaging and predictive/prognostic capabilities of clinicopathologic characteristics. Artificial intelligence has also been used to better detect early-stage ovarian cancer and predict surgical outcomes and treatment response. Conclusions and Relevance Artificial intelligence has been shown to enhance diagnosis, refine clinical decision making, and advance personalized therapies in gynecologic cancers. The rapid adoption of AI in gynecologic oncology will depend on overcoming the challenges related to data transparency, quality, and interpretation. Artificial intelligence is rapidly transforming health care. However, many physicians are unaware that this technology is being used in their practices and could benefit from a better understanding of the statistics and computer science behind these algorithms. This review provides a summary of AI, its applicability, and its limitations in gynecologic oncology. Target Audience Obstetricians and gynecologists, family physicians Learning Objectives After completing this CME activity, physicians should be better able to describe the basic functions of AI algorithms; explain the potential applications of machine learning in diagnosis, treatment, and prognostication of cervical, endometrial, and ovarian cancers; and identify the ethical concerns and limitations of the use of AI in the management of gynecologic cancer patients.\"}, {\"paperId\": \"8bbff77d2c633d446d82d5ce058332e2e72746fe\", \"abstract\": \"Summary Objective: To summarize significant research contributions to the field of artificial intelligence (AI) in health in 2018. Methods: Ovid MEDLINE\\u00ae and Web of Science\\u00ae databases were searched to identify original research articles that were published in the English language during 2018 and presented advances in the science of AI applied in health. Queries employed Medical Subject Heading (MeSH\\u00ae) terms and keywords representing AI methodologies and limited results to health applications. Section editors selected 15 best paper candidates that underwent peer review by internationally renowned domain experts. Final best papers were selected by the editorial board of the 2018 International Medical Informatics Association (IMIA) Yearbook. Results: Database searches returned 1,480 unique publications. Best papers employed innovative AI techniques that incorporated domain knowledge or explored approaches to support distributed or federated learning. All top-ranked papers incorporated novel approaches to advance the science of AI in health and included rigorous evaluations of their methodologies. Conclusions: Performance of state-of-the-art AI machine learning algorithms can be enhanced by approaches that employ a multidisciplinary biomedical informatics pipeline to incorporate domain knowledge and can overcome challenges such as sparse, missing, or inconsistent data. Innovative training heuristics and encryption techniques may support distributed learning with preservation of privacy.\"}, {\"paperId\": \"5c84a775c61cf406d9cc71d3bb0703a79d8877ee\", \"abstract\": \"The use of artificial intelligence (AI) and deep learning is progressively gaining credibility in medicine, particularly within the radiological sciences (1). In particular, convolutional neural networks (CNN) can solve even complex cases and provide diagnoses within short time frames with a level of accuracy that occasionally surpasses the capabilities of radiologists. In this vein, data scientists and data engineers have appeared before radiology society conferences and delivered interesting talks regarding AI. As a result of the use of AI in radiology, previously unheard-of terminology has been introduced to the field that will become common language in the near future. Thus, this paper seeks to define many of these common terms for the benefit of radiology practitioners. In their Canadian Association of Radiologists white paper on AI, Tang et al. very briefly mentioned an AI glossary (2). However, to the best of this author\\u2019s knowledge, this paper presents the first brief, practical glossary for AI in radiology in the English literature that can serve as a reference prototype for the purpose of simplifying the complex terminology. It is especially designed for radiology trainees and experienced radiologists who have an interest in AI. It is important to note that this is not intended as a precise or exhaustive glossary; therefore, some data science-specific, nonradiological terminology has been excluded from this paper. Instead, the most frequently encountered terms that relate to radiology are included below. Artificial intelligence (AI): Highly developed computer systems that have the ability to perform tasks that normally require human intelligence, such as visual perception, translation, speech recognition, image interpretation, interaction with humans (e.g. chatting), and decision-making. AI, sometimes called machine intelligence, is a type of intelligence demonstrated by machines (3,4). Algorithm: Step-by-step instructions completed by computers, including simple or complex tasks, such as setting reminders or identifying a group of people within a crowd (4). Backpropagation: The manner in which CNNs learn. They are able to recognize the differences between output and desired output and adjust calculations in reverse order of execution (5). Big data: A term for extremely large datasets that can be analyzed to reveal patterns, trends, and associations. Big data includes electronic medical records, which contain huge digital imaging archives, pathology department and laboratory archives, and millions of digital clinical notes and diagnoses (4). Blackbox: CNN\\u2019s (algorithm\\u2019s) unknown internal working pattern (6). Essentially unknown processing, especially within hidden layers. For example, it is difficult to comprehend how a CNN reaches an outcome. Cluster analysis (clustering): The task of grouping a set of objects similar to each other into clusters (7). Convolutional neural network (CNN): Deep learning intelligence network commonly used in diagnostic imaging (7). Data mining: A process used to extract usable data from a larger set of raw data. Data mining algorithms are particularly beneficial on complex datasets with a large number of variables and samples (8). Data science: A field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from data in various forms, both structured and unstructured. Similar to data mining (7). Data scientist: A person who analyzes and interprets data (9). Deep learning (machine learning): \\u201cA set of automatic pattern recognition methods that have been successfully applied across various problem domains, including biomedical image analysis\\u201d (10). Diagnosis (artificial intelligence): Concerned with the development of algorithms and techniques that are able to show whether the behavior of an AI system is correct\"}, {\"paperId\": \"d4a9f5dd8a13fb5d193e16c8877e5c2244d5ab82\", \"abstract\": null}, {\"paperId\": \"e95f3a8b8b7b7f73b17296da8f41da7ffd8cf4a4\", \"abstract\": \"Background: Artificial intelligence (AI) in healthcare delivery has become an important area of research due to the rapid progression of technology, which has allowed the growth of many processes historically reliant upon human input. AI has become particularly important in plastic surgery in a variety of settings. This article highlights current applications of AI in plastic surgery and discusses future implications. We further detail ethical issues that may arise in the implementation of AI in plastic surgery. Methods: We conducted a systematic literature review of all electronically available publications in the PubMed, Scopus, and Web of Science databases as of February 5, 2020. All returned publications regarding the application of AI in plastic surgery were considered for inclusion. Results: Of the 89 novel articles returned, 14 satisfied inclusion and exclusion criteria. Articles procured from the references of those of the database search and those pertaining to historical and ethical implications were summarized when relevant. Conclusions: Numerous applications of AI exist in plastic surgery. Big data, machine learning, deep learning, natural language processing, and facial recognition are examples of AI-based technology that plastic surgeons may utilize to advance their surgical practice. Like any evolving technology, however, the use of AI in healthcare raises important ethical issues, including patient autonomy and informed consent, confidentiality, and appropriate data use. Such considerations are significant, as high ethical standards are key to appropriate and longstanding implementation of AI.\"}, {\"paperId\": \"ac11a288a05c27b8868f97a712303fb0fa5df985\", \"abstract\": \"Sensory attributes of beer are directly linked to perceived foam-related parameters and beer color. The aim of this study was to develop an objective predictive model using machine learning modeling to assess the intensity levels of sensory descriptors in beer using the physical measurements of color and foam-related parameters. A robotic pourer (RoboBEER), was used to obtain 15 color and foam-related parameters from 22 different commercial beer samples. A sensory session using quantitative descriptive analysis (QDA\\u00ae ) with trained panelists was conducted to assess the intensity of 10 beer descriptors. Results showed that the principal component analysis explained 64% of data variability with correlations found between foam-related descriptors from sensory and RoboBEER such as the positive and significant correlation between carbon dioxide and carbonation mouthfeel (R = 0.62), correlation of viscosity to sensory, and maximum volume of foam and total lifetime of foam (R = 0.75, R = 0.77, respectively). Using the RoboBEER parameters as inputs, an artificial neural network (ANN) regression model showed high correlation (R = 0.91) to predict the intensity levels of 10 related sensory descriptors such as yeast, grains and hops aromas, hops flavor, bitter, sour and sweet tastes, viscosity, carbonation, and astringency.\\n\\n\\nPRACTICAL APPLICATIONS\\nThis paper is a novel approach for food science using machine modeling techniques that could contribute significantly to rapid screenings of food and brewage products for the food industry and the implementation of Artificial Intelligence (AI). The use of RoboBEER to assess beer quality showed to be a reliable, objective, accurate, and less time-consuming method to predict sensory descriptors compared to trained sensory panels. Hence, this method could be useful as a rapid screening procedure to evaluate beer quality at the end of the production line for industry applications.\"}, {\"paperId\": \"93145811e01413f4ea0e8f44737e93c40e31ab96\", \"abstract\": \"According to futurists, the artificial intelligence (AI) revolution in health care is here.1 While trending now, the concept is not new and was first introduced 70 years ago when Alan Turing described \\u201cthinking machines.\\u201d2 John McCarthy later coined the term \\u201cAI\\u201d to denote the idea of getting a computer to do things which, when done by people, are said to involve intelligence.3 What is new is the digitization of everything from electronic health records (EHRs) to genes and microbiomes, which provide the data that AI needs to learn. This conversion of images, handwritten notes, and pathology slides into 1\\u2019s and 0\\u2019s allows machines to perform a wide range of tasks, such as detecting retinopathy, skin cancer, and lung nodules.4-6 Even though this surge of available data exceeds what individuals and teams can realistically manage, computers have learned how to process these data to predict outcomes important to our patients, including opioid misuse, emergency department visits, and deaths.7-9 Advances like these led Andy Conrad, the CEO of Google\\u2019s life sciences subsidiary, to declare that in medicine, \\u201cthe most important tool is the computer.\\u201d10 This revolution has proceeded without AI publications in our journals, and our discipline is missing an opportunity to shape its future. A PubMed search reveals no AI or machine learning papers in Family Medicine, Annals of Family Medicine, or the Journal of the American Board of Family Medicine. For comparison, AI and machine learning papers number 18, 77, and 8 for Academic Medicine, JAMA, and the Journal of General Internal Medicine, respectively. While family medicine scholars are engaged in AI research11-13 and others reference it,14 family medicine\\u2019s voice needs to be amplified. Without our input, AI risks following the path of EHRs. When the Health Information Technology for Economic and Clinical Health (HITECH) Act was passed, policy makers believed that EHRs would lead to care that was more efficient, effective, and equitable,15 and EHRs have led to important advances in population health and quality.16 However, with increasing burnout and decreasing time with patients, many lament that EHRs cater to the needs of administrators and EHR vendors rather than physicians and patients.17 The usability and interoperability failures underlying these complaints are not the result of gaps in technological expertise. Instead, these failures emerged, in part, because end-users like ourselves have been insufficiently engaged in relevant design, policy, and implementation decisions. As AI spreads, our participation is needed to shape this revolution. Without our patient-centered orientation, AI has focused on delivering value to shareholders of technology companies rather than on problems that affect patients in our practices. Without our focus on value, AI has escalated health care costs and is available only to those with resources rather than those who would benefit the most. Without our focus on personal relationships, AI has further eroded face time by increasing the amount of time we spend interacting with computers. Without the breadth of our patients, AI has magnified existing biases. For example, as indicated in the literature, algorithms used by Artificial Intelligence and Family Medicine: Better Together\"}, {\"paperId\": \"83fb3eb9177b03116fe0c9e51ffed90c0aa2db01\", \"abstract\": \"Increased adoption of artificial intelligence (AI) systems into scientific workflows will result in an increasing technical debt as the distance between the data scientists and engineers who develop AI system components and scientists, researchers and other users grows. This could quickly become problematic, particularly where guidance or regulations change and once\\u2010acceptable best practice becomes outdated, or where data sources are later discredited as biased or inaccurate. This paper presents a novel method for deriving a quantifiable metric capable of ranking the overall transparency of the process pipelines used to generate AI systems, such that users, auditors and other stakeholders can gain confidence that they will be able to validate and trust the data sources and contributors in the AI systems that they rely on. The methodology for calculating the metric, and the type of criteria that could be used to make judgements on the visibility of contributions to systems are evaluated through models published at ModelHub and PyTorch Hub, popular archives for sharing science resources, and is found to be helpful in driving consideration of the contributions made to generating AI systems and approaches toward effective documentation and improving transparency in machine learning assets shared within scientific communities.\"}, {\"paperId\": \"5f2836111f7b2e3c9c4df81a7b9b8f183bfb5301\", \"abstract\": \"In this work, we present an ongoing, three year project funded by the Advanced Research Project Agency (ARPA-E) to develop an infrastructure to accelerate the development and deployment of artificial intelligence solutions for the electric grid. The project addresses the critical issues that we have identified as hampering the deployment of AI on electric grid measurements. These issues have stymied the potential for insight from high resolution grid measurements and generally hindered artificial intelligence innovation in the utility industry. The project consists of three main components. The first is the deployment of a diverse set of grid sensors to capture a variety of grid behaviour, both from the field and in simulation. The second is the deployment of a highly performant, scalable, cloud-based data management and AI platform designed for time series data to enable the easy storage, processing and analysis of grid sensor data. The third is the cultivation of an open research community of experts around the platform and data through useful educational material, code and data sharing, and data science competitions. Overall, the project will accelerate the development of analytics, machine learning, and AI by addressing existing gaps in data, tools, and people, with the aim of improving the electric grid.\"}, {\"paperId\": \"38fadf7c21c32b183fa3dcf32da1044e8441b813\", \"abstract\": \"Data frameworks, modules, and toolkits are for doing data but they\\u2019re also a good way to dive into the without actually understanding data In this you\\u2019ll how many of the most fundamental data science tools and algorithms work by implementing them from scratch. If you an aptitude for mathematics and some programming you get with the math and at the core of data and with hacking skills you need to get started as a data scientist. Today\\u2019s messy glut of data holds answers to questions no one\\u2019s even thought to ask. This book provides you with the know-how to dig those answers out. Get a crash course in Python Learn the basics of linear algebra, statistics, and probability\\u2014and understand how and when they're used in data science Collect, explore, clean, munge, and manipulate data Dive into the fundamentals of machine learning Implement models such as k-nearest Neighbors, Naive Bayes, linear and logistic regression, decision trees, neural networks, and clustering Explore recommender systems, natural language processing, network analysis, MapReduce, and databases of microbial community dynamics, Support Vector Machines, a robust prediction method with applications in bioinformatics, Bayesian Model Selection for Data with High Dimension, High dimensional statistical inference: theoretical development to data analytics, Big data challenges in genomics, Analysis of microarray gene expression data using information theory and stochastic algorithm, Hybrid Models, Markov Chain Monte Carlo Methods: Theory and Practice, and more. Provides the authority and expertise of leading contributors from an international board of authors Presents the latest release in the Handbook of Statistics series Updated release includes the latest information on Principles and Methods for Data Science file organization with UNIX/Linux shell, version control with Git and GitHub, and reproducible document preparation. This book is a textbook for a first course in data science. No previous knowledge of R is necessary, although some experience with programming may be helpful. The book is divided into six parts: R, data visualization, statistics with R, data wrangling, machine learning, and productivity tools. Each part has several chapters meant to be presented as one lecture. The author uses motivating case studies that realistically mimic a data scientist\\u2019s experience. He starts by asking specific questions and answers these through data analysis so concepts are learned as a means to answering the questions. Examples of the case studies included are: US murder rates by state, self-reported student heights, trends in world health and economics, the impact of vaccines on infectious disease rates, the financial crisis of 2007-2008, election forecasting, building a baseball team, image processing of hand-written digits, and movie recommendation systems. The statistical concepts used to answer the case study questions are only briefly introduced, so complementing with a probability and statistics textbook is highly recommended for in-depth understanding of these concepts. If you read and understand the chapters and complete the exercises, you will be prepared to learn the more advanced concepts and skills needed to become an expert. The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, and algorithmic foundations of data science, including machine learning, high-dimensional geometry, and analysis of large networks. Topics include the counterintuitive nature of data in high dimensions, important linear algebraic techniques such as singular value decomposition, the theory of random walks and Markov chains, the fundamentals of and important algorithms for machine learning, algorithms and analysis for clustering, probabilistic models for large networks, representation learning including topic modelling and non-negative matrix factorization, wavelets and compressed sensing. Important probabilistic techniques are developed including the law of large numbers, tail inequalities, analysis of random projections, generalization guarantees in machine learning, and moment methods for analysis of phase transitions in large random graphs. Additionally, important structural and complexity measures are discussed such as matrix norms and VC-dimension. This book is suitable for both undergraduate and graduate courses in the design and analysis of algorithms for data. As telescopes, detectors, and computers grow ever more powerful, the volume of data at the disposal of astronomers and astrophysicists will enter the petabyte domain, providing accurate measurements for billions of celestial objects. This book provides a comprehensive and accessible introduction to the cutting-edge statistical methods needed to efficiently analyze complex data sets from astronomical surveys such as the Panoramic Survey Telescope and Rapid Response System, the Dark Energy Survey, and the upcoming Large Synoptic Survey Telescope. It serves as a practical handbook for graduate students and advanced undergraduates in physics and astronomy, and as an indispensable reference for researchers. Statistics, Data Mining, and Machine Learning in Astronomy presents a wealth of practical analysis problems, evaluates techniques for solving them, and explains how to use various approaches for different types and sizes of data sets. For all applications described in the book, Python code and example data sets are provided. The supporting data sets have been carefully selected from contemporary astronomical surveys (for example, the Sloan Digital Sky Survey) and are easy to download and use. The accompanying Python code is publicly available, well documented, and follows uniform coding standards. Together, the data sets and code enable readers to reproduce all the figures and examples, evaluate the methods, and adapt them to their own fields of interest. Describes the most useful statistical and data-mining methods for extracting knowledge from huge and complex astronomical data sets Features real-world data sets from contemporary astronomical surveys Uses a freely available Python codebase throughout Ideal for students and working astronomers competitive by analyzing the data their support their decision-making an of the concepts, tools, and techniques behind the fields of data and artificial intelligence (AI) applied to business and industries. of Applied Data and Artificial in and all stages of data to AI their application real across industries\\u2014from and and together practice and science to successful from both and offline\"}, {\"paperId\": \"e3db110ebeef0e210300cc4c8cf62a7c756a2a25\", \"abstract\": \"Artificial intelligence\\u2014the mimicking of human cognition by computers\\u2014was once a fable in science fiction but is becoming reality in medicine. The combination of big data and artificial intelligence, referred to by some as the fourth industrial revolution,1 will change radiology and pathology along with other medical specialties. Although reports of radiologists and pathologists being replaced by computers seem exaggerated,2 these specialties must plan strategically for a future in which artificial intelligence is part of the health care workforce. Radiologists have always revered machines and technology. In 1960, Lusted predicted \\u201can electronic scannercomputer to examine chest photofluorograms, to separate the clearly normal chest films from the abnormal chest films.\\u201d3 Lusted further suggested that \\u201cthe abnormal chest films would be marked for later study by the radiologists.\\u201d3 Lusted\\u2019s intuitions were prescient: interpreting radiographs is pattern recognition; computers can recognize patterns and may be helpful because some roentgenographic analyses can be automated. Nearly 60 years after Lusted\\u2019s prediction, Enlitic, a technology company in Silicon Valley, inputted images of normal radiographs and radiographs with fractures into a computerized database.4 Using deep learning, a refined version of artificial neural networks, the\"}, {\"paperId\": \"51e4f0687aebdcb77ff7b5a07f370bc003c85f98\", \"abstract\": null}, {\"paperId\": \"acbc8962979a1499928af28d9453b4ee02815c05\", \"abstract\": \"Artificial intelligence has become a frequent topic in the news cycle, with reports of breakthroughs in speech recognition, computer vision, and textual understanding that have made their way into a bevy of products and services that are used every day. In contrast, clinical care has yet to reach the much lower bar of automating health care information transactions in the form of electronic health records. Medical leaders in the 1960s and 1970s were already speculating about the opportunities to bring automated inference methods to patient care,1 but the methods and data had not yet reached the critical mass needed to achieve those goals. The intellectual roots of \\u201cdeep learning,\\u201d which power the commodity and consumer implementations of presentday artificial intelligence, were planted even earlier in the 1940s and 1950s with the development of \\u201cartificial neural network\\u201d algorithms.2,3 These algorithms, as their name suggests, are very loosely based on the way in which the brain\\u2019s web of neurons adaptively becomes rewired in response to external stimuli to perform learning and pattern recognition. Even though these methods have had many success stories over the past 70 years, their performance and adoption in medicine in the past 5 years has seen a quantum leap. The catalyzing event occurred in 2012 when a team of researchers from the University of Toronto reduced the error rate in half on a well-known computer vision challenge using a deep learning algorithm.4 This work rapidly accelerated research and development in deep learning and propelled the field forward at a staggering pace. With the increased availability of digital clinical data, it remains to be seen how these deep learning models might be applied to the medical domain. In this issue of JAMA, Gulshan and colleagues5 present findings from a study evaluating the use of deep learning for detection of diabetic retinopathy and macular edema. To build their model, the authors collected 128 175 annotated images from the EyePACs database. Each image was rated by 3 to 7 clinicians for referable diabetic retinopathy, diabetic macular edema, and overall image quality. Each rater was selected from a panel of 54 board-certified ophthalmologists and senior ophthalmology residents. Using this data set, the algorithm learned to predict the consensus grade of the raters along each clinical attribute: referable diabetic retinopathy, diabetic macular edema, and image quality. To validate their algorithm, the authors assessed its performance on 2 separate and nonoverlapping data sets consisting of 9963 and 1748 images. On the validation data, the algorithm had high sensitivity and specificity. Only one of these values (sensitivity on the second validation data set) failed to be superior at a statistically significant level. The other performance metrics (eg, area under the receiver operating characteristic curve, negative predictive value, positive predictive value) were likewise impressive, giving the authors confidence that this algorithm could be of clinical utility. This work closely mirrors a recent \\u201cKaggle\\u201d contest in which 661 teams competed to build an algorithm to predict the grade of diabetic retinopathy, albeit on a smaller data set with fewer grades per image. Kaggle is a website that hosts machine learning and data science contests. Companies and researchers can post their data to Kaggle and have contestants from around the world build predictive models. In the diabetic retinopathy contest, nearly all of the top teams used some form of deep learning and had little to no knowledge of the eye or ophthalmology. The first-place team6 and secondplace team7 both used standard deep learning models and were data science practitioners, not medical professionals. Gulshan et al correctly pointed out that a prerequisite for a successful deep learning model is access to a large database of images with high-quality annotations. Accordingly, the investigators increased both the number of images available and the number of ratings per image, which allowed them to improve on the existing state of the art with respect to both Kaggle and the existing scientific literature. To build their algorithm, Gulshan et al leveraged a workhorse model in deep learning known as a convolutional neural network that has been critically important to recent advances in automatic image recognition. The convolutional neural network model used by the authors is known as the Inception-V3 network,8 which was developed by Google for entry in the Large Scale Visual Recognition Challenge, which it won in 2014. In this contest, known as ImageNet,9 researchers were given 1.2 million images that involve 1000 different categories that cover a wide variety of everyday objects, such as cats, dogs, automobiles, and different kinds of food. The goal of the contest was to build a classifier that could automatically recognize which object was present in an image and to identify which region of the image contained the object. This challenge was broad so that it covered many types of objects that a computer vision system could encounter in the real world. As a result of this contest, several techniques10-12 have been pioneered that improved the accuracy of these models immensely. As with the study by Gulshan et al, these improvements are beginning to trickle into other areas of computer vision, including medical image processing. For example, Gulshan et al not only used the same network that was originally built for ImageNet, they also used that network Editorial and Viewpoint\"}, {\"paperId\": \"7dcf11d9118494eea19211d73f145f2acf8cd6b8\", \"abstract\": \"The UKeiG Members\\u2019 Day, held at CILIP HQ on Friday 7th June 2019, showcased three experts: Michael Upshall (UNSILO), David Haynes (City, University of London) and Dr. Tony Russell-Rose (UXLabs, 2dSearch.) The event explored the impact of artificial intelligence on the knowledge, information management and library profession. Upshall kicked off the event by throwing down a gauntlet. Were taxonomies still relevant in an age when AI was transforming search by enabling concept clustering and semantic enrichment? Manual classification schemes, vocabularies, taxonomies and ontologies have always played an essential role in information retrieval but Upshall argues that they are expensive and fundamentally flawed; reactive not proactive. \\u201cThey will never be complete. They will never be large enough.\\u201d Haynes went on to explore the potential impact of AI on the information resource management cycle, leaving Russell Rose to deliver an illuminating crash course on the linguistic phenomena that make natural language processing (NLP) such a complex and multi-faceted field of research. \\u201cLanguage is ambiguous,\\u201d he said, \\u201cand the key tenet of NLP is resolving that ambiguity.\\u201d There was a consensus amongst the delegates that there are huge challenges and opportunities ahead for the LIS profession; that information science is fundamental to AI. \\nKeywords: AI, Artificial Intelligence, Alogorithms, Information Retrieval, Search, Machine Learning, Taxonomies, Natural Language Processing, NLP, Information Science.\"}, {\"paperId\": \"17af969fdf80f2e8d426984a04aeae659bee1dc5\", \"abstract\": \"New technologies can either improve or worsen health inequities.1 Innovative technologies involving artificial intelligence are no exception, particularly where they are adopted and implemented in health systems. Indeed, determining whether and how artificial intelligence might contribute to reducing or exacerbating health inequities has been identified as a priority research area by several stakeholders and by numerous ethics and policy guidance documents.2\\u20134 Understanding the connection between health inequities and artificial intelligence should be a priority when deploying these technologies in public health. Since public health activities typically target populations instead of individuals and require collective action instead of individual intervention,5 introducing artificial intelligence technologies to support these activities may influence (either positively or negatively, intentionally or unintentionally) health inequities more than in other areas. As such, identifying the distinctive equity considerations and dimensions that might emerge in the public health context is critical. However, doing so is not a straightforward task. First, we cannot simply look to past technological innovations to determine which health equity considerations or implications might arise with the use of artificial intelligence in public health because technological innovations and their diffusion in health systems each produce or interact with health inequities in novel ways.1 We may not be able to assume that the trends or pathways that create or prevent inequities will be the same when implementing artificial intelligence technologies as they are with other technological innovations. This limitation may be particularly challenging with artificial intelligence technologies given their use of big data and machine learning. Second, artificial intelligence represents a vast and sometimes contested area of study and application. Here we define artificial intelligence as a branch of computer science that explores the ability of computers to imitate aspects of intelligent human behaviour, such as problem-solving, reasoning and recognition.2 Technologies that are supported by artificial intelligence are therefore numerous, and include natural language processing, object recognition and reinforcement learning, among others. The ways in which these technologies might be deployed in public health are equally numerous, including digital disease surveillance, machine learning to predict incidences of noncommunicable diseases, and others. Finally, given that health inequities are often defined as differences in health that are unjust, even what should be counted as health inequities and what it means to achieve health equity may differ according to the nature of the new technology, how it is or has been integrated into health systems and our judgements about its interaction with the public\\u2019s health.6 As a result, before research or health system interventions in this area are developed or implemented, we should first seek to conceptually map the unique ways in which inequities might manifest when artificial intelligence is implemented or used in public health. Indeed, important work examining the unique equity dimensions associated with specific artificial intelligence technologies in this area has begun.7 Yet, we posit that there are general equity considerations and dimensions that can be identified and used as starting points for the reflection of equitable artificial intelligence in public health, and that it would be of benefit for the field to have these identified and enumerated. We will briefly describe four key equity considerations and dimensions and conclude by discussing how they can be used as starting points to further understand and enhance the equitable deployment of artificial intelligence in public health.\"}, {\"paperId\": \"57fbaf35321b2c4c4c0cc2b63e72bfb9c5d5d9c9\", \"abstract\": \"Graphs are widely used as a popular representation of the network structure of connected data. Graph data can be found in a broad spectrum of application domains such as social systems, ecosystems, biological networks, knowledge graphs, and information systems. With the continuous penetration of artificial intelligence technologies, graph learning (i.e., machine learning on graphs) is gaining attention from both researchers and practitioners. Graph learning proves effective for many tasks, such as classification, link prediction, and matching. Generally, graph learning methods extract relevant features of graphs by taking advantage of machine learning algorithms. In this survey, we present a comprehensive overview on the state-of-the-art of graph learning. Special attention is paid to four categories of existing graph learning methods, including graph signal processing, matrix factorization, random walk, and deep learning. Major models and algorithms under these categories are reviewed, respectively. We examine graph learning applications in areas such as text, images, science, knowledge graphs, and combinatorial optimization. In addition, we discuss several promising research directions in this field.\"}, {\"paperId\": \"7bf93d409033b67c349f840b94966a546253b85b\", \"abstract\": null}, {\"paperId\": \"70f3763e95698a45f7c24e41b6e2c1e2d0882045\", \"abstract\": null}, {\"paperId\": \"22b0499ea0cd9ed8a0e51f2ce17342d40e8fa6b6\", \"abstract\": \"Berkembangnya ilmu bioinformatika merupakan konsekuensi banyaknya data eksperimen laboratorium para peneliti biologi molekuler maupun biomedis. Selain pengembangan basis data terpusat yang merupakan kompetensi inti ilmu bioinformatika, pendekatan komputasi lain seperti pembelajaran mesin juga dikembangkan sehingga data tersebut dapat diolah menjadi informasi yang berguna bagi dunia kesehatan. Kajian ini akan menelaah perkembangan pendekatan pembelajaran mesin pada ilmu bioinformatika, dan aplikasinya pada dunia kesehatan terutama pada informatika kanker dan virus. Masa depan aplikasi medis dengan ilmu bioinformatika menarik karena melibatkan berbagai pendekatan baru seperti kecerdasan buatan dan biologi sistem. The development of bioinformatics science is a consequence of the massive data generation of laboratory experiments conducted by molecular biology and biomedical researchers. In addition to the development of a centralized database that is the core competence of bioinformatics science, other computing approaches such as machine learning are also developed so that the data can be processed into useful information for the human health. This review will examine the development of machine learning approaches in bioinformatics science, and its application to the human health, especially in cancer and virus informatics. The future of medical applications with bioinformatics science is exciting as it involves various new approaches such as artificial intelligence and system biology.\"}, {\"paperId\": \"762a0b6a9903670d89d6d671fd13f0beae80a2bf\", \"abstract\": null}, {\"paperId\": \"13fee92f3ed373c41a269c4f6155e9818ba5b241\", \"abstract\": \"In recent years, a paradigm shift on a global scale has occurred with respect to use of big data by information and communication technology (ICT). Key national strategies and legal systems are developed using artificial intelligence (AI), Internet of Things (IoT), and big data as the key, aiming to establish a data nation, also in Japan. Even in the medical field, the effect of using big data is highly expected. In addition, as a mechanism to support technology and research in the medical field, a bill concerning anonymous processing and medical information was submitted. Improvement of the social security situation is awaited not only for basic research and drug development, but also for realization of precision medicine. By using accumulated medical information, so-called big data, contribution to precision medicine that provides effective medical care to individual patients is also expected. In precision medicine, it is important to use genomic intelligence in addition to clinical information. The different genetic characteristics and constitutional personality of an individual patient can be grasped from the genomic information that constitutes an individual such as DNA sequence in addition to stored medical information. At the same time, it is necessary to secure high reliability of knowledge obtained by big data analysis in the medical field, particularly as precise knowledge is required for findings in analysis algorithms that realize precision medicine affecting human life. However, many of the production processes of knowledge obtained by machine learning are black boxes, and the basis is unclear. Therefore, it is necessary to ensure the reliability of the findings obtained by medical big data analysis. Currently, medical big data analysis technology handles only structured information represented by numerical values such as blood test results. In the future, it is necessary to treat unstructured information as an analysis target in order to deal with various diseases. We will also use AI for medical big data analysis technology. Unstructured information such as image information and text information is an area in which AI excels. In this edition of the Journal, nine articles were adopted, including three reviews and six original works. Of the original five clinical papers, two are based on the National Clinical Database (NCD), and one is the database of the Japan Society for Pancreatic Surgery. Thus, papers by retrospective studies based on nationwide data are rapidly increasing in the field of gastroenterological surgery. Such a trend may probably continue in the near future. However, it is not necessarily true that only big data reflect the correct medical situation. Among various data analyses, there is also strong opinion that quality data rather than big data are important in medical science; data should be secured not only in quantity but also in quality. In diversified medicine, large data are insufficient unless analysis based on high-quality data is carried out. The possibility that misunderstanding at the initial stage may adversely affect deep learning ICT cannot be denied. Nevertheless, how to master medical information in ICT analysis including AI and big data is a question to be answered. In such an era, personal characteristics of medical professionals are extremely important; valuable information gained from face-to-face interaction is the essence of medical treatment, as is the ability to discern the truth from few cases and an inquiring mind to determine its verity. It may be correct to use big data as a final screening or final confirmation in the clinical field of gastroenterological surgery.\"}, {\"paperId\": \"ae85aa763cdd4304b62fc0b4fc68cea89b5ce9d5\", \"abstract\": \"1189 term for extremely large datasets that can be analyzed computationally to reveal patterns, trends, and associations. A typical example of big data is the amount of information contained in today\\u2019s electronic medical records, which contain huge digital imaging archives, pathology department and laboratory archives, and millions of digital clinical notes and diagnoses that, when analyzed, could help physicians better identify trends in diagnoses. Natural language processing is another important field of computer science that is concerned with programming computers to process and understand natural human language text, such as the text contained in an electronic medical record. Finally, it is important to note that fastmoving advancements in computer hardware and processing, such as ultrafast graphic processing units and cloud computing, are enabling the accelerated applications of AI. All of the aforementioned components are part of the new world of the field of AI. AI has been developing for many years, but it has been advancing at a more rapid pace in recent years. First, we had computer-aided diagnosis, where computers were programmed by humans to detect certain characteristics on digital images. Computer-aided diagnosis was helpful but was limited to what the computer was programmed to detect. Through artificial neural networks, AI has introduced the ability for computers to learn from experience, thus enabling AI to go much further than CAD. However, the concept of AI as it applies to radiology is much easier in theory than in practice. Many complex and tedious steps must be taken before AI can play a major role in radiology [2, 3]. What is imperative is that radiologists must continually learn how to apply this new technology to improve the care of our patients. AI will likely replace some of what we do as radiologists, and, in other cases, AI will likely help us be more accurate in what we do today [4]. Those who believe that AI could replace radiologists don\\u2019t understand the complex role of radiologists in the care of patients. Although the naive concept that AI could replace radiologists will continue to be promoted, the truth of the matter is that AI will likely help radiologists achieve advancements way beyond those currently in motion through discovery and innovation. The uniquely human However, rather than fear these challenges, we should embrace them and look for new discoveries and innovations to help us meet those challenges. As the old English-language proverb says, \\u201cNecessity is the mother of invention.\\u201d In other words, we should look to discovery and innovation in radiology to help us meet the challenges facing us today. One of the most exciting emerging technologies on the radiology horizon is not a new scanner technology but, rather, what many refer to as artificial intelligence (AI). Many of us see AI as the next big important innovation in medicine and radiology. However, others see AI as a threat to our profession. Some have even gone so far as to say that radiologists could someday be replaced by AI [1]. Rather than fear this new technology, we should embrace it and discover new and exciting ways to improve and advance the field of radiology. To accomplish this, we, as radiologists, need to begin to understand the technology behind AI and look for ways to incorporate it into our practice and ultimately improve the care of our patients. AI is defined as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, written and spoken human language recognition, decision making, and translation between languages. A major component of AI is machine learning, which is a subfield of computer science that enables computers to learn without being explicitly programmed. This exciting technology of machine learning incorporates computational models and algorithms that are similar to the structure and function of our brain\\u2019s biologic neural networks. These computational models are often referred to as artificial neural networks. When these artificial neural networks process information (i.e., digital data) from numerous input flows, they have the ability to \\u201clearn\\u201d and alter their structure in much the same way that the neurons in our brain are altered with memory. Deep learning is a part of a broader family of machine learning methods based on learning representations of data such as recognizing characteristic images (i.e., face recognition). Deep learning is very significant as it relates to radiology because of its focus on recognizing objects in images. Big data is a Guest Editorial: Discovery and Artificial Intelligence\"}, {\"paperId\": \"d49ab3b23708570b259f6b7a4a28c49a9c543c43\", \"abstract\": \"Allosteric regulation is a common mechanism employed by complex biomolecular systems for regulation of activity and adaptability in the cellular environment, serving as an effective molecular tool for cellular communication. As an intrinsic but elusive property, allostery is a ubiquitous phenomenon where binding or disturbing of a distal site in a protein can functionally control its activity and is considered as the \\u201csecond secret of life.\\u201d The fundamental biological importance and complexity of these processes require a multi-faceted platform of synergistically integrated approaches for prediction and characterization of allosteric functional states, atomistic reconstruction of allosteric regulatory mechanisms and discovery of allosteric modulators. The unifying theme and overarching goal of allosteric regulation studies in recent years have been integration between emerging experiment and computational approaches and technologies to advance quantitative characterization of allosteric mechanisms in proteins. Despite significant advances, the quantitative characterization and reliable prediction of functional allosteric states, interactions, and mechanisms continue to present highly challenging problems in the field. In this review, we discuss simulation-based multiscale approaches, experiment-informed Markovian models, and network modeling of allostery and information-theoretical approaches that can describe the thermodynamics and hierarchy allosteric states and the molecular basis of allosteric mechanisms. The wealth of structural and functional information along with diversity and complexity of allosteric mechanisms in therapeutically important protein families have provided a well-suited platform for development of data-driven research strategies. Data-centric integration of chemistry, biology and computer science using artificial intelligence technologies has gained a significant momentum and at the forefront of many cross-disciplinary efforts. We discuss new developments in the machine learning field and the emergence of deep learning and deep reinforcement learning applications in modeling of molecular mechanisms and allosteric proteins. The experiment-guided integrated approaches empowered by recent advances in multiscale modeling, network science, and machine learning can lead to more reliable prediction of allosteric regulatory mechanisms and discovery of allosteric modulators for therapeutically important protein targets.\"}, {\"paperId\": \"c43e4825a2352e8737c350c04f3d4b104ff22bf2\", \"abstract\": null}, {\"paperId\": \"128024c935c9c7ba19595482605396624dc74818\", \"abstract\": null}, {\"paperId\": \"d590f35f89bda6e28b9ed4b21c59d51cebde75f7\", \"abstract\": null}, {\"paperId\": \"dad309a5a20c442c59bee49d5b25adaa2718de07\", \"abstract\": null}, {\"paperId\": \"410450f335477184ea53e3590ef2ce483e463647\", \"abstract\": null}, {\"paperId\": \"ae6872803e1c091e3c19c55b514723194fc2fef2\", \"abstract\": \"Radiation oncology, a major treatment modality in the care of patients with malignant disease, is a technology\\u2010 and computer\\u2010intensive medical specialty. As such, it should lend itself ideally to data science methods, where computer science, statistics, and clinical knowledge are combined to advance state\\u2010of\\u2010the\\u2010art care. Nevertheless, data science methods in radiation oncology research are still in their infancy and successful applications leading to improved patient care remain scarce. Here, we discuss data interoperability issues within and across organizational boundaries that hamper the introduction of big data and data science techniques in radiation oncology. At the semantic level, creating common underlying models and codification of the data, including the use of data elements with standardized definitions, an ontology, remains a work in progress. Methodological issues in data science and in the use of large population\\u2010based health data registries are identified. We show that data science methods and big data cannot replace randomized clinical trials in comparative effectiveness research by reviewing a series of instances where the outcomes of big data analyses and randomized trials are at odds. We also discuss the modern wave of machine learning and artificial intelligence as represented by deep learning and convolutional neural networks. Finally, we identify promising research avenues and remain optimistic that the data sources in radiation oncology can be linked to yield important insights in the near future. We argue that data science will be a valuable complement to, but not a replacement of, the traditional hypothesis\\u2010driven translational research chain and the randomized clinical trials that form the backbone of evidence\\u2010based medicine.\"}, {\"paperId\": \"d2c22ca8e04941d0afd9688944d9bb9e6a80b0c2\", \"abstract\": null}, {\"paperId\": \"14540152d38906a8e7ed8e0d53380d60c07987ed\", \"abstract\": \"\\n \\n \\n Autonomous physical science is revolutionizing materials science. In these systems, machine learning controls experiment design, execution, and analysis in a closed loop. Active learning, the machine learning field of optimal experiment design, selects each subsequent experiment to maximize knowledge toward the user goal. Autonomous system performance can be further improved with implementation of scientific machine learning, also known as inductive bias-engineered artificial intelligence, which folds prior knowledge of physical laws (e.g., Gibbs phase rule) into the algorithm. As the number, diversity, and uses for active learning strategies grow, there is an associated growing necessity for real-world reference datasets to benchmark strategies. We present a reference dataset and demonstrate its use to benchmark active learning strategies in the form of various acquisition functions.\\n \\n \\n \\n Active learning strategies are used to rapidly identify materials with optimal physical properties within a compositional phase diagram mapping a ternary materials system. The data is from an actual Fe-Co-Ni thin-film library and includes previously acquired experimental data for materials compositions, X-ray diffraction patterns, and two functional properties of magnetic coercivity and the Kerr rotation.\\n \\n \\n \\n Popular active learning methods along with a recent scientific active learning method are benchmarked for their materials optimization performance.\\n \\n \\n \\n Among the acquisition functions benchmarked, Expected Improvement demonstrated the best overall performance. We discuss the relationship between algorithm performance, materials search space complexity, and the incorporation of prior knowledge, and we encourage benchmarking more and novel active learning schemes.\\n\"}, {\"paperId\": \"71af6878cf6528e2df24a856809106059484e259\", \"abstract\": \"One of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is \\\"Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)\\\". Simulation might play a significant role to play in these developments as it can act as a testbed for such concepts and support the military decision makers in future operations that are enhanced by AI. Simulation is already making a significant impact in the development of AI outside of the defence sector. Companies such as DeepMind and Nvidia are using computer games and simulations to \\u201ctrain\\u201d AI and autonomous systems, analogous to humans training in simulations. The rate of progress is high, driven by increases in computing power, availability of data and improved algorithms. AI can now \\u201cbeat\\u201d humans at many computer and board games and is moving towards tackling more strategic games that have parallels with military C2. If such developments translate into the defence sphere then we could foresee humans and autonomous systems training in the same simulation systems, both separately and together, and the AI in the autonomous system being the same as that in the simulation. As autonomous systems proliferate across the nations, M&S technology and techniques might be used to improve the interoperability of autonomous systems. To maximise such synergies, it will be essential that NATO embraces all communities that have an interest in AI. Assessing the risks of potential adversary\\u2019s use of AI and commercial autonomous systems is also necessary. Despite recent advances, AI development still faces significant technological and ethical challenges and these must be monitored and addressed as necessary. 1.0 CONTEXT Artificial Intelligence or AI is a technology or concept that has developed over many decades and periodically becomes mainstream news. In the latter half of the 2010s this is still very much the case with regular forecasts of its impact on society, jobs and the world economy. Some of these predictions appear to be nearing reality and AI devices are even entering the home. AI also has the potential to influence and sometimes disrupt the ways that companies and organisations operate and AI-based technology revolutions are anticipated. AI also has enduring impact on media and culture and like much technology it can be considered to have beneficial and harmful uses and its impact has and will have political and ethical implications. Over the decades many AI predictions have come true to some degree but in some cases not at all or only partially. Good examples of this are to be found in transport where passenger aircraft have considerable levels of automation but society remains some way off from accepting pilotless passenger aircraft. For STO-MP-MSG-149 11 1 Developments in Artificial Intelligence \\u2013 Opportunities and Challenges for Military Modeling and Simulation railways, some are now fully automated whilst others continue to put high reliance on the human. Cars can now park themselves and have high levels of automation but are yet to be fully autonomous in all environments and applications. There is no doubt however, that there is a trend towards greater use of autonomous systems and AI. This is being driven by ever greater processing power together with the ability for very large data sets (\\u201cbig data\\u201d) to be captured and used to help build more capable AI. Such resources can also be accessed online in the cloud, driving down the cost of developing and distributing AI programs. The military have developed and deployed autonomous systems for a very long time, for example in the use of land and sea mines. In the 20th century proximity fuzes came into service that were semi-intelligent, sensing and exploding at the most appropriate time for the target. Analogue computers also assisted operators as part of fire control calculations and missiles and rockets in World War 2 become remotely piloted or fully autonomous. With the advent of digital computing, autonomy in military systems is commonplace, reducing the manpower requirement or in assisting the human, but there remains a significant ethical dimension in the use of fully autonomous systems. Developments in AI and autonomous systems outside of defence are of significant interest as they may provide answers how to better manage and interpret data within military command and control systems but also because they may enhance potential adversary\\u2019s capabilities. This was recognised in 2017 by the NATO Science and Technology Organization\\u2019s (STO) which made \\\"Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)\\\" one of its principal themes. The modelling and simulation (M&S) community has strived itself to develop AI, reducing or eliminating the need for human input. Sometimes termed Semi-Automated Forces (SAF) or Computer-Generated Forces (CGF) this has benefits in analysis and training, reducing the number of role players and improving consistency. Simulation itself is now being used to \\u201ctrain\\u201d AI/autonomous systems, as such environments are repeatable and controllable and can generate highly tailorable data output. However, reproducing credible and realistic behaviours in simulation remains a significant challenge and the M&S community continues to strive to enhance its AI. The computer games industry also sees AI as a challenge as games can easily lose their entertainment value if their AI is poorly implemented. 2.0 WHAT IS ARTIFICIAL INTELLIGENCE? Artificial intelligence (AI) is a broad topic area as it depends on what nature of human intelligence are being replicated and that AI technology can take many different forms. The Oxford Dictionary definition is \\u201cThe theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.\\u201d The nature of the intelligence can range from \\u201cnarrow\\u201d intelligence which is highly tailored or specialised through to artificial \\u201cgeneral\\u201d intelligence which is flexible, adaptive and inventive, much like the human brain. There are many approaches to AI for example, decision tress, fuzzy logic and neural nets with some approaches becoming synonymous with AI. For example, machine learning is an approach that gives \\\"computers the ability to learn without being explicitly programmed\\u201d by learning from and making predictions from data. In broad terms AI is the \\u2018what\\u2019, machine learning is an approach to the \\u2018how\\u2019, and self-driving cars might be the \\u2018why\\u2019. Machine learning methods are based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised. Neural nets or networks are computer systems modelled on the human brain and nervous system with an interconnected group of nodes, akin to the vast network of neurons in a brain. Deep or reinforcement learning, which is inspired by the way animals seem to learn, has taken the neural nets approach and added layers of nodes taking advantage of current day higher processing power and making significant advances in image recognition for example and is generally seen as at the current forefront of AI technology. Developments in Artificial Intelligence \\u2013 Opportunities and Challenges for Military Modeling and Simulation 11 2 STO-MP-MSG-149 An autonomous system builds on the use of AI and extends it into the physical world, in for example a robot or vehicle, requiring an awareness of the world through sensors, a task(s) and minimal human intervention. Some argue that some AI algorithms are not really showing intelligence but are a predetermined and limited set of responses to a predetermined and limited set of inputs. Professor Isbell of Georgia Tech suggests that systems should have two features before they can be considered AI. Firstly, they must learn over time as their environment changes. Secondly, their challenge must be demanding too for humans to learn, so a machine programmed to automate repetitive work would not be considered an AI system. Another example would be the AI in many computer video games; it may appear to represent human behaviour but this is preprogrammed and there is little or no learning over time. 3.0 HISTORICAL CONTEXT An overview of the general history of AI including work on AI by the M&S community will be provided to give context to the progress currently being made and the possible trajectory of the field into the future.\"}, {\"paperId\": \"d269c1aa3651e1f6de9b48b5c7921a6c47690701\", \"abstract\": \"Philip Ball, Science Writer, Nature, London, UK. S is typically slow work. It can take years or even decades for exploratory work on, say, a new concept in materials to become a product ready for the market place. But advances in artificial intelligence (AI) have the potential to greatly accelerate that tortuous process. Computer algorithms are increasingly helping with the exploring and understanding, and the direction of experimentation, modeling, and simulation. They are working in parallel with human creativity and ingenuity to find and refine new materials for tomorrow\\u2019s technologies. Launching one effort to harness computational and datadriven resources, the Materials Genome Initiative, in 2011, US President Barack Obama laid out the objective. \\u201cThe invention of silicon circuits and lithium-ion batteries made computers and iPods and iPads possible,\\u201d he said, \\u201cbut it took years to get those technologies from the drawing board to the market place. We can do it faster.\\u201d Yet sometimes the choices available to materials scientists are enough to make you despair. Take so-called \\u201chigh-entropy alloys,\\u201d1 which have high strength and are mixtures of five or more metallic elements; some may contain up to 20 elements. How can one ever hope to probe all the possible permutations and phases? Or take the exotic quantum mechanical properties discovered during the past decade or so in complex materials with compositions such as Ca10Cr7O28 and YbMgGaO4. How can we find out in any comprehensive, systematic way what other new and potentially useful behaviors might exist in combinations of elements that no one has thought to look at before? It is unfeasible to trawl blindly through all the options experimentally. Previously, the options were narrowed down largely through intuition. But human intuition becomes severely tested by both the range and complexity of the possible choices. Yet computer algorithms can now develop a kind of intuition too through the same process that we tend to use: looking for patterns and regularities in what we already know. This is machine learning (ML), an aspect of AI that aims to digest and generalize existing knowledge to find new solutions to problems. It is used today in all manner of applications in which a large amount of data exceeds human capability to assimilate it all\\u2014from genomics and drug design to analysis of financial markets and the development of gameplaying algorithms. It seems increasingly likely that some of the outstanding challenges in materials design will be solved this way too. The potential effect of AI in materials science, however, extends well beyond the discovery of new substances and compositions. Far from being merely a tool for automated materials exploration, said Benji Maruyama of the Air Force Research Laboratory in Dayton, Ohio, AI might supply nothing less than a new way of doing science, helping to improve, streamline, and guide the process of acquiring new knowledge about the materials universe (see the Materials Research Society OnDemand\\u00ae Webinar at mrs.org/ ondemand-ai). \\u201cWe are on a long-term trend towards more AI being integrated into the research process,\\u201d said Patrick Riley, an AI researcher at Google. \\u201cMachine-learning algorithms could help eliminate bottlenecks that appear in the research process,\\u201d said Kristofer Reyes, a computational materials scientist at the University at Buffalo, The State University of New York. Algorithms that learn from experience might help researchers to choose and design experiments, analyze the results, and generalize the knowledge extracted. And to judge from experience in other areas, such as the autonomous game-playing capabilities of AI, it is possible that such systems could take genuinely creative leaps beyond the bounds of human intuition. In the years to come, the relationship between humans and computers might be realigned: defined no longer in terms of users and tools, but as a collaboration. \\u201cCertainly, ML can predict materials with desirable properties,\\u201d said Bryce Meredig of Citrine Informatics in Redwood City, Calif., a company that has developed a commercially available AI platform for materials informatics. \\u201cBut if we think about a future in which all materials scientists have ML-based copilots, just like in our daily lives with Alexa and Google Assistant, the possibilities are much broader.\\u201d \\u201cWe\\u2019re still at the beginning of this journey,\\u201d cautioned Riley. But he feels that if the technical challenges can be met, AI could become an \\u201camazing research assistant.\\u201d\"}, {\"paperId\": \"5d660f9500a2364d50462073114deffae475fe7a\", \"abstract\": null}, {\"paperId\": \"b5f5dde39aa8cb8b5ecf56dce96f932be0d93dcb\", \"abstract\": null}, {\"paperId\": \"bcd960b2ca94caef25e3aa668a0a2844c394b477\", \"abstract\": \"The emergence of massive open online courses has initiated a broad national-wide discussion on higher education practices, models, and pedagogy. \\u00a0Artificial intelligence and machine learning courses were at the forefront of this trend and are also being used to serve personalized, managed content in the back-end systems. Massive open online courses are just one example of the sorts of pedagogical innovations being developed to better teach AI. This column will discuss and share innovative educational approaches that teach or leverage AI and its many subfields, including robotics, machine learning, natural language processing, computer vision, and others at all levels of education (K-12, undergraduate, and graduate levels). \\u00a0In particular, this column will serve the community as a venue to learn about the Symposium on Educational Advances in Artificial Intelligence (EAAI) (colocated with AAAI for the past four years); introductions to innovative pedagogy and best practices for AI and across the computer science curricula; resources for teaching AI, including model AI assignments, software packages, online videos and lectures that can be used in your classroom; topic tutorials introducing a subject to students and researchers with links to articles, presentations, and online materials; and discussion of the use of AI methods in education shaping personalized tutorials, learning analytics, and data mining\"}, {\"paperId\": \"90f2339f7c17125e3a55d6f70891d3481a254d5b\", \"abstract\": null}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 800, \"next\": 900, \"data\": [{\"paperId\": \"a73eef4e0ec8e49be1c0e6abb04cc1967a681c50\", \"abstract\": null}, {\"paperId\": \"33ee99463e2258c600f05e7b212d19d227adfac9\", \"abstract\": \"Noteworthy changes coming to the practice of medicine require significant medical education reforms. While proposals for such reforms abound, they are insufficient because they do not adequately address the most fundamental change\\u2014the practice of medicine is rapidly transitioning from the information age to the age of artificial intelligence. Increasingly, future medical practice will be characterized by: the delivery of care wherever the patient happens to be; the provision of care by newly constituted health care teams; the use of a growing array of data from multiple sources and artificial intelligence applications; and the skillful management of the interface between medicine and machines. To be effective in this environment, physicians must work at the top of their license, have knowledge spanning the health professions and care continuum, effectively leverage data platforms, focus on analyzing outcomes and improving performance, and communicate the meaning of the probabilities generated by massive amounts of data to patients, given their unique human complexities. The authors believe that a \\u201creboot\\u201d of medical education is required that makes better use of the findings of cognitive psychology and pays more attention to the alignment of humans and machines in education and practice. Medical education needs to move beyond the foundational biomedical and clinical sciences. Systematic curricular attention must focus on the organization of professional effort among health professionals, the use of intelligence tools involving large data sets, and machine learning and robots, all the while assuring the mastery of compassionate care.\"}, {\"paperId\": \"9b9654ec375b73b2001ec84cf59227e816f005e7\", \"abstract\": \"Artificial intelligence is driving one of the most important revolutions in organic chemistry. Multiple platforms, including tools for reaction prediction and synthesis planning based on machine learning, have successfully become part of the organic chemists\\u2019 daily laboratory, assisting in domain-specific synthetic problems. Unlike reaction prediction and retrosynthetic models, the prediction of reaction yields has received less attention in spite of the enormous potential of accurately predicting reaction conversion rates. Reaction yields models, describing the percentage of the reactants converted to the desired products, could guide chemists and help them select high-yielding reactions and score synthesis routes, reducing the number of attempts. So far, yield predictions have been predominantly performed for high-throughput experiments using a categorical (one-hot) encoding of reactants, concatenated molecular fingerprints, or computed chemical descriptors. Here, we extend the application of natural language processing architectures to predict reaction properties given a text-based representation of the reaction, using an encoder transformer model combined with a regression layer. We demonstrate outstanding prediction performance on two high-throughput experiment reactions sets. An analysis of the yields reported in the open-source USPTO data set shows that their distribution differs depending on the mass scale, limiting the data set applicability in reaction yields predictions.\"}, {\"paperId\": \"fffa2cec0b796707c975067654851e878a00a469\", \"abstract\": \"Modern advancements in mathematical analysis, computational hardware and software, and availability of big data have made possible commoditized machines that can learn to operate as investment managers, financial analysts, and traders. We briefly survey how and why AI and deep learning can influence the field of Finance in a very general way. Revisiting original work from the 1990s, we summarize a framework within which machine learning may be used for finance, with specific application to option pricing. We train a fully-connected feed-forward deep learning neural network to reproduce the Black and Scholes (1973) option pricing formula to a high degree of accuracy. We also offer a brief introduction to neural networks and some detail on the various choices of hyper-parameters that make the model as accurate as possible. This exercise suggests that deep learning nets may be used to learn option pricing models from the markets, and could be trained to mimic option pricing traders who specialize in a single stock or index. 1 Artificial Intelligence: A Reincarnation Artificial intelligence (AI) is in its second new age. While the notion that machines are capable of exhibiting human levels of intelligence saw its beginning implementations in the 1950s, success in creating AI machines was thin, and mostly, AI was deemed to be a failed enterprise. In the past few years, there has been a resurgence of AI, \\u2217Robbie Culkin is an undergraduate Computer Science and Engineering student, and Sanjiv Das is Professor of Finance and Data Science. They may be reached at rculkin@scu.edu, srdas@scu.edu.\"}, {\"paperId\": \"9acc43731195cd3234f9e198331996161e8b39aa\", \"abstract\": \"Numerous countries like China, France and Japan have declared Artificial Intelligence (AI) a key technology and announced comprehensive plans to promote research and development in AI. The German government has also begun to work on an AI strategy and just published a first blueprint with the core themes. In this paper, we argue that such a strategy needs to be broad and comprehensive, focusing on the development of an internationally-competitive AI ecosystem in Germany. \\nA strong AI ecosystem is characterized by strong networks between science, economic actors (big companies and startups alike) and society at large. Innovations arise in particular from close exchanges and collaboration between researchers, developers, universities, companies, investors and startups. To promote such an ecosystem, a wide range of different political measures on different levels have to be integrated into a broader, comprehensive strategy. \\nThis paper discusses the central building blocks of an AI ecosystem in Germany, and offers concrete ideas and recommendations for an AI strategy based on this ecosystem approach. \\n1. AI research: Compared to other countries, Germany is lagging behind in research expenditures and must drastically increase them. Research support needs to be open to different technological approaches within AI. It also needs to be more agile to better react to emerging trends and new opportunities in AI research. Better work conditions overall are needed to compete for the best AI talent worldwide as well as clear benchmarks to measure progress in AI research. \\n2. Development of AI competencies across society: We do not only need top research. We also need broadly distributed AI competencies in society. Thus AI should not only be taught in computer sciences, but core AI modules should also be integrated into engineering and natural science programs, and be taught at schools of applied sciences. \\n3. Data as a basic resource for AI development: A strong AI ecosystem needs data for research and for the development of AI applications in industry, particularly with regard to deep learning (DL). This dimension of the ecosystem needs far more attention in Germany. Possible approaches to mobilize data for AI include the development of data pools and more advanced methods of anonymizing or synthesizing data. It is hard to compete with the big Internet platforms from the United States and China in terms of quantity of data. Instead, special emphasis on machine data, quality of data and alternative approaches to AI that can work with little data could be the cornerstones of an alternative path to a strong AI ecosystem. 4. Infrastructure demands for AI: Deep learning requires not only huge amounts of data but also great computing power. A national AI strategy would address the question of how we can ensure middle- and long-term access to the most powerful processing hardware possible for German AI research and applications. \\n5. AI development and AI application in the economy: The German economy and industry already struggles with digitalization. AI exacerbates this issue because it represents the next step of digitalization. Small- and medium-sized businesses in Germany, known as the Mittelstand, especially need support. This support could be, for example, through state-funded AI laboratories, in which companies can experiment with AI with little risk and at low costs. Mobilizing venture capital through public funds and providing better incentives for AI investments represent two more critical challenges. 6. Societal dimension of AI: The ethical and regulatory questions regarding AI need to be openly discussed and require input from many different stakeholders in German society. Here, we already see numerous initiatives and approaches, representing the topic\\u2019s arrival on the political agenda. However, more has to be done to make AI competencies and technologies more familiar within society. \\n7. A national AI strategy in an international context: Germany can only succeed in the international competition in the long-term as part of an EU-wide approach. Striving for cooperation with France offers the chance to push for a comprehensive European AI strategy. Germany, and Europe as a whole, have to become more conscious of their strategic interests in AI and act accordingly. \\nA German AI strategy should focus on the ecosystem approach and propose concrete ideas and recommendations. The strategy also needs to address how we can identify and, if possible, measure how the strategy is being implemented, and how the AI ecosystem in Germany is developing. There are many important indicators that policy makers should consult in order to evaluate the effect of their actions: the attractiveness of German institutes and universities for leading international AI researchers, the number and quality of AI patents, achievements in publishing and visibility at the most important international AI conferences, venture capital investments, the founding of firms, or the number of companies with strong AI competencies and their growth. The good thing is that Germany does not have to start from scratch; numerous countries have already published national AI strategies in which many good ideas can be found. Now is the time for Germany to follow suit. Only then can Germany become a leader of AI development.\"}, {\"paperId\": \"9d1f9c5c2be2636328b14a4d4678adf5d17591d7\", \"abstract\": \"Artificial Intelligence platforms are driven by sophisticated algorithms which have been incorporated into A.I. robots. These algorithms are also programmed to be self-teaching. This technology has resulted in producing a \\u2018super intelligent\\u2019 robot, the current best example of which is IBM\\u2019s Watson. Watson is being increasingly applied to perform a variety of tasks in the medical field, tasks which had formerly been the exclusive preserve of doctors. A.I. is replacing doctors in fields such as interpreting X-rays and scans, performing diagnoses of patients\\u2019 symptoms, in what can be described as a \\u2018consulting physician\\u2019 basis. A.I. is also being used in psychology where robots are programmed to speak to patients and counsel them. Robots have also been designed to perform sensitive surgical techniques. One is therefore able to confidently predict that the role of robots in medicine is going to increase exponentially in the future. Because medicine is not an exact science it is possible that Watson, to use one example of an existing robot, can make errors which result in injury to patients. The injured patient should then be entitled to sue for damages, as they would have been able to do if the injury had been caused by a real doctor. However, the problem which arises in this regard is that the law of torts has developed to regulate the actions of natural persons. Watson, and similar A.I. platforms, are not natural persons. This means that a patient seeking redress cannot rely on existing law relating to medical negligence or malpractice to recover damages. It is therefore imperative that appropriate legislation is passed to bridge this gap and allow the apportionment of damages to a patient which have resulted from the actions of an A.I. robot. *Correspondence to: Michael Lupton, Professor, Bond University, Queensland, Australia, E-mail: mlupton@bond.edu.au Received: June 19, 2018; Accepted: July 04, 2018; Published: July 09, 2018 Definition of A.I. and some applications A.I. is usually defined as \\u2018the capability of a computer program to perform tasks or reasoning processes that we usually associate with intelligence in a human being [1]. Artificial intelligence is inextricably linked to the ever-increasing capabilities of algorithms. A.I. has been insidiously infiltrating our lives for a number of years in the form of the GPS built into or attached to motor cars and from its humble beginnings as an animated map it has now evolved to the point where it can control or \\u2018drive\\u2019 the motor car: Spam filters are based on A.I. The Google translate service, which is now capable of translating from and to more than 70 languages is the product of statistical machine learning which in turn is imbedded in A.I. The face recognition technology employed for security purposes at airports and railway stations is also driven by A.I. The much-used iPhone app, Siri, which understands us when we speak to it and mostly responds in an intelligent way, is based on A.I. algorithms developed to facilitate speech understanding. These are just a few examples of how A.I. is increasingly becoming an essential component of everyday life for the average citizen in developed countries. The examples above do not even include the so-called Internet of things which is linked to the application of cognitive computing capabilities [2]. Computing giant IBM continues to invest massive resources in order to employ its Watson cognitive computing system to finance, personalised education and of particular interest to this article, to the field of medicine [1]. The definition of A.I. usually identifies the fact that the field can be divided into so-called \\u2018strong\\u2019 A.I. which refers to the creation of computer systems whose behaviour at certain levels would be indistinguishable from that of humans. The alternative to the above system would be \\u2018weak\\u2019 A.I., which would examine human cognition and decide how it could be applied to assist and support our limited human cognition in multiple situations e.g. modern fighter aircraft are filled with such \\u2018weak\\u2019 A.I. systems. \\u2018Weak\\u2019 A.I. systems will help pilots to maximize the potential of their sophisticated aircraft, but they will not be empowered to have an independent existence and decisionmaking process [3]. The goal with which A.I. systems in Medicine have been created is to assist and support healthcare workers to execute their normal duties more efficiently, especially in those areas which require the manipulation of data and knowledge [4]. This characteristic of the system will allow it to evaluate an electronic medical record system on an ongoing basis. This constant analysis of the records will enable it to alert the clinician when it detects patterns in clinical data which suggest significant changes in a patient\\u2019s condition or if it detects a probable contraindication to a planned treatment [5]. The fact that the algorithms in A.I. systems have the capacity to learn, will lead to the discovery of new phenomena and thus the creation of new medical knowledge. On the other hand, A.I. is a form of automation that will reduce the number of current jobs in the medical field, and there is as yet no certainty that new jobs in sufficient quantities will be created to replace those lost [5]. Lupton M (2018) Some ethical and legal consequences of the application of artificial intelligence in the field of medicine Volume 18(4): 2-7 Trends Med, 2018 doi: 10.15761/TiM.1000147 Major concerns arising from A.I. Humans owe their dominant position in the world to their intelligence not their speed or strength. Therefore, the development of A.I. systems that are \\u2018super intelligent\\u2019 in that they exceed the ability of the best human brains in practically every field could impact drastically on humanity and we should proceed down this road with care [6]. It is human intelligence which allowed man to develop tools and the technology to enable us to control our environment. It is therefore not illogical to deduce that a super intelligent system would likewise be capable of developing its own tools and technology for exerting control [7]. The dangers attached to the above occurring is that such A.I. systems would not share our evolutionary history and there is therefore no reason to believe that they would be driven by human characteristics such as a lust for power. Their default position is likely to be that they are driven to compete for and acquire resources currently used by humans, which is likely given the fact that the system is devoid of the human sense of fairness, compassion or conservatism [8]. An onus therefore rests on the creators of A.I. systems to construct and train them in such a way that the systems are wired to develop \\u2018moral\\u2019 and \\u2018ethical\\u2019 behaviour patterns so as to ensure that these super intelligent A.I. systems have a positive rather than a negative impact on society, or to use the terminology of A.I scientists, that these systems are \\u2018aligned with human interests\\u2019. To achieve this end designers, need to develop and employ agent architectures which avert the incentives of A.I. systems to manipulate and deceive their human operators, and instead remain tolerant of programmer errors [9]. Just one example of the unexpected outcomes of a task allocated to an A.I. agent is described by authors Bird and Lydell. It involved a generic algorithm which was tasked with making an oscillator. The algorithm instead repurposed the tracks on a printed circuit board on the mother board, to act as a makeshift radio to amplify oscillating signals from nearby computers. Had the algorithms been simulated on a virtual circuit board which only possessed the features that seemed relevant to the problem, it would have delivered an outcome closer to what its controllers had anticipated [4]. The above example clearly illustrates the ability of an A.I. agent, operating in the real world, to use resources in unexpected ways by for example finding \\u2018shortcuts\\u2019 or \\u2018cheats\\u2019 not accounted for in a simplified model [10]. A.I. and medical diagnosis The remarks above illustrate the scope and potential of A.I. systems. It is therefore not surprising that there is ample opportunity to employ A.I. systems in the field of medicine, some of which we will discuss below.\"}, {\"paperId\": \"d21bc07ca53034f37b0d4dd6ab7e2cb704e9aa0c\", \"abstract\": \"The Rough Set (RS) theory can be considered as a tool to reduce the input dimensionality and to deal with vagueness and uncertainty in datasets. Over the years, there has been a rapid growth in interest in rough set theory and its applications in artificial intelligence and cognitive sciences, especially in research areas such as machine learning, intelligent systems, inductive reasoning, pattern recognition, data preprocessing, knowledge discovery, decision analysis, and expert systems. This paper discusses the basic concepts of rough set theory and point out some rough set-based research directions and applications. The discussion also includes a review of rough set theory in various machine learning techniques like clustering, feature selection and rule induction. General Terms Information and decision systems, (in)discernibility , approximation spaces, rough sets, rough membership functions, reducts, decision rules, dependencies of attributes. Clustering, Rule Induction, Feature Selection\"}, {\"paperId\": \"59a80bd36ac4cddec81ee3ef84318969f9997fb1\", \"abstract\": \"Machine learning is one of the oldest subfields of artificial intelligence and is concerned with the design and development of computational systems that can adapt themselves and learn. The most common machine learning algorithms can be either supervised or unsupervised. Supervised learning algorithms generate a function that maps inputs to desired outputs, based on a set of examples with known output (labeled examples). Unsupervised learning algorithms find patterns and relationships over a given set of inputs (unlabeled examples). Other categories of machine learning are semi-supervised learning, where an algorithm uses both labeled and unlabeled examples, and reinforcement learning, where an algorithm learns a policy of how to act given an observation of the world. Data mining is a more recently emerged field than machine learning is. Traditional data analysis techniques often fail to process large amounts of-often noisy-data efficiently. The scope of data mining is the knowledge discovery from large data amounts with the help of computers. It is an interdisciplinary area of research, that has its roots in databases, machine learning, and statistics and has contributions from many other areas such as information retrieval, pattern recognition, visualization, parallel and distributed computing. The main difference between machine learning and data mining is that machine learning algorithms focus on their effectiveness, whereas data mining algorithms focus on their efficiency and scalability. Recently, the collection of biological data has been increasing at explosive rates due to improvements of existing technologies as well as the introduction of new ones that made possible the conduction of many large scale experiments. example of the rapid biological data accumulation is the exponential growth of GenBank (Figure 1), the U.S. NIH genetic sequence database (www.ncbi.nlm.nih.gov). The explosive growth in the amount of biological data demands the use of computers for the organization, the maintenance and the analysis of these data. This led to the evolution of bioinformatics, an interdisciplinary field at the intersection of biology, computer science, and information technology. Luscombe et al. (2001) identify the aims of bioinformatics as follows: \\u2022 The organization of data in a way that allows researchers to access existing information and to submit new entries as they are produced. \\u2022 The development of tools that help in the analysis of data. \\u2022 The use of these tools to analyze the individual systems in detail, in order to gain new biological insights. There is a strong interest in methods of knowledge discovery \\u2026\"}, {\"paperId\": \"dbc0764f1c752ac6dc8dc2b468201fe362a2f037\", \"abstract\": \"of a five-game match in Seoul, Korea. (AlphaGo is a program developed by DeepMind, a British AI company acquired by Google two years ago.) After Deep Blue\\u2019s victory against chess world champion Gary Kasparov in 1997, the game of Go was the next grand challenge for game-playing artificial intelligence. Go has defied the brute-force methods in game-tree search that worked so successfully in chess. In 2012, Communications published a Research Highlight article by Sylvain Gelly et al. on computer Go, which reported that \\u201cPrograms based on Monte-Carlo tree search now play at human-master levels and are beginning to challenge top professional players.\\u201d AlphaGo combines tree-search techniques with searchspace reduction techniques that use deep learning. Its victory is a stunning achievement and another milestone in the inexorable march of AI research. By using deep\\u2013learning techniques to prune the search tree, AlphaGo can be said to augment bruteforce search with \\u201cintuition,\\u201d which it has developed by playing numerous games against itself. (Google said AlphaGo does not use Lee\\u2019s games as its training data.) By relying on learned \\u201cintuition,\\u201d AlphaGo is able to overcome the so-called Polanyi\\u2019s Paradox. The philosopher Michael Polanyi observed in 1966, \\u201cWe can know more than we can tell ... The skill of a driver cannot be replaced by a thorough schooling in the theory of the motorcar.\\u201d Some labor economists have viewed Polanyi\\u2019s Paradox as a major barrier for AI, arguing it implies a limit on its potential to automate human jobs. AlphaGo\\u2019s victory demonstrates machine learning provides a path around that barrier. Indeed, the automation of driving has been a major challenge for AI research over the past decade. In 2004, economists argued driving was unlikely to be automated in the near future due to Polanyi\\u2019s Paradox. A year later, a Stanford autonomous vehicle won a DARPA Grand Challenge by driving over 100 miles along an unrehearsed desert trail. Now, more than a decade later, both technology companies and car companies vigorously pursue the automation of driving. I expect the technical challenges to be resolved in the coming decade. In 1843, Ada Lovelace, well known for her work on Charles Babbage\\u2019s Analytical Engine, wrote to Babbage that she wished to see computing technology developed for \\u201cthe most effective use of mankind.\\u201d It is difficult for me to think of any computing technology other than automated driving that can be deployed in a decade or two and with such benefit for humanity. About 1.25 million people worldwide die from car accidents every year. Over 90% of these accidents are caused by human error. By automating driving, we could save over a million lives a year, as well as avoid countless injuries. At the same time, the automation of driving would have a huge disruptive effect on the global economy. Existing industries will shrivel, and whole new industries will rise. In the U.S., close to 10% of all jobs involve operating a vehicle and we can expect to see the majority of these jobs disappear. The human cost of such a profound change cannot be underestimated. As a precedent we can see what happened to U.S. manufacturing over the past 35 years. While manufacturing output in constant dollars is at an all-time high, manufacturing employment peaked around 1980 and is today lower than it was in 1947. The disappearance of millions of jobs due to automation may explain a recent, rather shocking, finding by economists Angus Deaton, winner of the 2015 Nobel Memorial Prize in Economic Science, and Anne Case, that mortality for white middle-aged Americans has been increasing over the past 25 years, due to an epidemic of suicides and afflictions stemming from substance abuse. Thus, the automation of driving would be hugely beneficial, saving lives and preventing injuries on a massive scale. At the same time, it would have a profoundly adverse impact on the labor market. In the balance, life saving and injury prevention must take precedence, and we have a moral imperative to develop and deploy automated driving. The solution to the labor problem will not be technical, but sociopolitical. As computing professionals, we also have a moral imperative to acknowledge the adverse societal consequences of the technology we develop and to engage with social scientists to find ways to address these consequences. Follow me on Facebook, Google+, and Twitter.\"}, {\"paperId\": \"3d45e4dcf9204efd279f2e60653e2eababbc3bac\", \"abstract\": null}, {\"paperId\": \"8bc5d74352893ce58f48ed0929c96170437dbcfe\", \"abstract\": null}, {\"paperId\": \"d06a7079b6a011a6fac96c9708d41b9f062ae207\", \"abstract\": \"Artificial Intelligence (AI) is a new science and technology that researches and develops theories, methods, technologies and application systems for simulating, extending and expanding human intelligence, and is an important branch of computer science (Iranirad et al., 2017; Evans, 2019). It is an attempt to understand the nature of intelligence and thus to produce an intelligent machine that can respond in a similar way to human intelligence. This field covers a very wide range, including robots, image recognition, language recognition, natural language processing, food security and expert systems etc (Li et al., 2018; Jackson & Cameron, 2020; Hollands et al., 2018). There are various traditional machine learning algorithms for AI implementation, but the most effective and powerful algorithm is the deep learning algorithm, so AI technology at this stage usually refers to deep learning (Zeng et al., 2019). With the rapid social and economic development, and the continuous improvement of living standards, people have paid more and more attention to their health issues and hoped to get high-quality and convenient health care services. The application of AI deep learning technology in medical industry has also obtained tentatively favorable effect, especially in data processing of medical history and predication of illness conditions (Lustberg et al., 2018). As one of the most common cardiovascular diseases in China, the patients with coronary heart disease have severe conditions and rapid changes, heavy clinical treatment and nursing, presenting various risks of potential cardiovascular disease (Weber et al., 2019). Therefore, the patients with coronary heart disease were taken as research objects in this paper to analyze and explore the application value of AI deep learning techniques in prediction of possible complications of this disease, and its effect on improvement of nursing quality.\"}, {\"paperId\": \"07a5a4418c0b84a88c2a03487affcccc4bf245e7\", \"abstract\": \"Background Coding of underlying causes of death from death certificates is a process that is nowadays undertaken mostly by humans with potential assistance from expert systems, such as the Iris software. It is, consequently, an expensive process that can, in addition, suffer from geospatial discrepancies, thus severely impairing the comparability of death statistics at the international level. The recent advances in artificial intelligence, specifically the rise of deep learning methods, has enabled computers to make efficient decisions on a number of complex problems that were typically considered out of reach without human assistance; they require a considerable amount of data to learn from, which is typically their main limiting factor. However, the C\\u00e9piDc (Centre d\\u2019\\u00e9pid\\u00e9miologie sur les causes m\\u00e9dicales de D\\u00e9c\\u00e8s) stores an exhaustive database of death certificates at the French national scale, amounting to several millions of training examples available for the machine learning practitioner. Objective This article investigates the application of deep neural network methods to coding underlying causes of death. Methods The investigated dataset was based on data contained from every French death certificate from 2000 to 2015, containing information such as the subject\\u2019s age and gender, as well as the chain of events leading to his or her death, for a total of around 8 million observations. The task of automatically coding the subject\\u2019s underlying cause of death was then formulated as a predictive modelling problem. A deep neural network\\u2212based model was then designed and fit to the dataset. Its error rate was then assessed on an exterior test dataset and compared to the current state-of-the-art (ie, the Iris software). Statistical significance of the proposed approach\\u2019s superiority was assessed via bootstrap. Results The proposed approach resulted in a test accuracy of 97.8% (95% CI 97.7-97.9), which constitutes a significant improvement over the current state-of-the-art and its accuracy of 74.5% (95% CI 74.0-75.0) assessed on the same test example. Such an improvement opens up a whole field of new applications, from nosologist-level batch-automated coding to international and temporal harmonization of cause of death statistics. A typical example of such an application is demonstrated by recoding French overdose-related deaths from 2000 to 2010. Conclusions This article shows that deep artificial neural networks are perfectly suited to the analysis of electronic health records and can learn a complex set of medical rules directly from voluminous datasets, without any explicit prior knowledge. Although not entirely free from mistakes, the derived algorithm constitutes a powerful decision-making tool that is able to handle structured medical data with an unprecedented performance. We strongly believe that the methods developed in this article are highly reusable in a variety of settings related to epidemiology, biostatistics, and the medical sciences in general.\"}, {\"paperId\": \"f8469ec34ab9ddcce2492a8ea6c86e17bafe217a\", \"abstract\": \"Smart farming is a new concept that makes agriculture more efficient and effective by using advanced information technologies. The latest advancements in connectivity, automation, and artificial intelligence enable farmers better to monitor all procedures and apply precise treatments determined by machines with superhuman accuracy. Farmers, data scientists and, engineers continue to work on techniques that allow optimizing the human labor required in farming. With valuable information resources improving day by day, smart farming turns into a learning system and becomes even smarter. Deep learning is a type of machine learning method, using artificial neural network principles. The main feature by which deep learning networks are distinguished from neural networks is their depth and that feature makes them capable of discovering latent structures within unlabeled, unstructured data. Deep learning networks that do not need human intervention while performing automatic feature extraction have a significant advantage over previous algorithms. The focus of this study is to explore the advantages of using deep learning in agricultural applications. This bibliography reviews the potential of using deep learning techniques in agricultural industries. The bibliography contains 120 papers from the database of the Science Citation Index on the subject that were published between 2016 and 2019. These studies have been retrieved from 39 scientific journals. The papers are classified into the following categories as disease detection, plant classification, land cover identification, precision livestock farming, pest recognition, object recognition, smart irrigation, phenotyping, and weed detection.\"}, {\"paperId\": \"d9ce0a58a1e4d2d080a61beba7ed2a3b2692dd20\", \"abstract\": \"To practise evidence-based medicine, clinicians need to apply the findings of scientific research to the circumstances of individual patients as part of their decision-making process. For centuries, medicine and clinical reasoning was based on subjective personal experience, until the wider adoption of evidence-based medicine, defined as \\u2018The conscientious explicit and judicious use of current best evidence in making decisions about the care of individual patients\\u2019. To date, simple mathematical and statistical methods have been used to describe patterns within relatively small-size datasets. We are now faced with a generation of novel datasets in large quantities, from an almost infinite number of digital fingerprints (phenomes) and sources such as electronic health records, high-resolution medical imaging, wearable devices and biosensors, more widely available genetic testing and even inputs from social media. It appears that the standard means of critical appraisal and data interpretation have reached a point of saturation. Evidence-based medicine, as we know it today, is necessary, but not sufficient to meet the demands of analysing large and complex datasets, as it can be time-consuming, resource-intensive, slow to completion and expensive. Data science is still a novel field, at the intersection of several disciplines including computer science, mathematics, statistics, and health and business analytics. It has evolved across many industries to be an advanced statistical method of gathering insights from large amounts of data in order to achieve business objectives or understand user behaviours. We are at a point in healthcare where data are more widely available and accessible in a computer readable fashion than before, coincided with the surge in computer processing power that occurred over the past decade. Recently, there has been significant interest in the field of artificial intelligence and its applications to healthcare and artificial intelligence was one of the major topics of the recently published Topol Review. Artificial intelligence broadly implies the operation of computer programmes with human cognitive ability in order to enable automated solving of complex problems, including perception, recognition, memory and learning. With a growing number of massive datasets and the increased computational power of the machines, a more unique type of artificial intelligence called machine learning has emerged. Machine learning transforms the inputs of an algorithm into outputs using statistical, data-driven rules that are automatically derived from a large set of examples, rather than being explicitly specified by humans. More specifically, a subfield of machine learning called deep learning where the algorithms are created from the data itself and determined by the number of layers (unsupervised learning) rather than initial human rules and postulates (supervised learning) is gaining increasing application in healthcare and already showing to compare with clinicians in the field of imaging, pathology, skin disease diagnosis, ophthalmology, cardiac arrhythmia detection and even sepsis management. Although the majority of the described examples have not been tested in pragmatic clinical trials, they hold significant potential, primarily because they could enable assessment, monitoring, diagnosis and management of a greater number of patients at a lower cost. In addition, artificial intelligence has the potential to reduce diagnostic errors as a quality assurance tool, as well as being used in workflow management. Consequently, it could reduce the administrative burden of clinicians and provide them with the \\u2018gift of time\\u2019 to prioritise treating patients. To derive meaningful inferences from newly generated data, the data need to be stored and analysed with advanced statistical methods; it appears that the next step of evidence-based medicine could be data science with processing software through increased Journal of the Royal Society of Medicine; 2019, Vol. 112(12) 493\\u2013494\"}, {\"paperId\": \"54137df24b9f2aedefe83eee9701f700600a98b7\", \"abstract\": \"Achievement of human-level machine intelligence has profound implications for modern society\\u2014 a society which is becoming increasingly infocentric in its quest for efficiency, convenience and enhancement of quality of life. Humans have many remarkable capabilities. Among them a capability that stands out in importance is the human ability to perform a wide variety of physical and mental tasks without any measurements and any computations, based on perceptions of distance, speed, direction, intent, likelihood and other attributes of physical and mental objects. A familiar example is driving a car in city traffic. Mechanization of this ability is a challenging objective of machine intelligence. Science deals not with reality but with models of reality. In large measure, models of reality in scientific theories are based on classical, Aristotelian, bivalent logic. The brilliant successes of science are visible to all. But when we take a closer look, what we see is that alongside the brilliant successes there are areas where achievement of human-level machine intelligence is still a distant objective. We cannot write programs that can summarize a book. We cannot automate driving a car in heavy city traffic. And we are far from being able to construct systems which can understand natural language. Why is the achievement of human-level machine intelligence a distant objective? What is widely unrecognized is that one of the principal reasons is the fundamental conflict between the precision of bivalent logic and imprecision of the real world. In the world of bivalent logic, every proposition is either true or false, with no shades of truth allowed. In the real world, as perceived by humans, most propositions are true to a degree. Humans have a remarkable capability to reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information and partiality of truth. It is this capability that is beyond the reach of bivalent logic\\u2014a logic which is intolerant of imprecision and partial truth. A much better fit to the real world is fuzzy logic. In fuzzy logic, everything is or is allowed to be graduated, that is, be a matter of degree or, equivalently, fuzzy. Furthermore, in fuzzy logic everything is or is allowed to be granulated, with a granule being a clump of elements drawn together by indistinguishability, similarity, proximity or functionality. Graduation and granulation play key roles in the ways in which humans deal with complexity and imprecision. In this connection, it should be noted that, in large measure, fuzzy logic is inspired by the ways in which humans deal with complexity, imprecision and partiality of truth. It is in this sense that fuzzy logic is human-centric. In coming years, fuzzy logic and fuzzylogic-based methods are likely to play increasingly important roles in achievement of human-level machine intelligence. In addition, soft computing is certain to grow in visibility and importance. Basically, soft computing is a coalition of methodologies which in one way or another are directed at the development of better models of reality, human reasoning, risk assessment and decision making. This is the primary motivation for soft computing\\u2014a coalition of fuzzy logic, neurocomputing, evolutionary computing, probabilistic computing and machine learning. The guiding principle of soft computing is that, in general, better results can be achieved through the use of constituent methodologies of soft computing in combination rather than in a stand-alone mode. Brief biography of the speaker: LOTFI A. ZADEH is a Professor in the Graduate School, Computer Science Division, Department of EECS, University of California, Berkeley. In addition, he is serving as the Director of BISC (Berkeley Initiative in Soft Computing).Lotfi Zadeh is an alumnus of the University of Tehran, MIT and Columbia University. He held visiting appointments at the Institute for Advanced Study, Princeton, NJ; MIT, Cambridge, MA; IBM Research Laboratory, San Jose, CA; AI Center, SRI International, Menlo Park, CA; and the Center for the Study of Language and Information, Stanford University. His earlier work was concerned in the main with systems analysis, decision analysis and information systems. His current research is focused on fuzzy logic, computing with words and soft computing, which is a coalition of fuzzy logic, neurocomputing, evolutionary computing, probabilistic computing and parts of machine learning. Lotfi Zadeh is a Fellow of the IEEE, AAAS, ACM, AAAI, and IFSA. He is a member of the National Academy of Engineering and a Foreign Member of the Russian Academy of Natural Sciences, the Finnish Academy of Sciences, the Polish Academy of Sciences, Korean Academy of Science & Technology and the Bulgarian Academy of Sciences. He is a recipient of the IEEE Education Medal, the IEEE Richard W. Hamming Medal, the IEEE Medal of Honor, the ASME Rufus Oldenburger Medal, the B. Bolzano Medal of the Czech Academy of Sciences, the Kampe de Feriet Medal, the AACC Richard E. Bellman Control Heritage Award, the Grigore Moisil Prize, the Honda Prize, the Okawa Prize, the AIM Information Science Award, the IEEE-SMC J. P. Wohl Career Achievement Award, the SOFT Scientific Contribution Memorial Award of the Japan Society for Fuzzy Theory, the IEEE Millennium Medal, the ACM 2001 Allen Newell Award, the Norbert Wiener Award of the IEEE Systems, Man and Cybernetics Society, Civitate Honoris Causa by Budapest Tech (BT) Polytechnical Institution, Budapest, Hungary, the V. Kaufmann Prize, International Association for Fuzzy-Set Management and Economy (SIGEF), the Nicolaus Copernicus Medal of the Polish Academy of Sciences, the J. Keith Brimacombe IPMM Award, the Silicon Valley Engineering Hall of Fame, the Heinz Nixdorf MuseumsForum Wall of Fame, other awards and twenty-six honorary doctorates. He has published extensively on a wide variety of subjects relating to the conception, design and analysis of information/intelligent systems, and is serving on the editorial boards of over sixty journals.\"}, {\"paperId\": \"065bc4a562e21c6645d56076738e494b5873e06b\", \"abstract\": \"Machine learning methods originated from artificial intelligence and are now used in various fields in environmental sciences today. This is the first single-authored textbook providing a unified treatment of machine learning methods and their applications in the environmental sciences. Due to their powerful nonlinear modeling capability, machine learning methods today are used in satellite data processing, general circulation models(GCM), weather and climate prediction, air quality forecasting, analysis and modeling of environmental data, oceanographic and hydrological forecasting, ecological modeling, and monitoring of snow, ice and forests. The book includes end-of-chapter review questions and an appendix listing web sites for downloading computer code and data sources. A resources website containing datasets for exercises, and password-protected solutions are available. The book is suitable for first-year graduate students and advanced undergraduates. It is also valuable for researchers and practitioners in environmental sciences interested in applying these new methods to their own work. Preface Excerpt Machine learning is a major subfield in computational intelligence (also called artificial intelligence). Its main objective is to use computational methods to extract information from data. Neural network methods, generally regarded as forming the first wave of breakthrough in machine learning, became popular in the late 1980s, while kernel methods arrived in a second wave in the second half of the 1990s. This is the first single-authored textbook to give a unified treatment of machine learning methods and their applications in the environmental sciences. Machine learning methods began to infiltrate the environmental sciences in the 1990s. Today, thanks to their powerful nonlinear modeling capability, they are no longer an exotic fringe species, as they are heavily used in satellite data processing, in general circulation models (GCM), in weather and climate prediction, air quality forecasting, analysis and modeling of environmental data, oceanographic and hydrological forecasting, ecological modeling, and in the monitoring of snow, ice and forests, etc. This book presents machine learning methods and their applications in the environmental sciences (including satellite remote sensing, atmospheric science, climate science, oceanography, hydrology and ecology), written at a level suitable for beginning graduate students and advanced undergraduates. It is also valuable for researchers and practitioners in environmental sciences interested in applying these new methods to their own work. Chapters 1-3, intended mainly as background material for students, cover the standard statistical methods used in environmental sciences. The machine learning methods of chapters 4-12 provide powerful nonlinear generalizations for many of these standard linear statistical methods. End-of-chapter review questions are included, allowing readers to develop their problem-solving skills and monitor their understanding of the material presented. An appendix lists websites available for downloading computer code and data sources. A resources website is available containing datasets for exercises, and additional material to keep the book completely up-to-date. About the Author WILLIAM W. HSIEH is a Professor in the Department of Earth and Ocean Sciences and in the Department of Physics and Astronomy, as well as Chair of the Atmospheric Science Programme, at the University of British Columbia. He is internationally known for his pioneering work in developing and applying machine learning methods in environmental sciences. He has published over 80 peer-reviewed journal publications covering areas of climate variability, machine learning, oceanography, atmospheric science and hydrology.\"}, {\"paperId\": \"d950f3280bb30131a874936add0cd375bcc363ef\", \"abstract\": null}, {\"paperId\": \"a34b4c5a963103d09fe887cd5b5a6f9572dbc701\", \"abstract\": \"This report provides an overview of recent work that harnesses the Big Data Revolution and Large Scale Computing to address grand computational challenges in Multi-Messenger Astrophysics, with a particular emphasis on real-time discovery campaigns. Acknowledging the transdisciplinary nature of Multi-Messenger Astrophysics, this document has been prepared by members of the physics, astronomy, computer science, data science, software and cyberinfrastructure communities who attended the NSF-, DOE- and NVIDIA-funded \\\"Deep Learning for Multi-Messenger Astrophysics: Real-time Discovery at Scale\\\" workshop, hosted at the National Center for Supercomputing Applications, October 17-19, 2018. Highlights of this report include unanimous agreement that it is critical to accelerate the development and deployment of novel, signal-processing algorithms that use the synergy between artificial intelligence (AI) and high performance computing to maximize the potential for scientific discovery with Multi-Messenger Astrophysics. We discuss key aspects to realize this endeavor, namely (i) the design and exploitation of scalable and computationally efficient AI algorithms for Multi-Messenger Astrophysics; (ii) cyberinfrastructure requirements to numerically simulate astrophysical sources, and to process and interpret Multi-Messenger Astrophysics data; (iii) management of gravitational wave detections and triggers to enable electromagnetic and astro-particle follow-ups; (iv) a vision to harness future developments of machine and deep learning and cyberinfrastructure resources to cope with the scale of discovery in the Big Data Era; (v) and the need to build a community that brings domain experts together with data scientists on equal footing to maximize and accelerate discovery in the nascent field of Multi-Messenger Astrophysics.\"}, {\"paperId\": \"d53cd4dc81a93f496c92864509de9c9c8ca4513e\", \"abstract\": \"Machine learning methods, having originated from computational intelligence (i.e. artificial intelligence), are now ubiquitous in the environmental sciences. This is the first single-authored textbook to give a unified treatment of machine learning methods and their applications in the environmental sciences. Machine learning methods began to infiltrate the environmental sciences in the 1990s. Today, thanks to their powerful nonlinear modelling capability, they are no longer an exotic fringe species, as they are heavily used in satellite data processing, in general circulation models (GCM), in weather and climate prediction, air quality forecasting, analysis and modelling of environmental data, oceanographic and hydrological forecasting, ecological modelling, and in the monitoring of snow, ice and forests, etc. End-of-chapter review questions are included, allowing readers to develop their problem-solving skills and monitor their understanding of the material presented. An appendix lists websites available for downloading computer code and data sources. A resources website is available containing datasets for exercises, and additional material to keep the book completely up-to-date. This book presents machine learning methods and their applications in the environmental sciences (including satellite remote sensing, atmospheric science, climate science, oceanography, hydrology and ecology), written at a level suitable for beginning graduate students and advanced undergraduates. It is also valuable for researchers and practitioners in environmental sciences interested in applying these new methods to their own work.\"}, {\"paperId\": \"272694c36eeb4f62d9b71e9654abf4448ec856fb\", \"abstract\": \"Objective: Electroencephalography (EEG) is very crucial for understanding the dynamic healthy and pathological complex processes in the brain. However, the manual analysis of the EEG signal is very complex, time-consuming, and depends on the expertise and experience of the users. Hence, nowadays research is conducted on automated EEG signal analysis using artificial intelligence and computer-aided technologies. This would allow fast and highly accurate results. The goal of this paper is to provide an extensive review of the EEG signal analysis using deep learning (DL).Methods: This systematic literature review of EEG processing using Deep Learning (DL) was achieved on Web of Science, PubMed, and Science Direct databases, resulting in 403 identified papers. All collected studies were analyzed based on main disorders studied, type of tasks performed, data source, stages of analysis, and DL architecture.Results: DL in EEG processing is promising in various research applications. It covered the common neurological disorders diagnosis such as epilepsy, movement disorder, depression, schizophrenia, autism, alcohol use, attention, memory, sleep, pain, etc. The main tasks covered by the included studies are detection and classification. The average range of data sources utilized by the included studies is 127 subjects with an EEG recording a total duration of 458 hours. In fact, we identified the use of a plethora of DL architecture for EEG analysis. 57% of papers used Convolutional Neural Networks (CNNs), whereas Recurrent Neural Networks (RNNs) were the architecture choice of about 12% of papers. Combinations of CNNs and Long Short-Term Memory (LSTM) were used in 13% of studies. Generative Adversarial Networks (GAN) and Autoencoder (AEs) were used in 5% and 4% of papers respectively. Restricted Boltzmann Machine (RBMs), Deep Belief Networks (DBNs), and other hybrid architectures appeared in 1% of studies.Conclusion: This systematic review showed that DL is a powerful tool to process, analyze, and interpret EEG data without requiring extraction steps. These intelligent models can allow self-learning from the training data. On the other hand, DL models need a lot of data to learn, while suffering from a lack of confidence due to their black-box nature. Hence, studies on transfer learning and Explainable Artificial Intelligence (XAI) could help in solving such issues. Big Data, Cloud Computing, the Internet of Things (IoT), and closed-loop technology can also help DL-based systems in achieving fast, and accurate processing of EEG recordings.\"}, {\"paperId\": \"15bbfdea5d3b65c9cdc11b399562a0242f159fcb\", \"abstract\": \"Currently, great efforts are being put toward the early identification of preclinical, biological, clinical, and laboratory markers that are able to predict the conversion of \\u201ccognitively intact\\u201d patients to overt dementia. An example of a successful biomarker for neurodegenerative disorders, and specifically of conversion from early stage of dementia of Alzheimer\\u2019s disease or Lewy body dementia (encompassing dementia with Lewy bodies and Parkinson\\u2019s disease (PD) dementia) and of progression of the diseases from mild to more pronounced stages of dementia, is represented by resting state, eyes-closed electroencephalographic (EEG) rhythms. Patients with dementia exhibit a general reduction of power in the alpha and beta bands and high power of widespread delta and theta rhythms when compared with nondemented patients. Specifically, dementia is associated with dominant frequency variability with the appearance of a prealpha (a fast theta rhythm of 5.5-7.5 Hz) band, now listed as a supportive biomarker in the latest consensus criteria for the diagnosis of dementia with Lewy bodies, and a progressively increased relative power of theta band is described in PD dementia patients as cognition declines. The renaissance of applying clinical EEG to dementia in neurodegenerative diseases has been associated with the development of new analytical methods and breakthrough discoveries pertaining to the neuronal mechanisms underlying EEG features. Quantitative EEG (QEEG) is a derivative of regular EEG in which an offline analysis of frequency and amplitude allows the identification of specific, discrete patterns of brain wave activity. It is important to note that the source analogical data must first be visually inspected and evaluated by an expert neurophysiologist before mathematical translation of the data occurs. A thorough understanding and firm knowledge of clinical EEG features, and of mathematics and computing science, is required to prevent erroneous interpretations of digitally displayed mathematical constructs (eg, amplitude, frequency, coherence maps). The application of artificial intelligence, and more specifically machine-learning techniques, to EEG signal analysis is one possible solution that will allow wider application of EEG analysis in clinical practice as it does not require a deep knowledge of the internal workings of the machine when processing input data to give output results, assuming that machine learning could be envisioned, in clinical practice, as a \\u201cblack box\\u201d (although it is not, strictly). The black box metaphor, which dates back to the early days of cybernetics and behaviorism, typically refers to a system in which we can observe only the inputs and outputs, but not the internal workings. To entirely understand the internal workings would require a meta system, that is, a system with a higher degree than the system itself that allows one to examine the internal working of the system from the outside. This essentially represents a variation of the G\\u00f6del incompleteness theorems, whose explanation is well beyond the scope of this editorial. Machine-learning algorithms use mathematical, computational methods to derive information directly from data without applying predetermined mathematical models or equations. These algorithms adaptively improve their own performances as the analyzed examples increase in number. Machine learning might thus appear as the keystone of replacing human intelligence when interpreting complex signals (including EEG signals) and applying the results to routine clinical practice. It is important to distinguish between learning tasks that human examiners can already do well and learning those tasks where physicians have only limited success. Interesting examples of machine learning in medical research are algorithms that allow computers to make increasingly accurate predictions to prevent infectious epidemics, improve global health, or accurately and timely diagnose tumors or rare diseases. These are examples of a supervised learning method where the computer is provided both the input dataset and *Corresponding to: Dr. Laura Bonanni, Department of Neuroscience and Imaging, University G. d\\u2019Annunzio of Chieti-Pescara, Via dei Vestini 66100 Chieti, Italy; E-mail: l.bonanni@unich.it\"}, {\"paperId\": \"e6864cd49f78d4ff630c94e41233154f59a26c24\", \"abstract\": \"1. MD, Radiologist, Hospital Israelita Albert Einstein, S\\u00e3o Paulo, SP, Brazil. E-mail: omir@outlook.com. 2. MD, MPH, Neuroradiologist, Head of the Division of Medical Imaging Informatics, The Ohio State University Wexner Medical Center, Columbus, OH, USA. E-mail: luciano.prevedello@osumc.edu. Artificial intelligence (AI) will bring changes to the professional life of radiologists, just as it has modified many other aspects of our lives. Since the invention of electricity, the internet, and most recently AI, general purpose technologies have made it possible for societies to progress and improve their quality of life. Radiologists are not the only, or even the first, professionals to have their specialty modified by AI. Other areas of medicine have also been thus affected. Recently, one group of researchers developed software for cell phones and conducted a study in which the software was shown to distinguish malignant melanomas from benign lesions more accurately than did the dermatologists participating in the study(1). Although the role of the dermatologist is much broader than the classification of lesions, that finding could be an initial indication that AI will also modify the labor market in that field. Currently, the vast majority of laboratory tests are analyzed in automated systems, which does not seem to bother anyone. The need for trained personnel to quantify red blood cells has long since ceased to exist. Likewise, advances in the field of computer vision have begun to produce improved sensitivity in the detection of tumor in histopathological specimens when compared to human analysis(2). The main factors that allowed such advances in artificial intelligence were the abundance of data, the development of artificial neural networks, and the increased affordability of the hardware: \\u2022 Currently, data are collected by various instruments. If AI is the new electricity, data is the new coal. \\u2022 The development of artificial neural networks allowed some of the problems with other machine learning techniques to be solved. In traditional methods, which are the basis of some auxiliary diagnostic software\\u2014computer-aided detection\\u2014increasing the amount of data improves accuracy to a certain extent. In the deep learning technique, which uses hidden layers of artificial neural networks, the accuracy tends to continue to increase as new data are added. For example, in 2015, in an annual competition of object recognition in everyday images, the performance of a machine surpassed that of a human for the first time(3). Deep learning is a branch of machine learning, which is in turn one of the branches of AI. \\u2022 The increased affordability of hardware, notably processing and storage devices, also fostered advancement in the area. Currently, one can purchase a laptop with the same processing power as the most powerful supercomputer manufactured in the year 2000, the difference being that the laptop will cost approximately 40,000 times less than the 110 million dollars spent on the supercomputer and will weigh much less than its 106 tons(4). Advances in the field of computer vision have aroused the interest of technology giants and various government sectors in highly lucrative areas of great global interest. Autonomous cars and drones with automatic target recognition require instantaneous processing of images, do not tolerate errors, and are not supposed to rely on a human who is readily able to modify their operation. Although this dispersion could apparently cause a delay in the influence of AI in the radiology market, advances in the automotive and space sectors are commonly catalysts for advances in other sectors, potentially accelerating the pace of new discoveries in the field of imaging. The implementation of AI techniques in medical imaging has particular challenges. Diagnoses are not always confirmed; classifications and concepts are not always unanimous, nor are they eternal(5). The structures of the human body present great variation in terms of their normal dimensions and textures, such variation potentially masking pathological conditions. Knowledge regarding the automated analysis of medical images has spread rapidly. One example is provided by the most recent Data Science Bowl(6), which offered a total of a million dollars in prizes for teams that developed and made available the best automated nodule classification algorithms for chest computed tomography (CT). Several participating teams had never worked in the field of medical imaging, and many of them likely chose to work in the field only because of the competition. The impact of AI on the routine of the radiologist will probably occur gradually. Software will provide data that we cannot extract from the images, prioritize examinations according to disease severity(7), and, among other resources, will gradually become part of the routine. In certain areas of radiology, AI has already proven capable of generating parts of the radiological report, including a preliminary description of the imaging findings and the measurement of some lesions. However, the main contribution of the radiologist is not simply to provide this information but to integrate it with the clinical data, contributing in a more holistic way to the diagnosis and individualized treatment of the patient. This type of information integration will take more time to be replicated by machines. Nevertheless, radiologists who know how to use the\"}, {\"paperId\": \"d6ff1bb3cbdf74ccbd93bd13dfb105a5f81a4757\", \"abstract\": null}, {\"paperId\": \"2fe5ef87aa59e66a80032b840ace1f8a629e2825\", \"abstract\": \"We propose a quantum machine learning algorithm for efficiently solving a class of problems encoded in quantum controlled unitary operations. The central physical mechanism of the protocol is the iteration of a quantum time-delayed equation that introduces feedback in the dynamics and eliminates the necessity of intermediate measurements. The performance of the quantum algorithm is analyzed by comparing the results obtained in numerical simulations with the outcome of classical machine learning methods for the same problem. The use of time-delayed equations enhances the toolbox of the field of quantum machine learning, which may enable unprecedented applications in quantum technologies. Introduction One of the main consequences of the revolution in computation sciences, started by Alan Turing, Konrad Zuse and John Von Neumann, among others,1, 2 is that computers are capable of substituting us and improving our performance in an increasing number of tasks. This is due to the advances in the development of complex algorithms and the technological refinement allowing for faster processing and larger storage. One of the goals in this area, in the frame of bio-inspired technologies, is the design of algorithms that provide computers human-like capacities such as image and speech recognition, as well as preliminary steps in some aspects related to creativity. These achievements would enable us to interact with computers in a more efficient manner. This research, together with other similar projects, is carried out in the field of artificial intelligence.3 In particular, researchers in the area of machine learning (ML) inside artificial intelligence are devoted to the design of algorithms responsible of training the machine with data, such that it is able to find a given optimal relation according to specified criteria.4 More precisely, ML is divided in three main lines depending on the nature of the protocol. In supervised learning, the goal is to teach the machine a known function without explicitly introducing it in its code. In unsupervised learning, the goal is that the machine develops the ability to classify data by grouping it in different subsets depending on its characteristics. In reinforcement learning, the goal is that the machine selects a sequence of actions depending on its interaction with an environment for an optimal transition from the initial to the final state. The previous ML techniques have also been studied in the quantum regime in a field called quantum machine learning5\\u201312 with two main motivations. The first one is to exploit the promised speedup of quantum protocols for improving the already existing classical ones. The second one is to develop unique quantum machine learning protocols for combining them with other quantum computational tasks. Apart from quantum machine learning, fields like quantum neural networks, or the more general quantum artificial intelligence, have also addressed similar problems.13\\u201317 Here, we introduce a quantum machine learning algorithm for finding the optimal control state of a multitask controlled unitary operation. It is based on a sequentially-applied time-delayed equation that allows one to implement feedback-driven dynamics without the need of intermediate measurements. The purely quantum encoding permits to speedup the training process by evaluating all possible choices in parallel. Finally, we analyze the performance of the algorithm comparing the ideal solution with the one obtained by the algorithm. Results Quantum Machine Learning Algorithm The first step in the description of the algorithm is the definition of the concept of multitask controlled unitary operations U . In essence, these do not differ from ordinary controlled operations, but the multitask label is selected to emphasize that more than ar X iv :1 61 2. 05 53 5v 1 [ qu an tph ] 1 6 D ec 2 01 6 two operations on the target subspace are in principle possible. Mathematically, we define them as U = d \\u2211 i=1 |ci\\u3009\\u3008ci|\\u2297 si, (1) where |ci\\u3009 denotes the control state, and si is the reduced or effective unitary operation that U performs on the target subspace when the control is on |ci\\u3009. Our algorithm is appropriate when U is experimentally implementable but its internal structure, |ci\\u3009 and si, are unknown. The goal is to find the optimal |ci\\u3009 for fixed input and output states, |in\\u3009 and |out\\u3009, in the target subspace. The protocol consists in sequentially reapplying the same dynamics in such a way that the initial state in the signal subspace is always |in\\u3009, while the initial state in the control subspace is the output of the previous cycle. The equation modeling the dynamics is d dt |\\u03c8(t)\\u3009=\\u2212i [ \\u03b8(t\\u2212 ti)\\u03b8(t f \\u2212 t)\\u03ba1H1 |\\u03c8(t)\\u3009+\\u03ba2H2 (|\\u03c8(t)\\u3009\\u2212 |\\u03c8(t\\u2212\\u03b4 )\\u3009) ] . (2) In this equation \\u03b8 is the Heaviside function, H1 is the Hamiltonian giving rise to U with U = e\\u2212i\\u03ba1H1(t f\\u2212ti), and H2 is the Hamiltonian connecting the input and output states, with \\u03ba1 and \\u03ba2 the coupling constants of each Hamiltonian. We point out that this evolution cannot be realized with ordinary unitary or dissipative techniques. Nevertheless, recent studies in time delayed equations provide all the ingredients for the implementation of this kind of processes.18\\u201320 Up to future experimental analyses involving the scalability of the presented examples, the inclusion of time delayed terms in the evolution equation is a realistic approach in the technological framework provided by current quantum platforms. Another important feature of Eq. (2) is that it only acquires physical meaning once the output is normalized. Regarding the behavior of the equation, each term has a specific role in the learning algorithm. The mechanism is inspired in the most intuitive classical technique for solving this problem, which is the comparison between the input and output states together with the correspondent modification of the control state. Here, the first Hamiltonian produces the input-output transition while the second Hamiltonian produces the reward by populating the control states responsible of the desired modification of the target subspace. The structure of H2 guarantees that only the population in the control |ci\\u3009 associated with the optimal si is increased, H2 = 1\\u2297 (\\u2212i |in\\u3009\\u3008out|+ i |out\\u3009\\u3008in|) . (3) Notice that while this Hamiltonian does not contain explicit information about |ci\\u3009, the solution of the problem, its multiplication with the feedback term, |\\u03c8(t)\\u3009\\u2212 |\\u03c8(t\\u2212\\u03b4 )\\u3009, is responsible for introducing the reward as an intrinsic part of the dynamics. This is a convenient approach because it eliminates the measurements required during the training phase. We would also like to point out the similarity existing between the effect of this term in our quantum evolution and gradient ascent techniques in algorithms for artificial intelligence.3 A possible strategy to perform the learning protocol would be to feed the system with random control states, measure each result, and combine them to obtain the final solution. However, we have discovered that it suffices to initialize the control subspace in a superposition of the elements of the basis. We would like to remark that this purely quantum feature reduces significantly the required resources, because a single initial state replaces a set of random states large enough to cover all possible solutions. The specific example we address is given by the excitation transport produced by the controlled SWAP gate. In this scenario, the complete system is an n-node network, where each node is composed by a control and a target qubit. The control states are in a superposition of open and close, |o\\u3009 and |c\\u3009, while the target qubits are written in the standard {|0\\u3009 , |1\\u3009} basis denoting the absence or presence of excitations. We define U , the multitask controlled unitary operation, to implement the SWAP gate between connected nodes only if all the controls of the corresponding nodes are in the open state, |o\\u3009. See Fig. 1 for a graphical representation of the most simple cases, the two and three node line networks. The explicit formula for U2 is given by U2 = (|cc\\u3009\\u3008cc|+ |co\\u3009\\u3008co|+ |oc\\u3009\\u3008oc|)\\u22971+ |oo\\u3009\\u3008oo|\\u2297 s12, (4) where si j represents the SWAP gate between qubits i and j. Although we have employed unitary operations for illustration purposes, the equation requires the translation to Hamiltonians. In order to do so, we first select \\u03ba1(t f \\u2212 ti) to be \\u03c0/2 and calculate the matrix logarithm, H1 = (|oo\\u3009\\u3008oo|)\\u2297h12. Denoting with \\u03c3k the Pauli matrices, hi j for i < j reads hi j = 1 2 ( 3 \\u2211 k=1 1\\u2297\\u03c3k\\u22971 \\u2297\\u03c3k\\u22971 j\\u22121\\u2297n )\"}, {\"paperId\": \"3042169871be3121eca1d0a3d98f5da7841dbab1\", \"abstract\": \"Background Uncertainty surrounds the ethical and legal implications of algorithmic and data-driven technologies in the mental health context, including technologies characterized as artificial intelligence, machine learning, deep learning, and other forms of automation. Objective This study aims to survey empirical scholarly literature on the application of algorithmic and data-driven technologies in mental health initiatives to identify the legal and ethical issues that have been raised. Methods We searched for peer-reviewed empirical studies on the application of algorithmic technologies in mental health care in the Scopus, Embase, and Association for Computing Machinery databases. A total of 1078 relevant peer-reviewed applied studies were identified, which were narrowed to 132 empirical research papers for review based on selection criteria. Conventional content analysis was undertaken to address our aims, and this was supplemented by a keyword-in-context analysis. Results We grouped the findings into the following five categories of technology: social media (53/132, 40.1%), smartphones (37/132, 28%), sensing technology (20/132, 15.1%), chatbots (5/132, 3.8%), and miscellaneous (17/132, 12.9%). Most initiatives were directed toward detection and diagnosis. Most papers discussed privacy, mainly in terms of respecting the privacy of research participants. There was relatively little discussion of privacy in this context. A small number of studies discussed ethics directly (10/132, 7.6%) and indirectly (10/132, 7.6%). Legal issues were not substantively discussed in any studies, although some legal issues were discussed in passing (7/132, 5.3%), such as the rights of user subjects and privacy law compliance. Conclusions Ethical and legal issues tend to not be explicitly addressed in empirical studies on algorithmic and data-driven technologies in mental health initiatives. Scholars may have considered ethical or legal matters at the ethics committee or institutional review board stage. If so, this consideration seldom appears in published materials in applied research in any detail. The form itself of peer-reviewed papers that detail applied research in this field may well preclude a substantial focus on ethics and law. Regardless, we identified several concerns, including the near-complete lack of involvement of mental health service users, the scant consideration of algorithmic accountability, and the potential for overmedicalization and techno-solutionism. Most papers were published in the computer science field at the pilot or exploratory stages. Thus, these technologies could be appropriated into practice in rarely acknowledged ways, with serious legal and ethical implications.\"}, {\"paperId\": \"10ffcae001fa724f9fd49fe0726c32166df9209b\", \"abstract\": \"Data mining is the discovery of meaningful information or unrevealed patterns in data. Traditional data\\u2010mining approaches, using statistical calculations, machine learning, artificial intelligence, and database technology, cannot interpret data on a conceptual or semantic level and fail to reveal the meanings within the data. This results in a user not being analyzed and determines its signification and implications. Several semantic data\\u2010mining approaches have been proposed in the past decade that overcome these limitations by using a domain ontology as background knowledge to enable and enhance data\\u2010mining performance. The main contributions of this literature survey include organizing the surveyed articles in a new way that provides ease of understanding for interested researchers, and the provision of a critical analysis and summary of the surveyed articles, identifying the contribution of these papers to the field, and the limitations of the analysis methods and approaches discussed in this corpus, with the intention of informing researchers in this growing field in their innovative approaches to new research. Finally, we identify the future trends and challenges in this study track that will be of concern to future researchers, such as dynamic knowledge\\u2010based methods or big\\u2010data tool collaboration. This survey article provides a comprehensive overview of the literature on domain ontologies as used in the various semantic data\\u2010mining tasks, such as preprocessing, modeling, and postprocessing. We investigated the role of semantic data mining in the field of data science and the processes and methods of applying semantic data mining to a data resource description framework.\"}, {\"paperId\": \"8f3a6404b9fe0519e3add0636a3e015c824e9f5e\", \"abstract\": \"The data are ever increasing with the increase in population, communication of different devices in networks, Internet of Things, sensors, actuators, and so on. This increase goes into different shapes such as volume, velocity, variety, veracity, and value extracting meaningful information and insights, all are challenging tasks and burning issues. Decision-making based on multicriteria is one of the most critical issues solving ways to select the most suitable decision among a number of alternatives. Deep learning algorithms and multicriteria-based decision-making have effective applications in big data. Derivations are made based on the use of deep algorithms and multicriteria. Due to its effectiveness and potentiality, it is exploited in several domains such as computer science and information technology, agriculture, and business sector. The aim of the proposed study is to present a systematic literature study in order to show the applications of deep learning algorithms and multicriteria decision approaches for the problems of big data. The research finds novel means to make the decision support system for the problems of big data using multiple criteria in integration with machine learning and artificial intelligence approaches.\"}, {\"paperId\": \"5cab50974b97872d2f8e9f4f486f0eb12ebfa67c\", \"abstract\": \"Machine learning (ML) has emerged as an indispensable methodology to describe, discover, and predict complex physical phenomena that efficiently help us learn underlying functional rules, especially in cases when conventional modeling approaches cannot be applied. While conventional feedforward neural networks are typically limited to performing tasks related to static patterns in data, recursive models can both work iteratively based on a changing input and discover complex dynamical relationships in the data. Deep language models can model flexible modalities of data and are capable of learning rich dynamical behaviors as they operate on discrete or continuous symbols that define the states of a physical system, yielding great potential toward end-to-end predictions. Similar to how words form a sentence, materials can be considered as a self-assembly of physically interacted building blocks, where the emerging functions of materials are analogous to the meaning of sentences. While discovering the fundamental relationships between building blocks and function emergence can be challenging, language models, such as recurrent neural networks and long-short term memory networks, and, in particular, attention models, such as the transformer architecture, can solve many such complex problems. Application areas of such models include protein folding, molecular property prediction, prediction of material failure of complex nonlinear architected materials, and also generative strategies for materials discovery. We outline challenges and opportunities, especially focusing on extending the deep-rooted kinship of humans with symbolism toward generalizable artificial intelligence (AI) systems using neuro-symbolic AI, and outline how tools such as ChatGPT and DALL\\u00b7E can drive materials discovery.\"}, {\"paperId\": \"01e3ffef9abed67230413d40b8eb25ec220b205a\", \"abstract\": null}, {\"paperId\": \"26c6a32c576439f389fe940f082808bda829cb2b\", \"abstract\": \"Data-driven strategies are gaining increased attention in protein engineering due to recent advances in access to large experimental databanks of proteins, next-generation sequencing (NGS), high-throughput screening (HTS) methods, and the development of artificial intelligence algorithms. However, the reliable prediction of beneficial amino acid substitutions, their combination, and the effect on functional properties remain the most significant challenges in protein engineering, which is applied to develop proteins and enzymes for biocatalysis, biomedicine, and life sciences. Here, we present a general-purpose framework (PyPEF: pythonic protein engineering framework) for performing data-driven protein engineering using machine learning methods combined with techniques from signal processing and statistical physics. PyPEF guides the identification and selection of beneficial proteins of a defined sequence space by systematically or randomly exploring the fitness of variants and by sampling random evolution pathways. The performance of PyPEF was evaluated concerning its predictive accuracy and throughput on four public protein and enzyme data sets using common regression models. It was proved that the program could efficiently predict the fitness of protein sequences for different target properties (predictive models with coefficient of determination values ranging from 0.58 to 0.92). By combining machine learning and protein evolution, PyPEF enabled the screening of proteins with various functions, reaching a screening capacity of more than 500,000 protein sequence variants in the timeframe of only a few minutes on a personal computer. PyPEF displayed significant accuracies on four public data sets (different proteins and properties) and underlined the potential of integrating data-driven technologies for covering different philosophies by either predicting the fitness of the variants to the highest accuracy accounting for epistatic effects or capturing the general trend of introduced mutations on the fitness in directed protein evolution campaigns. In essence, PyPEF can provide a powerful solution to current sequence exploration and combinatorial problems faced in protein engineering through exhaustive in silico screening of the sequence space.\"}, {\"paperId\": \"debad915d24069c261c3f5ae967dcb3fdc0a0142\", \"abstract\": null}, {\"paperId\": \"816ea0b33781f666b682e2949d47d0422d532009\", \"abstract\": \"Despite its importance to OM, demand forecasting has been perceived as a \\u201cproblem-solving\\u201d exercise; most empirical work in the field has focused on explanatory models but neglected prediction problems that are part of empirical science. The present study, involving one of the leading electronics distributors in the world, aims to improve prediction accuracy under high demand volatility for procurement managers to make better inventory decisions. In response to requests for an integrated forecasting methodology, we undertook an iterative process based on three guiding principles \\u2014 data pooling, theory-informed feature engineering, and ensemble-based machine learning. The resulting framework managed to improve forecast accuracy significantly and is applicable to a broad range of situations. We present reflections and insights derived abductively through engagement with managers in this problem situation. This \\u201cproblem-driven\\u201d process corresponds to intervention as a research strategy that can foster theoretical and methodological innovations in OM. Our contribution goes beyond the development of the prediction framework as it elucidates ways OM researchers could leverage theoretical foundations to inform feature derivation and model construction. We posit that this work points to a way forward to the combination of OM principles with the emerging innovations in data science and artificial intelligence.\"}, {\"paperId\": \"0e0d72367be96583522cb10194b806d167293b80\", \"abstract\": \"An introductory Artificial Intelligence (AI) course provides students with basic knowledge of the theory and practice of AI as a discipline concerned with the methodology and technology for solving problems that are difficult to solve by other means. It is generally recognized that an introductory Artificial Intelligence course is challenging to teach. This is, in part, due to the diverse and seemingly disconnected core AI topics that are typically covered. Recently, work has been done to address the diversity of topics covered in the course and to create a theme-based approach. Russell and Norvig present an agent-centered approach [9]. Others have been working to integrate Robotics into the AI course [1, 2, 3].We present work on a project funded by the National Science Foundation with a goal of unifying the artificial intelligence course around the theme of machine learning. This involves the development and testing of an adaptable framework for the presentation of core AI topics that emphasizes the relationship between AI and computer science. Machine learning is inherently connected with the AI core topics and provides methodology and technology to enhance real-world applications within many of these topics. Machine learning also provides a bridge between AI technology and modern software engineering. In his article, Mitchell discusses the increasingly important role that machine learning plays in the software world and identifies three important areas: data mining, difficult-to-program applications, and customized software applications [6].We have developed a suite of adaptable, hands-on laboratory projects that can be closely integrated into the introductory AI course. Each project involves the design and implementation of a learning system which will enhance a particular commonly-deployed application. The goal is to enhance the student learning experience in the introductory artificial intelligence course by (1) introducing machine learning elements into the AI course, (2) implementing a set of unifying machine learning laboratory projects to tie together the core AI topics, and (3) developing, applying, and testing an adaptable framework for the presentation of core AI topics which emphasizes the important relationship between AI and computer science in general, and software development in particular. Details on this project as well as samples of course materials developed are published in [4, 5, 7, 8] and are available at the project website at http://uhaweb.hartford.edu/compsci/ccli.We present an overview of our work along with a detailed presentation of one of these projects and how it meets our goals.The project involves the development of a learning system for web document classification. Students investigate the process of classifying hypertext documents, called tagging, and apply machine learning techniques and data mining tools for automatic tagging. Our experiences using the projects are also presented.\"}, {\"paperId\": \"8fa73b871d15a445a1202dcf80f831c3464ba2e1\", \"abstract\": \"In January 2019, the U.S. Office of the Director of National Intelligence (ODNI) released a new strategy on the use of artificial intelligence (AI) technologies in U.S. intelligence. The report called for incorporating AI and automation technologies into intelligence work \\u2018to amplify the effectiveness of our workforce . . . advance mission capability and enhance the IC\\u2019s [Intelligence Community\\u2019s] ability to provide data interpretation to decision makers\\u2019. The ODNI noted it was evaluating and monitoring how these technologies might also have \\u2018vulnerabilities in development and adoption\\u2019. The report stated it was critical to address issues of \\u2018AI assurance, transparency, and reliability . . . to . . . understand how AI algorithms may fail\\u2019, and noted the importance of developing AI systems that \\u2018can demonstrate the underlying rationale behind decisions and responses to both users and overseers\\u2019. Finally, it emphasized the importance of monitoring \\u2018implementation and user feedback\\u2019 in a future AI-enabled workforce. This imagined future is not only to come; it is being realized now. Within the past few years, probably one of the most visibly controversial IC projects involving AI and intelligence analysis was Project Maven, a 2017 Department of Defense\\u2013driven intelligence project that used advances in big data, machine learning, and deep learning to extract objects of interest from drone-derived imagery, saving intelligence analysts hours of tedious imagery processing time. Project Maven partnered with Google to use some of the tech giant\\u2019s AI technology, but the project was ultimately cancelled in 2018 because of Google employee protests against the use of company algorithms for military targeting. Beyond Project Maven, a number of less controversial R&D programs ARE underway that aim to augment intelligence analysts\\u2019 capabilities. For example, the Defense Advanced Research Project Agency (DARPA)\\u2019s Explainable (XAI) Program is creating a set of new machine-learning techniques to \\u2018enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners\\u2019. The National Security Agency\\u2019s Laboratory for Analytic Sciences is developing big-data and AI-enabled technological platforms to bring more AI-enabled systems to analysts\\u2019 desktops. To date, these are individual proof-ofconcept technologies that may be applied and integrated into a variety of potential intelligence tools, such as: Journaling, a productivity device for intelligence analysts that enables them to understand their own individual work flows; OpenKE, which automates the collection and analysis of open-source information; BEAST, a platform of natural language processing and other extraction services that can scrape and process data from various sources for anticipatory intelligence analysis; and CyberMonkey, which allows a large number of analytic tools to be executed proactively in-browser instead of having to utilize tools sequentially or separately. All of these technologies aim \\u2018to assist the analyst without the analyst explicitly telling the machine everything it needs to do\\u2019. The Intelligence Advanced Research Project Activity (IARPA)\\u2019s Multimodal\"}, {\"paperId\": \"27c32f7176f9b990860aa3a4b13b9eec839a768d\", \"abstract\": \"This theme issue has the founding ambition of landscaping data ethics as a new branch of ethics that studies and evaluates moral problems related to data (including generation, recording, curation, processing, dissemination, sharing and use), algorithms (including artificial intelligence, artificial agents, machine learning and robots) and corresponding practices (including responsible innovation, programming, hacking and professional codes), in order to formulate and support morally good solutions (e.g. right conducts or right values). Data ethics builds on the foundation provided by computer and information ethics but, at the same time, it refines the approach endorsed so far in this research field, by shifting the level of abstraction of ethical enquiries, from being information-centric to being data-centric. This shift brings into focus the different moral dimensions of all kinds of data, even data that never translate directly into information but can be used to support actions or generate behaviours, for example. It highlights the need for ethical analyses to concentrate on the content and nature of computational operations\\u2014the interactions among hardware, software and data\\u2014rather than on the variety of digital technologies that enable them. And it emphasizes the complexity of the ethical challenges posed by data science. Because of such complexity, data ethics should be developed from the start as a macroethics, that is, as an overall framework that avoids narrow, ad hoc approaches and addresses the ethical impact and implications of data science and its applications within a consistent, holistic and inclusive framework. Only as a macroethics will data ethics provide solutions that can maximize the value of data science for our societies, for all of us and for our environments. This article is part of the themed issue \\u2018The ethical impact of data science\\u2019.\"}, {\"paperId\": \"4da040eaeb7f816c608ac49a1e568f7adae6ace8\", \"abstract\": \"This paper provides a broad bibliometric overview of the important conceptual advances that have been published during COVID-19 within \\u201ce-learning in higher education.\\u201d E-learning as a concept has been widely used in the academic and professional communities and has been approved as an educational approach during COVID-19. This article starts with a literature review of e-learning. Diverse subjects have appeared on the topic of e-learning, which is indicative of the dynamic and multidisciplinary nature of the field. These include analyses of the most influential authors, of models and networks for bibliometric analysis, and progress towards the current research within the most critical areas. A bibliometric review analyzes data of 602 studies published (2020\\u20132021) in the Web of Science (WoS) database to fully understand this field. The data were examined using VOSviewer, CiteSpace, and KnowledgeMatrix Plus to extract networks and bibliometric indicators about keywords, authors, organizations, and countries. The study concluded with several results within higher education. Many converging words or sub-fields of e-learning in higher education included distance learning, distance learning, interactive learning, online learning, virtual learning, computer-based learning, digital learning, and blended learning (hybrid learning). This research is mainly focused on pedagogical techniques, particularly e-learning and collaborative learning, but these are not the only trends developing in this area. The sub-fields of artificial intelligence, machine learning, and deep learning constitute new research directions for e-learning in light of COVID-19 and are suggestive of new approaches for further analysis.\"}, {\"paperId\": \"a1a28dcd3eb8a4085bf2ecdb63222af566e37751\", \"abstract\": null}, {\"paperId\": \"12c4cbf5f92233a88ffad0835764b5e9a411052f\", \"abstract\": null}, {\"paperId\": \"0e26f7c8d8b16fe2f4d1e315f42f0790990e8d4e\", \"abstract\": null}, {\"paperId\": \"d4bf41c55a0da57734f0604377c62fed4637e83d\", \"abstract\": \"Abstract \\u2018Deep learning\\u2019 is causing rapid technological changes in many fields of science, and conjectures about its potential for transforming everyone's work and lives is a matter of great debate. Unfortunately, it is all too easy to apply it as a \\u2018black box\\u2019 tool with little consideration of its potential limitations, especially when the data it is being applied to is less than perfect. In this Perspective, I try to put deep learning into a broader mechanistic and historical context by showing how it relates to older forms of artificial intelligence; by providing a general explanation of how it operates; and by exploring some of the challenges involved in its implementation. Examples wherein it has been applied to pest management problems are provided to illustrate how the technology works and the challenges deep learning faces. At least in the near term, its biggest impact on agrochemical development seems likely to come in automating the tedious work involved in assessing agrochemical efficacy, but getting there will require major investments in building large, well\\u2010curated data sets to work from and in providing the expertise required to assess the resulting model predictions in real\\u2010world scenarios. Deep learning may also come to complement the machine learning methodologies already available for use in pesticide discovery and development, but it seems unlikely to supplant them. \\u00a9 2020 The Authors. Pest Management Science published by John Wiley & Sons Ltd on behalf of Society of Chemical Industry.\"}, {\"paperId\": \"4e383bb858f43beacbef91fe65db5b81f5cc8d6c\", \"abstract\": \"Interdisciplinary research from the learning sciences has helped us understand a great deal about the way that humans learn, and as a result we now have an improved understanding about how best to teach and train people. This same body of research must now be used to better inform the development of Artificial Intelligence (AI) technologies for use in education and training. In this paper, we use three case studies to illustrate how learning sciences research can inform the judicious analysis, of rich, varied and multimodal data, so that it can be used to help us scaffold students and support teachers. Based on this increased understanding of how best to inform the analysis of data through the application of learning sciences research, we are better placed to design AI algorithms that can analyse rich educational data at speed. Such AI algorithms and technology can then help us to leverage faster, more nuanced and individualised scaffolding for learners. However, most commercial AI developers know little about learning sciences research, indeed they often know little about learning or teaching. We therefore argue that in order to ensure that AI technologies for use in education and training embody such judicious analysis and learn in a learning sciences informed manner, we must develop inter\\u2010stakeholder partnerships between AI developers, educators and researchers. Here, we exemplify our approach to such partnerships through the EDUCATE Educational Technology (EdTech) programme. Practitioner NotesWhat is already known about this topic? The progress of AI Technology and learning analytics lags behind the adoption of these approaches and technologies in other fields such as medicine or finance.Data are central to the empirical work conducted in the learning sciences and to the development of machine learning Artificial Intelligence (AI).Education is full of doubts about the value that any technology can bring to the teaching and learning process.What this paper adds? We argue that the learning sciences have an important role to play in the design of educational AI, through their provision of theories that can be operationalised and advanced.Through case studies, we illustrate that the analysis of data appropriately informed by interdisciplinary learning sciences research can be used to power AI educational technology.We provide a framework for inter\\u2010stakeholder, interdisciplinary partnerships that can help educators better understand AI, and AI developers better understand education.Implications for practice and/or policy? AI is here to stay and that it will have an increasing impact on the design of technology for use in education and training.Data, which is the power behind machine learning AI, can enable analysis that can vastly increase our understanding of when and how the teaching and learning process is progressing positively.Inter\\u2010stakeholder, interdisciplinary partnerships must be used to make sure that AI provides some of the educational benefits its application in other areas promise us. [ABSTRACT FROM AUTHOR]\"}, {\"paperId\": \"8cfda941a398cc2c5c6b0b861e764acc254bce4e\", \"abstract\": \"The world needs new materials to stimulate the chemical industry in key sectors of our economy: environment and sustainability, information storage, optical telecommunications, and catalysis. Yet, nearly all functional materials are still discovered by \\\"trial-and-error\\\", of which the lack of predictability affords a major materials bottleneck to technological innovation. The average \\\"molecule-to-market\\\" lead time for materials discovery is currently 20 years. This is far too long for industrial needs, as highlighted by the Materials Genome Initiative, which has ambitious targets of up to 4-fold reductions in average molecule-to-market lead times. Such a large step change in progress can only be realistically achieved if one adopts an entirely new approach to materials discovery. Fortunately, a fundamentally new approach to materials discovery has been emerging, whereby data science with artificial intelligence offers a prospective solution to speed up these average molecule-to-market lead times. This approach is known as data-driven materials discovery. Its broad prospects have only recently become a reality, given the timely and major advances in \\\"big data\\\", artificial intelligence, and high-performance computing (HPC). Access to massive data sets has been stimulated by government-regulated open-access requirements for data and literature. Natural-language processing (NLP) and machine-learning (ML) tools that can mine data and find patterns therein are becoming mainstream. Exascale HPC capabilities that can aid data mining and pattern recognition and also generate their own data from calculations are now within our grasp. These timely advances present an ideal opportunity to develop data-driven materials-discovery strategies to systematically design and predict new chemicals for a given device application. This Account shows how data science can afford materials discovery via a four-step \\\"design-to-device\\\" pipeline that entails (1) data extraction, (2) data enrichment, (3) material prediction, and (4) experimental validation. Massive databases of cognate chemical and property information are first forged from \\\"chemistry-aware\\\" natural-language-processing tools, such as ChemDataExtractor, and enriched using machine-learning methods and high-throughput quantum-chemical calculations. New materials for a bespoke application can then be predicted by mining these databases with algorithmic encodings of relationships between chemical structures and physical properties that are known to deliver functional materials. These may take the form of classification, enumeration, or machine-learning algorithms. A data-mining workflow short-lists these predictions to a handful of lead candidate materials that go forward to experimental validation. This design-to-device approach is being developed to offer a roadmap for the accelerated discovery of new chemicals for functional applications. Case studies presented demonstrate its utility for photovoltaic, optical, and catalytic applications. While this Account is focused on applications in the physical sciences, the generic pipeline discussed is readily transferable to other scientific disciplines such as biology and medicine.\"}, {\"paperId\": \"020640f95452f3f8af0634115059d8a696757707\", \"abstract\": \"Healthcare technologies have seen a surge in utilization during the COVID 19 pandemic. Remote patient care, virtual follow-up and other forms of futurism will likely see further adaptation both as a preparational strategy for future pandemics and due to the inevitable evolution of artificial intelligence. This manuscript theorizes the healthcare applications of digital twin technology. Digital twin is a triune concept that involves a physical model, a virtual counterpart, and the interplay between the two constructs. This interface between computer science and medicine is a new frontier with broad potential applications. We propose that digital twin technology can exhaustively and methodologically analyze the associations between a physical cancer patient and a corresponding digital counterpart with the goal of isolating predictors of neurological sequalae of disease. This proposition stems from the premise that data science can complement clinical acumen to scientifically inform the diagnostics, treatment planning and prognostication of cancer care. Specifically, digital twin could predict neurological complications through its utilization in precision medicine, modelling cancer care and treatment, predictive analytics and machine learning, and in consolidating various spectra of clinician opinions.\"}, {\"paperId\": \"a4cec122a08216fe8a3bc19b22e78fbaea096256\", \"abstract\": null}, {\"paperId\": \"6db137480b8cbc71c4c955147f870f2b112e467f\", \"abstract\": \"The Fourth Industrial Revolution drives industries from traditional manufacturing to the smart manufacturing approach. In this transformation, existing equipment, processes, or devices are retrofitted with some sensors and other cyber-physical systems (CPS), and adapted towards digital production, which is a blend of critical enabling technologies. In the current scenario of Industry 4.0, industries are shaping themselves towards the development of customized and cost-effective processes to satisfy customer needs with the aid of a digital twin framework, which enables the user to monitor, simulate, control, optimize, and identify defects and trends within, ongoing process, and reduces the chances of human prone errors. This paper intends to make an appraisal of the literature on the digital twin (DT) framework in the domain of smart manufacturing with the aid of critical enabling technologies such as data-driven systems, machine learning and artificial intelligence, and deep learning. This paper also focuses on the concept, evolution, and background of digital twin and the benefits and challenges involved in its implementation. The Scopus and Web of Science databases from 2016 to 2021 were considered for the bibliometric analysis and used to study and analyze the articles that fall within the research theme. For the systematic bibliometric analysis, a novel approach known as Proknow-C was employed, including a series of procedures for article selection and filtration from the existing databases to get the most appropriate articles aligned with the research theme. Additionally, the authors performed statistical and network analyses on the articles within the research theme to identify the most prominent research areas, journal/conference, and authors in the field of a digital twin. This study identifies the current scenarios, possible research gaps, challenges in implementing DT, case studies and future research goals within the research theme.\"}, {\"paperId\": \"857e515825f3da0c100825351433c85230af2157\", \"abstract\": \"Wearable sensors enable the real-time and non-invasive monitoring of biomechanical, physiological, or biochemical parameters pertinent to the performance of athletes. Sports medicine researchers compile datasets involving a multitude of parameters that can often be time consuming to analyze in order to create value in an expeditious and accurate manner. Machine learning and artificial intelligence models may aid in the clinical decision-making process for sports scientists, team physicians, and athletic trainers in translating the data acquired from wearable sensors to accurately and efficiently make decisions regarding the health, safety, and performance of athletes. This narrative review discusses the application of commercial sensors utilized by sports teams today and the emergence of descriptive analytics to monitor the internal and external workload, hydration status, sleep, cardiovascular health, and return-to-sport status of athletes. This review is written for those who are interested in the application of wearable sensor data and data science to enhance performance and reduce injury burden in athletes of all ages.\"}, {\"paperId\": \"c44cd1c2c4ee0406d81d0ef8677d7c873ab1be2d\", \"abstract\": null}, {\"paperId\": \"b87df5dd0dab53c681ef42421cfd2d4bd82f4e8e\", \"abstract\": \"The presumed data owners' right to explanations brought about by the General Data Protection Regulation in Europe has shed light on the social challenges of explainable artificial intelligence (XAI). In this paper, we present a case study with Deep Learning (DL) experts from a research and development laboratory focused on the delivery of industrial-strength AI technologies. Our aim was to investigate the social meaning (i.e. meaning to others) that DL experts assign to what they do, given a richly contextualized and familiar domain of application. Using qualitative research techniques to collect and analyze empirical data, our study has shown that participating DL experts did not spontaneously engage into considerations about the social meaning of machine learning models that they build. Moreover, when explicitly stimulated to do so, these experts expressed expectations that, with real-world DL application, there will be available mediators to bridge the gap between technical meanings that drive DL work, and social meanings that AI technology users assign to it. We concluded that current research incentives and values guiding the participants' scientific interests and conduct are at odds with those required to face some of the scientific challenges involved in advancing XAI, and thus responding to the alleged data owners' right to explanations or similar societal demands emerging from current debates. As a concrete contribution to mitigate what seems to be a more general problem, we propose three preliminary XAI Mediation Challenges with the potential to bring together technical and social meanings of DL applications, as well as to foster much needed interdisciplinary collaboration among AI and the Social Sciences researchers.\"}, {\"paperId\": \"ff77ca5f0e569f722c5acb925f7f032fe577d5b4\", \"abstract\": \"Challenges in risk estimation for agricultural insurance bring to the fore statistical problems of modeling complex weather and climate dynamics, analyzing massive multi-resolution, multi-source data. Nonstationary space-time structure of such data also introduces greater complexity when assessing the highly nonlinear relationship between weather events and crop yields. In this setting, conventional parametric statistical and actuarial models may no longer be appropriate. In turn, modern machine learning and artificial intelligence procedures, which allow fast and automatic learning of hidden dependencies and structures, offer multiple operational benefits and now prove to deliver a highly competitive performance in a variety of applications, from credit card fraud detection to the next best product offer and customer segmentation. Yet their potential in actuarial sciences, and particularly agricultural insurance, remains largely untapped. In this project, we introduce a modern deep learning methodology into the assessment of climate-induced risks in agriculture and evaluate its potential to deliver a higher predictive accuracy, speed, and scalability. We present a pilot study of deep learning algorithms\\u2014specifically, deep belief networks\\u2014using historical crop yields, weather station\\u2013based records, and gridded weather reanalysis data for Manitoba, Canada from 1996 to 2011. Our findings show that deep learning can attain higher prediction accuracy, based on benchmarking its performance against more conventional approaches, especially in multiscale, heterogeneous data environments of agricultural risk management.\"}, {\"paperId\": \"7ba32ff45f2c682f9fc5e12316cbb9ea983db7d0\", \"abstract\": \"More by this author Imagine a world where computational simulations can be inverted as easily as running them forwards, where data can be used to refine models automatically, and where the only expertise one needs to carry out powerful statistical analysis is a basic proficiency in scientific coding. Creating such a world is the ambitious long-term aim of probabilistic programming. The bottleneck for improving the probabilistic models, or simulators, used throughout the quantitative sciences... Inference in probabilistic logic programs using weighted CNFs. In Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI), AUAI Press, Corvallis, Oregon, USA, 211\\u2013220. Getoor, L. and Taskar, B. 2007. Learning the parameters of probabilistic logic programs from interpretations. In Proceedings of the 2008 European Conference on Machine Learning and Knowledge Discovery in Databases (ECML/PKDD), Part 1. Springer-Verlag, Berlin, Germany, 581\\u2013596. Ishihata, M., Kameya, Y., Sato, T. and Minato, S. 2008. 1.1 Automated Variational Inference for Probabilistic Programming. Probabilistic programming languages simplify the development of probabilistic models by allowing. programmers to specify a stochastic process using syntax that resembles modern programming lan 2 Automated Variational Inference. An unconditional probabilistic program f is defined as a parameterless function with an arbitrary. mix of deterministic and stochastic elements.\"}, {\"paperId\": \"a5f86802a93834dbbb2b8cc86505cec79d2b15bf\", \"abstract\": \"COVID-19 is a global pandemic disease, which results from a dangerous coronavirus attack, and spreads aggressively through close contacts with infected people and artifacts So far, there is not any prescribed line of treatment for COVID-19 patients Measures to control the disease are very limited, partly due to the lack of knowledge about technologies which could be effectively used for early detection and control the disease Early detection of positive cases is critical in preventing further spread, achieving the herd immunity, and saving lives Unfortunately, so far we do not have effective toolkits to diagnose very early detection of the disease Recent research findings have suggested that radiology images, such as X-rays, contain significant information to detect the presence of COVID-19 virus in early stages However, to detect the presence of the disease in in very early stages from the X-ray images by the naked eye is not possible Artificial Intelligence (AI) techniques, machine learning in particular, are known to be very helpful in accurately diagnosing many diseases from radiology images This paper proposes an automatic technique to classifyCOVID-19 patients from their computerized tomography (CT) scan images The technique is known as Advanced Inception based Recurrent Residual Convolution Neural Network (AIRRCNN), which uses machine learning techniques for classifying data We focus on the Advanced Inception based Recurrent Residual Convolution Neural Network, because we do not find it being used in the literature Also, we conduct principal component analysis, which is used for dimensional deduction Experimental results of our method have demonstrated an accuracy of about 99%, which is regarded to be very efficient \\u00a9 2021 Tech Science Press All rights reserved\"}, {\"paperId\": \"e40624c627c955c6c655e0d69c6027e92d83ab32\", \"abstract\": \"Big data analytics (BDA) is a novel concept focusing on leveraging large volumes of heterogeneous data through advanced analytics to drive information discovery. This paper aims to highlight the potential role BDA can play to improve groundwater management in the Southern African Development Community (SADC) region in Africa. Through a review of the literature, this paper defines the concepts of big data, big data sources in groundwater, big data analytics, big data platforms and framework and how they can be used to support groundwater management in the SADC region. BDA may support groundwater management in SADC region by filling in data gaps and transforming these data into useful information. In recent times, machine learning and artificial intelligence have stood out as a novel tool for data-driven modeling. Managing big data from collection to information delivery requires critical application of selected tools, techniques and methods. Hence, in this paper we present a conceptual framework that can be used to manage the implementation of BDA in a groundwater management context. Then, we highlight challenges limiting the application of BDA which included technological constraints and institutional barriers. In conclusion, the paper shows that sufficient big data exist in groundwater domain and that BDA exists to be used in groundwater sciences thereby providing the basis to further explore data-driven sciences in groundwater management.\"}, {\"paperId\": \"e5aeec169d2c4d5e8e730a3c382006ba983b8709\", \"abstract\": \"In recent years, we have witnessed a significant shift toward ever-more complex and ever-larger-scale systems in the majority of the grand societal challenges tackled in applied sciences. The need to comprehend and predict the dynamics of complex systems have spurred developments in large-scale simulations and a multitude of methods across several disciplines. The goals of understanding and prediction in complex dynamical systems, however, have been hindered by high dimensionality, complexity and chaotic behaviours. Recent advances in data-driven techniques and machine-learning approaches have revolutionized how we model and analyse complex systems. The integration of these techniques with dynamical systems theory opens up opportunities to tackle previously unattainable challenges in modelling and prediction of dynamical systems. While data-driven prediction methods have made great strides in recent years, it is still necessary to develop new techniques to improve their applicability to a wider range of complex systems in science and engineering. This focus issue shares recent developments in the field of complex dynamical systems with emphasis on data-driven, data-assisted and artificial intelligence-based discovery of dynamical systems. This article is part of the theme issue 'Data-driven prediction in dynamical systems'.\"}, {\"paperId\": \"5f71c4164d7e71c15c8e69342894c19a7adaf627\", \"abstract\": \"The emergence and growth of research on issues of ethics in Artificial Intelligence, and in particular algorithmic fairness, has roots in an essential observation that structural inequalities in our society are reflected in the data used to train predictive models and in the design of objective functions. While research aiming to mitigate these issues is inherently interdisciplinary, the design of unbiased algorithms and fair socio-technical systems are key desired outcomes which depend on practitioners from the fields of data science and computing. However, these computing fields broadly also suffer from the same under-representation issues that are found in the datasets we analyze. This disconnect affects the design of both the desired outcomes and metrics by which we measure success. If the ethical AI research community accepts this, we tacitly endorse the status quo and contradict the goals of non-discrimination and equity which work on algorithmic fairness, accountability, and transparency seeks to address. Therefore, we advocate in this work for diversifying computing as a core priority of the field and our efforts to achieve ethical AI practices. We draw connections between the lack of diversity within academic and professional computing fields and the type and breadth of the biases encountered in datasets, machine learning models, problem formulations, and the interpretation of results. Examining the current fairness/ethics in AI literature, we highlight cases where this lack of diverse perspectives has been foundational to the inequity in the treatments of underrepresented and protected group data. We also look to other professional communities, such as in the law and health domains, where disparities have been reduced both in the educational diversity of trainees and among their professional practices. We use these lessons to develop a set of key recommendations that provide concrete steps for the computing community to increased diversity: 1) building collaborations with minority serving institutions, 2) prioritizing research collaboration between the ethics in AI community and underrepresented/interdisciplinary groups, and 3) providing enhanced mentorship to trainees at research conferences.\"}, {\"paperId\": \"e8ae8381a8f0e9e26da13fd30087b31e8ec1649f\", \"abstract\": null}, {\"paperId\": \"239427493fca1a02be1836ca6f5e3ac06dd8ca10\", \"abstract\": \"A central problem in cognitive science and behavioural neuroscience as well as in machine learning and artificial intelligence research is to ascertain whether two or more decision makers (e.g. brains or algorithms) use the same strategy. Accuracy alone cannot distinguish between strategies: two systems may achieve similar accuracy with very different strategies. The need to differentiate beyond accuracy is particularly pressing if two systems are at or near ceiling performance, like Convolutional Neural Networks (CNNs) and humans on visual object recognition. Here we introduce trial-by-trial error consistency, a quantitative analysis for measuring whether two decision making systems systematically make errors on the same inputs. Making consistent errors on a trial-by-trial basis is a necessary condition if we want to ascertain similar processing strategies between decision makers. Our analysis is applicable to compare algorithms with algorithms, humans with humans, and algorithms with humans. When applying error consistency to visual object recognition we obtain three main findings: (1.) Irrespective of architecture, CNNs are remarkably consistent with one another (2.) The consistency between CNNs and human observers, however, is little above what can be expected by chance alone--indicating that humans and CNNs are likely implementing very different strategies (3.) CORnet-S, a recurrent model termed the \\\"current best model of the primate ventral visual stream\\\", fails to capture essential characteristics of human behavioural data and behaves essentially like a ResNet-50 in our analysis--that is, just like a standard feedforward network. Taken together, error consistency analysis suggests that the strategies used by human and machine vision are still very different--but we envision our general-purpose error consistency analysis to serve as a fruitful tool for quantifying future progress.\"}, {\"paperId\": \"1b98d66ef56dc511b6d7400fa33e8bfcf9f753d7\", \"abstract\": \"Many of the current scientific advances in the life sciences have their origin in the intensive use of data for knowledge discovery. In no area this is so clear as in bioinformatics, led by technological breakthroughs in data acquisition technologies. It has been argued that bioinformatics could quickly become the field of research generating the largest data repositories, beating other data-intensive areas such as high-energy physics or astroinformatics. Over the last decade, deep learning has become a disruptive advance in machine learning, giving new live to the long-standing connectionist paradigm in artificial intelligence. Deep learning methods are ideally suited to large-scale data and, therefore, they should be ideally suited to knowledge discovery in bioinformatics and biomedicine at large. In this brief paper, we review key aspects of the application of deep learning in bioinformatics and medicine, drawing from the themes covered by the contributions to an ESANN 2018 special session devoted to this topic.\"}, {\"paperId\": \"c30e572e972f488b49e9ccbf150fd500445442e1\", \"abstract\": \"Understanding the role that the environment plays in influencing public health often involves collecting and studying large, complex data sets. There have been a number of private and public efforts to gather sufficient information and confront significant unknowns in the field of environmental public health, yet there is a persistent and largely unmet need for findable, accessible, interoperable, and reusable (FAIR) data. Even when data are readily available, the ability to create, analyze, and draw conclusions from these data using emerging computational tools, such as augmented and artificial intelligence (AI) and machine learning, requires technical skills not currently implemented on a programmatic level across research hubs and academic institutions. We argue that collaborative efforts in data curation and storage, scientific computing, and training are of paramount importance to empower researchers within environmental sciences and the broader public health community to apply AI approaches and fully realize their potential. Leaders in the field were asked to prioritize challenges in incorporating big data in environmental public health research: inconsistent implementation of FAIR principles in data collection and sharing, a lack of skilled data scientists and appropriate cyber-infrastructures, and limited understanding of possibilities and communication of benefits were among those identified. These issues are discussed, and actionable recommendations are provided.\"}, {\"paperId\": \"4345878689d79fc25113c1941b49c268cc81f1af\", \"abstract\": \"SensorsTracking and with Tracking and of Multisensor Data FusionMulti-sensor Multi-target Data Fusion, and Identification Techniques for and Control ApplicationsMultisensor Data Fusion and Machine Learning for Remote SensingActivity-based Data Fusion for the Automated Progress of Construction ProjectsDesign and Analysis of Modern Tracking SystemsTarget Tracking and Data Fusion MATLAB(TM) workingin remote sensors and tracking, Estimation with Applications controlling the actions. Here, the focus is on the more mature phase of data fusion, namely the detection and identification / classification of phenomena being observed and exploitation of the related methods for Security-Related Civil Science and Technology (SST) applications. It is necessary to; expand on the data fusion methodology pertinent to Situation Monitoring, Incident Detection, Alert and Response Management; discuss some related Cognitive Engineering and visualization issues; provide an insight into the architectures and methodologies for building a data fusion system; discuss fusion approaches to image exploitation with emphasis on security applications; discuss novel distributed tracking approaches as a necessary step of situation monitoring and incident detection; and provide examples of real situations, in which data fusion can enhance incident detection, prevention and response capability. In order to give a logical presentation of the data fusion material, first the general concepts are highlighted (Fusion Methodology, Human Computer Interactions and Systems and Architectures), closing with several applications (Data Fusion for Imagery, Tracking and Sensor Fusion and Applications and Opportunities for Fusion).Multisensor Data Fusion: From Algorithms and Architectural Design to Applications covers the contemporary theory and practice of multisensor data fusion, from fundamental concepts to cutting-edge techniques drawn from a broad array of disciplines. Featuring contributions from the world\\u2019s leading data fusion researchers and academicians, this authoritative book: Presents state-of-the-art advances in the design of multisensor data fusion algorithms, addressing issues related to the nature, location, and computational ability of the sensors Describes new materials and achievements in optimal fusion and multisensor filters Discusses the advantages and challenges associated with multisensor data fusion, from extended spatial and temporal coverage to imperfection and diversity in sensor technologies Explores the topology, communication structure, computational resources, fusion level, goals, and optimization of multisensor data fusion system architectures Showcases applications of multisensor data fusion in fields such as medicine, transportation's traffic, defense, and navigation Multisensor Data Fusion: From Algorithms and Architectural Design to Applications is a robust collection of modern multisensor data fusion methodologies. The book instills a deeper understanding of the basics of multisensor data fusion as well as a practical knowledge of the problems that can be faced during its execution.This textbook provides a comprehensive introduction to the concepts and idea of multisensor data fusion. It is This text explores the use of statistical/probabilistic signal/image processing, filtering, component analysis, image algebra, decision making, and neuro-FL\\u2013GA paradigms in studying, developing and validating data fusion processes (DFP). It covers major mathematical expressions, and formulae and equations as well as, where feasible, their derivations. It also discusses SDF concepts, DF models and architectures, aspects and methods of type 1 and 2 fuzzy logics, and related practical applications. In addition, the author covers soft computing paradigms that are finding increasing applications in multisensory DF approaches and applications. This book: Explores the use of interval type 2 fuzzy logic and ANFIS in DF Covers the mathematical treatment of many types of filtering algorithms, target-tracking methods, and kinematic DF methods Presents single and multi-sensor tracking and fusion mathematics Considers specific DF architectures in the context of decentralized systems Discusses information filtering, Bayesian approaches, several DF rules, image algebra and image fusion, decision fusion, and wireless sensor network (WSN) multimodality fusion Data Fusion Mathematics: Theory and Practice incorporates concepts, processes, methods, and approaches in data fusion that can help you with integrating DF mathematics and achieving higher levels of fusion activity, and clarity of performance. This text is geared toward researchers, scientists, teachers and practicing engineers interested and working in the multisensor data fusion area.Data Fusion Performance Evaluation for Dissimilar Sensors: Application to Road Obstacle Tracking.The security of the U.S. focuses on data fusion as a means to significantly improve the ability of the existing suite of airport detection systems and access control systems to detect and prevent attacks. The book presents a discussion of the data fusion, an analysis of current data fusion efforts, and an assessment of data fusion opportunities for various airport security activities.The emerging technology of multisensor data fusion has a wide range of applications, both in Department of Defense (DoD) areas and in the civilian arena. The techniques of multisensor data fusion draw from an techniques: fuzzy logic, random set theory, and conditional and relational event algebra. Audience: This volume can be used as a reference book for researchers and practitioners in data fusion or expert systems theory, or for graduate students as text for a research seminar or graduate level course.This book includes papers from the section \\u201cMultisensor Information Fusion\\u201d, from Sensors between 2018 to 2019. It focuses on the latest research results of current multi-sensor fusion technologies and represents the latest research trends, including traditional information fusion technologies, estimation and filtering, and the latest research, artificial intelligence involving deep learning.The direction of arrival (DOA) computed from the monopulse ratio is known to fluctuate widely in the presence of multiple unresolved targets. This confounds traditional trackers operating on unresolved targets, leading to erroneous state estimates or loss of track. This paper presents a computationally feasible solution to this problem using Metron s Unified Theory of Data Fusion (UDF). UDF is a Bayesian method that maintains a probability density on the joint target state space. It operates without explicitly enumerating multiple data-totarget associations. This is particularly important for unresolved targets where the data cannot be attributed to a single target. Likelihood functions for two Rayleigh targets over a range of SNRs are examined first to develop insight. The final example presents the application to tracking two low-SNR targets crossing the radar beam.Sensor Data Fusion is the process of combining incomplete and imperfect pieces of mutually complementary sensor information in such a way that a better understanding of an underlying real-world phenomenon is achieved. Typically, this insight is either unobtainable otherwise or a fusion result exceeds what can be produced from a single sensor output in accuracy, reliability, or cost. This book provides an introduction Sensor Data Fusion, as an information technology as well as a branch of engineering science and informatics. Part I presents a coherent methodological framework, thus providing the prerequisites for discussing selected applications in Part II of the book. The presentation mirrors the author's views on the subject and emphasizes his own contributions to the development of particular aspects. dynamic system by processing sensor data. The book then employs principal component analysis, spatial frequency, and wavelet-based image fusion algorithms for the fusion of image data from sensors. It also presents procedures for combing tracks obtained from imaging sensor and ground-based radar. The final chapters discuss how DF is applied to mobile intelligent autonomous systems and intelligent monitoring systems. Fusing sensors\\u2019 data can lead to numerous benefits in a system\\u2019s performance. Through real-world examples and the evaluation of algorithmic results, this detailed book provides an understanding of MSDF concepts and methods from a practical point of view. Select MATLAB programs are available for download on www.crcpress.comThe goal of this report is to conduct an exhaustive formal literature survey on multisensor data fusion and use the results to conduct performance analyses of the following sensor data fusion subjects: sensor data association & fusion architectures; data association; data fusion; data alignment or registration; target attribute estimation & fusion; application of artificial intelligence techniques; target state estimation analysis; and target model (type of maneuver) identification analysis. The different analysis approaches published in the surveyed literature are identified for each of the above subjects, and relative merits and trade-offs between these approaches are evaluated. The analyses focus on those approaches which could be pertinent to a naval platform employing dissimilar and non-imaging sensors. Includes glossary.Here's a thorough overview of the state-of-the-art in design and implementation of advanced tracking for single and multiple sensor systems. This practical resource provides modern system designers and analysts with in-depth evaluations of sensor management, kinematic and attribute data processing, data association, situation assessment, and modern tracking and data fusion methods as applied in both military and non-military arenas.This book illustrates the benefits of sensor fusion by considering the characteristics of infrared, microwave, and m\"}, {\"paperId\": \"29d12a495ae810de9e3bc6e53af31934a164b821\", \"abstract\": \"Artificial intelligence (AI)-driven science is an integral component in several science domains such as materials, biology, high energy physics, and smart energy. Science workflows can span one or more computational, observational, and experimental systems. The AI for Science report put forth by a wide community of stakeholders from national laboratories, academia, and industry collectively stress the need for a tighter integration of AI infrastructure ecosystem with experimental and leadership computing facilities. The AI component of science applications, which generally deploy deep learning (DL) models, are unique and exhibit different characteristics from traditional industrial workloads. They implement complex models and typically incorporate hundreds of millions of model parameters. Data from simulations are usually sparse, multimodal, multidimensional, and exhibit temporal and spatial correlations.Moreover, AI-driven science applications benefit from flexible coupling of simulations with DL training or inference. Such complexity of the AI for science workloads with increasingly large DL models is typically limited by traditional computing architectures. The adoption of novel AI architectures and systems aimed to accelerate machine learning models is critical to reduce the time-to-discovery for science. The Argonne Leadership Computing Facility (ALCF), a US Department of Energy Office of Science user facility, provides supercomputing resources to power scientific breakthroughs. Applications with significant DL components are increasingly being run on existing supercomputers at the facility. Scientists at ALCF are exploring novel AI-hardware systems, such as SambaNova, in an attempt to address the challenges in scaling the performance of AI models.\"}, {\"paperId\": \"2166b1a417b1a64bfb8ca790e69b1d411d7d5473\", \"abstract\": \"The discipline, science, and art of orthodontics are concerned with the face and ability to modify its growth. Orthodontists achieve their goals by manipulating the craniofacial skeleton, with particular emphasis on modifying the dentoalveolar region, external orthopedic forces are applied that mirror some techniques used in medical orthopedics. Most treatments, however, focus on modifying the occlusion and controlling dentoalveolar development and abnormal facial growth, thus, enormous amounts of designs and techniques invented in the diagnostic and treatment domains aiming at boolean etiological identification and optimized strategies of solution delivered. A valid problem assessment enables health providers to determine treatment need and priority, and as health care moves toward more stringent financial accountability. the inventory of the computer and its implementation in different medical field was of great interest, this interest are even greater with the artificial intelligence introduction (AI). The best definition for the phrase \\u201cAI\\u201d calls for formalization of the term \\u201cintelligence\\u201d. Psychologist and cognitive theorists are of the opinion that intelligence helps in identifying the right piece of knowledge at the appropriate instances of decision making [1,2].The phrase \\u201cAI\\u201d thus can be defined as the simulation of human intelligence on a machine. Thus, AI alternatively may be stated as a subject dealing with computational models that can think and act rationally [3-7]. The subject of AI spans a wide horizon. It deals with the various kinds of knowledge representation schemes, different techniques of intelligent search, various methods for resolving uncertainty of data and knowledge, diffrent schemes for automated machine learning and many others. Among the application areas of AI, we have Expert systems, Game-playing, and Theorem-proving, Natural language processing, Image recognition, Robotics and many others. This chapter aims at bringing the insight of interest to the conjugation relatively recently happened between orthodontics discipline and AI subject.\"}, {\"paperId\": \"9cd93137427957a0ce5b071965d78b946e6f5088\", \"abstract\": \"The science education community has embraced the deployment of contemporaneous technological tools and platforms in the service of improving science teaching, learning, and assessment. Technology use in science education has ranged\\u2014among many other things\\u2014from computer-assisted instruction in the 1970s, to using microcomputer-based laboratories and first-generation simulations and micro-worlds in the 1980s and throughout the 1990s. The 1990s also witnessed the deployment of interactive videodiscs, multimedia, hypermedia, and other digital resources as cognitive tools in science classrooms (Songer, 2007), which was followed by efforts to harness the power of the Internet to, for instance, share data in support of multisite student-driven inquiry projects, among many other applications (Abd-El-Khalick, 2001). The last two decades featured the expanded and integrated use of learning-specific software applications, interactive visualizations, modeling tools, and immersive e-learning environments in science teaching and learning (Krajcik & Mun, 2014). The last decade or so has witnessed rapid and groundbreaking advancements in technologies and digital platforms, as well as their application to teaching and learning. These include advances that made high-powered computing and powerful applications in serious gaming, and virtual and augmented reality more accessible to mainstream users. Similar advances have been evident in big data curation, data mining and data analytics, natural language processing, as well as next-generation machine learning and the application of artificial intelligence (AI) to the real-time and adaptive assessment of learning. The coordination through powerful computing and AI of tangible, immersive, intelligent, and multiuser technologies and digital media systems (coordinating, for instance, learner interactions with interactive wall displays, intelligent interfaces, multitouch tables, motion sensors, etc.) now allow the creation of digital ecologies that provide learners with highly engaging and authentic interactive science learning experiences. Simultaneously, these technologies and interfaces enable the collection of massive data about the choices, behaviors, and cognition of an individual learner or groups of learners (keyboard strokes, mouse clicks, eye tracking, body movement, etc.) that allow for real-time feedback both to learners and their teachers, as well as the delivery of adaptive and personalized learning experiences (ILSDI, 2014). These digital technologies and ecologies have the potential to transform science teaching and learning, as well as deliver on the promise of more personalized science education experiences in service of promoting scientific literacy for all (NGSS Lead States, 2013) including, but not limited to, historically underrepresented populations in science, culturally, ethnically, and linguistically diverse students, as well as learners in underprivileged and underserved communities in the United States and around the globe. The aims of this Special Issue of JRST is to provide a platform for reporting on empirical research that examines the use and impact of 21st century cutting-edge technologies, technological platforms, technological activity, and digital ecologies on science teaching, learning, and assessment. We also Received: 20 December 2018 Accepted: 20 December 2018\"}, {\"paperId\": \"a39315dccafd125fdb9998b2d8498130d6d90d05\", \"abstract\": \"Last years have been characterized by an upsurge of opaque automatic decision support systems, such as Deep Neural Networks (DNNs). Although they have great generalization and prediction skills, their functioning does not allow obtaining detailed explanations of their behaviour. As opaque machine learning models are increasingly being employed to make important predictions in critical environments, the danger is to create and use decisions that are not justifiable or legitimate. Therefore, there is a general agreement on the importance of endowing machine learning models with explainability. The reason is that EXplainable Artificial Intelligence (XAI) techniques can serve to verify and certify model outputs and enhance them with desirable notions such as trustworthiness, accountability, transparency and fairness. This tutorial is meant to be the go-to handbook for any audience with a computer science background aiming at getting intuitive insights of machine learning models, accompanied with straight, fast, and intuitive explanations out of the box. We believe that this article provide a valuable contribution for applying XAI techniques in their particular day-to-day models, datasets and use-cases. Figure 1 acts as a flowchart/map for the reader and should help him to find the ideal method to use according to his type of data. The reader will find a description of the proposed method as well as an example of use and a Python notebook that he can easily modify as he pleases in order to apply it to his own case of application.\"}, {\"paperId\": \"ed88ab30162e3fb43fb0c7cec598202b1cb4516d\", \"abstract\": \"Machine learning is a highly influential field that has made major contributions to the increased effectiveness of artificial intelligence. Machine learning utilizes different methods, four of which have been particularly effective. The Analogizers classify patterns based on their similarity to other patterns. Multidimensional scaling provides support. The Bayesians revise the probability of hypotheses based on new evidence. The Connectionists adjust the strength between layers of \\u201cneurons.\\u201d Deep leaning based on many layers of connections has proven particularly successful. The Symbolists use rules that combine pieces of pre-existing knowledge. Hybrid systems combine these methods to create systems that are more effective than individual methods.\"}, {\"paperId\": \"2792f3a8058c7a019b2a18fb3b7e43c828086b81\", \"abstract\": null}, {\"paperId\": \"d91b507ffc7141ace9b8ea53b2b1a9171f1e39b6\", \"abstract\": \"The motivation of this study is to leverage recent breakthroughs in artificial intelligence research to unlock novel solutions to important scientific problems encountered in computational science. To address the human intelligence limitations in discovering reduced-order models, we propose to supplement human thinking with artificial intelligence. Our three-pronged strategy consists of learning (i) models expressed in analytical form, (ii) which are evaluated a posteriori, and iii) using exclusively integral quantities from the reference solution as prior knowledge. In point (i), we pursue interpretable models expressed symbolically as opposed to black-box neural networks, the latter only being used during learning to efficiently parameterize the large search space of possible models. In point (ii), learned models are dynamically evaluated a posteriori in the computational solver instead of based on a priori information from preprocessed high-fidelity data, thereby accounting for the specificity of the solver at hand such as its numerics. Finally in point (iii), the exploration of new models is solely guided by predefined integral quantities, e.g., averaged quantities of engineering interest in Reynolds-averaged or large-eddy simulations (LES). We use a coupled deep reinforcement learning framework and computational solver to concurrently achieve these objectives. The combination of reinforcement learning with objectives (i), (ii) and (iii) differentiate our work from previous modeling attempts based on machine learning. In this report, we provide a high-level description of the model discovery framework with reinforcement learning. The method is detailed for the application of discovering missing terms in differential equations. An elementary instantiation of the method is described that discovers missing terms in the Burgers' equation.\"}, {\"paperId\": \"1fe7ce8e3eec85c9a35f213044d1fa3e855c3897\", \"abstract\": \"Computer or human? Proving that we are human is now part of many tasks that we do on the internet, such as creating an email account, voting in an online poll, or even downloading a scientific paper. One of the most popular tests is text-based CAPTCHA, where would-be users are asked to decipher letters that may be distorted, partially obscured, or shown against a busy background. This test is used because computers find it tricky, but (most) humans do not. George et al. developed a hierarchical model for computer vision that was able to solve CAPTCHAs with a high accuracy rate using comparatively little training data. The results suggest that moving away from text-based CAPTCHAs, as some online services have done, may be a good idea. Science, this issue p. eaag2612 A hierarchical computer vision model solves CAPTCHAs with a high accuracy rate using relatively little training data. INTRODUCTION Compositionality, generalization, and learning from a few examples are among the hallmarks of human intelligence. CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart), images used by websites to block automated interactions, are examples of problems that are easy for people but difficult for computers. CAPTCHAs add clutter and crowd letters together to create a chicken-and-egg problem for algorithmic classifiers\\u2014the classifiers work well for characters that have been segmented out, but segmenting requires an understanding of the characters, which may be rendered in a combinatorial number of ways. CAPTCHAs also demonstrate human data efficiency: A recent deep-learning approach for parsing one specific CAPTCHA style required millions of labeled examples, whereas humans solve new styles without explicit training. By drawing inspiration from systems neuroscience, we introduce recursive cortical network (RCN), a probabilistic generative model for vision in which message-passing\\u2013based inference handles recognition, segmentation, and reasoning in a unified manner. RCN learns with very little training data and fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters. In addition, RCN outperforms deep neural networks on a variety of benchmarks while being orders of magnitude more data-efficient. RATIONALE Modern deep neural networks resemble the feed-forward hierarchy of simple and complex cells in the neocortex. Neuroscience has postulated computational roles for lateral and feedback connections, segregated contour and surface representations, and border-ownership coding observed in the visual cortex, yet these features are not commonly used by deep neural nets. We hypothesized that systematically incorporating these findings into a new model could lead to higher data efficiency and generalization. Structured probabilistic models provide a natural framework for incorporating prior knowledge, and belief propagation (BP) is an inference algorithm that can match the cortical computational speed. The representational choices in RCN were determined by investigating the computational underpinnings of neuroscience data under the constraint that accurate inference should be possible using BP. RESULTS RCN was effective in breaking a wide variety of CAPTCHAs with very little training data and without using CAPTCHA-specific heuristics. By comparison, a convolutional neural network required a 50,000-fold larger training set and was less robust to perturbations to the input. Similar results are shown on one- and few-shot MNIST (modified National Institute of Standards and Technology handwritten digit data set) classification, where RCN was significantly more robust to clutter introduced during testing. As a generative model, RCN outperformed neural network models when tested on noisy and cluttered examples and generated realistic samples from one-shot training of handwritten characters. RCN also proved to be effective at an occlusion reasoning task that required identifying the precise relationships between characters at multiple points of overlap. On a standard benchmark for parsing text in natural scenes, RCN outperformed state-of-the-art deep-learning methods while requiring 300-fold less training data. CONCLUSION Our work demonstrates that structured probabilistic models that incorporate inductive biases from neuroscience can lead to robust, generalizable machine learning models that learn with high data efficiency. In addition, our model\\u2019s effectiveness in breaking text-based CAPTCHAs with very little training data suggests that websites should seek more robust mechanisms for detecting automated interactions. Breaking CAPTCHAs using a generative vision model. Text-based CAPTCHAs exploit the data efficiency and generative aspects of human vision to create a challenging task for machines. By handling recognition and segmentation in a unified way, our model fundamentally breaks the defense of text-based CAPTCHAs. Shown are the parses by our model for a variety of CAPTCHAs . Learning from a few examples and generalizing to markedly different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing\\u2013based inference handles recognition, segmentation, and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects such as data efficiency and compositionality that may be important in the path toward general artificial intelligence.\"}, {\"paperId\": \"ef117e009875e908f2bc3fe11f56f42272eec8a9\", \"abstract\": \"Artificial intelligence (AI) and machine learning, in particular, have gained significant interest in many fields, including pharmaceutical sciences. The enormous growth of data from several sources, the recent advances in various analytical tools, and the continuous developments in machine learning algorithms have resulted in a rapid increase in new machine learning applications in different areas of pharmaceutical sciences. This review summarizes the past, present, and potential future impacts of machine learning technologies on different areas of pharmaceutical sciences, including drug design and discovery, preformulation, and formulation. The machine learning methods commonly used in pharmaceutical sciences are discussed, with a specific emphasis on artificial neural networks due to their capability to model the nonlinear relationships that are commonly encountered in pharmaceutical research. AI and machine learning technologies in common day-to-day pharma needs as well as industrial and regulatory insights are reviewed. Beyond traditional potentials of implementing digital technologies using machine learning in the development of more efficient, fast, and economical solutions in pharmaceutical sciences are also discussed.\"}, {\"paperId\": \"341c8932f43532f11ced980ecd1023fbbebd5c68\", \"abstract\": \"That phytochemicals are a critical part of enhancing healthspan is in our minds a linch-pinofresponsiblepublichealthmessaging,yet,recentdietaryguidelinesandstrategies for implementing precision nutrition largely ignore roles of phytochemicals. Epidemiological evidence points strongly to beneficial effects of phytochemical-rich foods on the prevention of essentially all chronic diseases. An extraordinary multitude of phytochemicals have been shown in preclinical settings to be potent allies in our fight against the entire spectrum of chronic diseases and many acute conditions such as infections. Yet unequivocal proof of this concept is problematic due to the nature of the clinical trials that must be part of such a proof. More rigor in the design and implementation of such trials is essential. Artificial intelligence, machine learning, metabolomics, microbiomics, proteomics, and other high-powered data processing modalities, may inform the phyto-dynamic actions on very specific metabolic pathways. As phytochemical abundance appears to be declining in our food supply, the need for better and more strategically focused science is great.\"}, {\"paperId\": \"fbf24b0ce465336e409f34b83fc624016be4cb3b\", \"abstract\": \"With each new year, numerous reports, columns, and blogs are written to project the upcoming technologies and trends in health care. Several 2017 technology trends point to the growing use of artificial intelligence (AI) in health care. Gartner's Top 10 Strategic Technology Trends for 2017 (Cearley, Walker, & Burke, 2016) highlight three top trends: A and advanced machine learning, intelligent apps, and intelligence things. Let's look at each.First, certain technologies and specific techniques, such as deep learning, neural networks, and natural language processing, are encompassed within the AI and machine-learning concept. These techniques create software programs that are more than just rulebased systems. Rather, these systems can \\\"understand, learn, predict, adapt and appear intelligent\\\" (Cearley et al., 2016). Their ability to learn is key to their functionality. For example, a machine-learning system can analyze numerous electronic health records (EHRs) and recommend potential effective treatments. As more datasets are added, the system can learn and adapt the recommendations, for example, adding genomic data to the EHR database.Intelligent apps, the second trend, refer to virtual personal assistants (VPAs). VPAs help users with everyday tasks, for example, sorting email or answering simple questions (just as Siri\\u2122 and Cortana\\u2122 do on our smartphones). VPAs will become more available in health care in the coming year. Intelligent things, the third trend, break down into three distinct categories: robots, drones, and autonomous vehicles.In discussing health information technology trends for 2017, Health Data Management (2016) also named AI as the first trend, stating that, although AI exploded in health care in 2016, applications were typically very specialized. More general use is projected, which \\\"will mean better access to actionable intelligence.\\\" Padmanabhan (2016) echoed this notion: \\\"In 2017, we are more likely to hear terms such as 'cognitive computing' and 'artificial intelligence' \\u201e.and less likely to hear the term 'big data analytics,' which now seems to be limiting in its description of the actual work being done in advanced analytics.\\\"AI BASICSThe term artificial intelligence is not new. It dates back to the 1940s and 1950s, and Turing (1950) asked if machines can think. AI was even used in health care in the 1970s, for example, with a system called MYCIN, developed by Stanford University, that identified bacterial infections and recommended treatments (Shortliffe, 1976). MYCIN contained three components: a knowledge base created by experts, an inference engine with rule-based algorithms, and a user interface. Although there were other instances of expert system designed in that era, there was never a critical mass of users to adopt them for clinical practice. Most health care professionals did not see the need for machines to tell them how to practice.Circumstances have changed. According to the Executive Office of the President, National Science and Technology Council Committee on Technology (2016, p. 6), the current motivation for AI was \\\"driven by three mutually reinforcing factors: the availability of big data.. .dramatically improved machine learning approaches and algorithms \\u201eand the capabilities of more powerful computers.\\\"COGNTIVE COMPUTINGCognitive computing (CC) is an emerging term that some view as a subset of AI. Kelly (2016, p. 1) believes that the future oftechnology will be \\\"cognitive and not artificial\\\" and defines CC in terms of \\\"systems that learn at scale, reason with purpose and interact with humans naturally.\\\" One other distinguishing factor is that CC can handle \\\"unstructured data,\\\" whereas AI applications are typically based on structured or numeric data. Marr (2016) summarizes CC as \\\"a mashup of cognitive science - the study of the human brain and how it functions - and computer science, and the results will have far-reaching impacts on our private lives, healthcare, business, and more. \\u2026\"}, {\"paperId\": \"5c5b86ba7cccf65f88bb415de64e2c291c58fdd2\", \"abstract\": \"PurposePredictive analytics and artificial intelligence are perceived as significant drivers to improve organizational performance and managerial decision-making. Hiring employees and contract renewals are instances of managerial decision-making problems that can incur high financial costs and long-term impacts on organizational performance. The primary goal of this study is to identify the Major League Baseball (MLB) free agents who are likely to receive a contract.Design/methodology/approachThis study used the design science research paradigm and the cognitive analytics management (CAM) theory to develop the research framework. A dataset on MLB's free agents between 2013 and 2017 was collected. A decision support tool was built using artificial neural networks.FindingsThere are clear links between a player's statistical performance and the decision of the player to sign a new offered contract. \\u201cAge,\\u201d \\u201cWins above Replacement\\u201d and \\u201cthe team on which a player last played\\u201d are the most significant factors in determining if a player signs a new contract.Originality/valueThis paper applied analytical modeling to personnel decision-making using the design science paradigm and guided by CAM as the kernel theory. The study employed machine learning techniques, producing a model that predicts the probability of free agents signing a new contract. Also, a web-based tool was developed to help decision-makers in baseball front offices so they can determine which available free agents to offer contracts.\"}, {\"paperId\": \"a82879f4bb1c72b3fc58e703b68888bdb07c953d\", \"abstract\": \"Decrypting pathogen effector networks Many disease-causing bacteria use a molecular syringe to inject dozens of their proteins, called effectors, into intestinal cells, blocking key immune responses. Ruano-Gallego et al. used the mouse pathogen Citrobacter rodentium to model effector function in vivo. They found that effectors work together as a network, allowing the microbe great flexibility in maintaining pathogenicity. An artificial intelligence platform correctly predicted colonization outcomes of alternative networks from the in vivo data. However, the host was able to bypass the obstacles erected by different effector networks and activate complementary immune responses that cleared the pathogen and induced protective immunity. Science, this issue p. eabc9531 The cooperative functions of bacterial effectors form networks that can withstand network contractions and maintain pathogenicity. INTRODUCTION Infections with many Gram-negative pathogens, including Escherichia coli, Salmonella, Shigella, and Yersinia, rely on the injection of effectors via type III secretion systems (T3SSs). The effectors hijack cellular processes through multiple mechanisms, including molecular mimicry and diverse enzymatic activities. Although in vitro analyses have shown that individual effectors can exhibit complementary, interdependent, or antagonistic relationships, most in vivo studies have focused on the contribution of single effectors to pathogenesis. Citrobacter rodentium is a natural mouse pathogen that shares infection strategies and virulence factors with the human pathogens enteropathogenic and enterohemorrhagic E. coli (EPEC and EHEC). The ability of these pathogens to colonize the gastrointestinal tract is mediated by the injection of effectors via a T3SS. Although C. rodentium infects 31 effectors, the prototype EPEC strain E2348/69 translocates 21 effectors. RATIONALE The aim of this study was to test the hypotheses that, rather than operating individually, the T3SS effectors form robust intracellular networks that can sustain large contractions and that expanded effector repertoires play a role in distinct disease phenotypes and host adaption. RESULTS We tested the effector-network paradigm by infecting mice with >100 C. rodentium effector mutant combinations. First, using machine learning prediction algorithms, we discovered additional effectors, NleN and NleO. We then sequentially deleted effector genes from two distinct starting points to reach sustainable endpoints, which resulted in strains missing 19 unrelated effectors (CR14) or 10 effectors involved in the modulation of innate immune responses in intestinal epithelial cells (IECs) (CRi9). Moreover, we deleted Map and EspF, which target the mitochondria and disrupt tight junctions. Unexpectedly, all strains colonized the colon and activated conserved metabolic and antimicrobial processes in the IECs while eliciting distinct cytokine and immune cell infiltration responses. In particular, although infection with C. rodentium \\u0394map/\\u0394espF failed to induce secretion of interleukin-22 (IL-22), CR14 and CRi9 triggered heightened secretion of IL-6 and granulocyte-macrophage colony-stimulating factor (GM-CSF) and of IL-22, interferon-\\u03b3 (IFN-\\u03b3), and IL-17 from colonic explants, respectively. Nonetheless, infection with CR14 or CRi9 induced protective immunity against secondary infections. Although Tir, EspZ, and NleA are essential, other effectors exhibit context-dependent essentiality in vivo. Moreover, C. rodentium expressing the effector repertoire of EPEC E2348/69 failed to efficiently colonize mice. We used curated functional information and our in vivo data to train a machine learning model that predicted values for colonization efficiency of previously uncharacterized mutant combinations. Notably, a mutant with a low predicted value, lacking only nleF, nleG8, nleG1, nleB, and espL, failed to colonize. CONCLUSION Our analysis revealed that T3SS effectors form robust networks, which can sustain substantial contractions while maintaining virulence, and that the composition of the effector network contributes to host adaptation. Alternative effector networks within a single pathogen triggered markedly different immune responses yet induced protective immunity. CR14 did not tolerate any further contraction, which suggests that this network reached its robustness limit with only 12 effectors. As the robustness limits of other effector networks depend on the contraction starting point and the order of the deletions, machine learning models could transform our ability to predict alternative network functions. Together, this study demonstrates the robustness of T3SS effector networks and the ability of IECs to withstand drastic perturbations while maintaining antibacterial functions. T3SS effectors form robust intracellular networks. T3SS effector networks can sustain substantial contractions while maintaining virulence. Using C. rodentium as a model showed that although triggering the conserved infection signatures in IECs, distinct networks induce divergent immune responses and affect host adaption. Because the robustness limit depends on the contraction sequence, machine learning models could transform our ability to predict the virulence potential of alternative networks. Infections with many Gram-negative pathogens, including Escherichia coli, Salmonella, Shigella, and Yersinia, rely on type III secretion system (T3SS) effectors. We hypothesized that while hijacking processes within mammalian cells, the effectors operate as a robust network that can tolerate substantial contractions. This was tested in vivo using the mouse pathogen Citrobacter rodentium (encoding 31 effectors). Sequential gene deletions showed that effector essentiality for infection was context dependent and that the network could tolerate 60% contraction while maintaining pathogenicity. Despite inducing very different colonic cytokine profiles (e.g., interleukin-22, interleukin-17, interferon-\\u03b3, or granulocyte-macrophage colony-stimulating factor), different networks induced protective immunity. Using data from >100 distinct mutant combinations, we built and trained a machine learning model able to predict colonization outcomes, which were confirmed experimentally. Furthermore, reproducing the human-restricted enteropathogenic E. coli effector repertoire in C. rodentium was not sufficient for efficient colonization, which implicates effector networks in host adaptation. These results unveil the extreme robustness of both T3SS effector networks and host responses.\"}, {\"paperId\": \"a8f59ac02f845e7932528a28d013cbf25bdcf324\", \"abstract\": null}, {\"paperId\": \"9cd5d98b7d3fa51e1bc7ea3c195798cbf21dc464\", \"abstract\": \"The near-term artificial intelligence, commonly referred as \\u2018weak AI\\u2019 in the last couple years was achieved thanks to the advances in machine learning (ML), particularly deep learning, which has currently the best in-class performance outperforming other machine learning algorithms. In the deep learning framework, many natural tasks such as object, image, and speech recognition that were impossible to be performed by classical ML algorithms in the previous decades can now be be done by typical home personal computer. Deep learning requires large amount of data that has to be rapidly collected (also known as \\u2018big data\\u2019) in order to create robust model parameters that are able to predict future occurrences of certain event. In some domains, a large dataset such as CIFAR-10, MNIST, or Kaggle exist already. However, in many other domains such as aircraft visual inspection, such a large dataset is not easily available and this clearly restricts deep learning to perform well to recognize material damage in aircraft structures. As many computer science researchers believe, we also think that in order to achieve a performance similar to human-level intelligence, AI could and should not start from scratch. Introducing an inductive bias into deep learning might be one solution to achieve that humanlevel intelligence. In this paper, we give an example how to incorporate aerospace domain knowledge into the development of deep learning algorithms. We performed a relatively simple procedure: we conducted fatigue testing of an aluminum plate that is typically used in aircraft fuselage and build a deep convolutional neural network that classifies crack length according to crack propagation curve obtained from fatigue test. The results of this network are then compared to the results of the same network that was not injected by domain knowledge\"}, {\"paperId\": \"e89a5ca28ef35c43ee7760664fdafcad496fbd22\", \"abstract\": null}, {\"paperId\": \"a9d2ef6a558eb60e8c61abf25283359c7b3aee90\", \"abstract\": null}, {\"paperId\": \"577abc9c2a5bd34f4905f1bac4c78d2bc87105b1\", \"abstract\": \"Background Tremendous opportunities for health research have been unlocked by the recent expansion of big data and artificial intelligence. However, this is an emergent area where recommendations for optimal use and implementation are needed. The objective of these European League Against Rheumatism (EULAR) points to consider is to guide the collection, analysis and use of big data in rheumatic and musculoskeletal disorders (RMDs). Methods A multidisciplinary task force of 14 international experts was assembled with expertise from a range of disciplines including computer science and artificial intelligence. Based on a literature review of the current status of big data in RMDs and in other fields of medicine, points to consider were formulated. Levels of evidence and strengths of recommendations were allocated and mean levels of agreement of the task force members were calculated. Results Three overarching principles and 10 points to consider were formulated. The overarching principles address ethical and general principles for dealing with big data in RMDs. The points to consider cover aspects of data sources and data collection, privacy by design, data platforms, data sharing and data analyses, in particular through artificial intelligence and machine learning. Furthermore, the points to consider state that big data is a moving field in need of adequate reporting of methods and benchmarking, careful data interpretation and implementation in clinical practice. Conclusion These EULAR points to consider discuss essential issues and provide a framework for the use of big data in RMDs.\"}, {\"paperId\": \"958b81374543a6ecb0c47e0810b8e140da7698b8\", \"abstract\": \"The ability to understand the complexity of cancer-related data has been prompted by the applications of (1) computer and data sciences, including data mining, predictive analytics, machine learning, and artificial intelligence, and (2) advances in imaging technology and probe development. Computational modelling and simulation are systematic and cost-effective tools able to identify important temporal/spatial patterns (and relationships), characterize distinct molecular features of cancer states, and address other relevant aspects, including tumor detection and heterogeneity, progression and metastasis, and drug resistance. These approaches have provided invaluable insights for improving the experimental design of therapeutic delivery systems and for increasing the translational value of the results obtained from early and preclinical studies. The big question is: Could cancer theranostics be determined and controlled in silico? This review describes the recent progress in the development of computational models and methods used to facilitate research on the molecular basis of cancer and on the respective diagnosis and optimized treatment, with particular emphasis on the design and optimization of theranostic systems. The current role of computational approaches is providing innovative, incremental, and complementary data-driven solutions for the prediction, simplification, and characterization of cancer and intrinsic mechanisms, and to promote new data-intensive, accurate diagnostics and therapeutics.\"}, {\"paperId\": \"f03c7a6194578218f7851f1990c6d3afaa9118c1\", \"abstract\": \"Artificial intelligence (AI) and machine learning (ML) have caused a paradigm shift in healthcare that can be used for decision support and forecasting by exploring medical data. Recent studies have shown that AI and ML can be used to fight COVID-19. The objective of this article is to summarize the recent AI- and ML-based studies that have addressed the pandemic. From an initial set of 634 articles, a total of 49 articles were finally selected through an inclusion-exclusion process. In this article, we have explored the objectives of the existing studies (i.e., the role of AI/ML in fighting the COVID-19 pandemic); the context of the studies (i.e., whether it was focused on a specific country-context or with a global perspective; the type and volume of the dataset; and the methodology, algorithms, and techniques adopted in the prediction or diagnosis processes). We have mapped the algorithms and techniques with the data type by highlighting their prediction/classification accuracy. From our analysis, we categorized the objectives of the studies into four groups: disease detection, epidemic forecasting, sustainable development, and disease diagnosis. We observed that most of these studies used deep learning algorithms on image-data, more specifically on chest X-rays and CT scans. We have identified six future research opportunities that we have summarized in this paper. Impact Statement: Artificial intelligence (AI) and machine learning(ML) methods have been widely used to assist in the fight against COVID-19 pandemic. A very few in-depth literature reviews have been conducted to synthesize the knowledge and identify future research agenda including a previously published review on data science for COVID-19 in this article. In this article, we synthesized reviewed recent literature that focuses on the usages and applications of AI and ML to fight against COVID-19. We have identified seven future research directions that would guide researchers to conduct future research. The most significant of these are: develop new treatment options, explore the contextual effect and variation in research outcomes, support the health care workforce, and explore the effect and variation in research outcomes based on different types of data.\"}, {\"paperId\": \"792f93c43a104e1a6deaaf51710a66a8c7cd085c\", \"abstract\": \"The Factory of the Future vision focuses on new generation of enterprises that will exploit latest breakthroughs in data sciences and in Internet-connected devices to effectively address the ever increasing pace at which data and goods are nowadays traded. Among the many challenges emerging in this domain, particularly important is the ability to enhance the connection between the value-production chains of factories and the outer network of supply partners, and service providers, that enable enterprise business. In this paper, the authors highlight how the critical issue of connecting inner and outer value chains in factories can be effectively tackled through state-of-the-art IT solutions based on Machine Learning, Artificial Intelligence and Internet of Things. In particular, the approach to these challenges pursued in the COMPOSITION project is depicted and envisioned solutions are discussed.\"}, {\"paperId\": \"53e9d718ec981850cfc6110385ac42ca2da2f612\", \"abstract\": \"Applications of Machine LearningNeural Networks with Keras CookbookHuman Activity Recognition ChallengeICDSMLA 2020Intelligent SystemsArtificial Intelligence for Autonomous NetworksMachine Learning Approaches to Human Movement AnalysisStrategic System Assurance and Business AnalyticsIntelligent Data Engineering and Automated Learning \\u2012 IDEAL 2018Software Technology: Methods and ToolsHandbook of Computer Networks and Cyber SecurityDeep Learning Applications of Short-Range RadarsIntelligent Systems Design and ApplicationsMachine Learning and Artificial IntelligenceDeep Learning Classifiers with Memristive NetworksArtificial Intelligence and Machine Learning in Business ManagementSignal Processing in Medicine and BiologyAdvances in Intelligent Systems Research and InnovationCommunication, Smart Technologies and Innovation for SocietyMachine Intelligence and Signal ProcessingDatabase Systems for Advanced ApplicationsSpeech and ComputerProceedings of International Conference on Communication, Circuits, and SystemsAdvanced Computing and Intelligent EngineeringMachine Learning and Metaheuristics Algorithms, and ApplicationsFundamentals of Deep Learning and Computer VisionProceedings of International Conference on Innovations in Software Architecture and Computational SystemsMediterranean Forum \\u2012 Data Science ConferenceAdvances in Visual InformaticsDeep Learning Applications for Cyber SecurityFundamentals of Deep Learning and Computer VisionNeural Networks with RAdvances in Computational IntelligenceAdvances in Decision Sciences, Image Processing, Security and Computer VisionFuzzy Machine Learning Algorithms for Remote Sensing Image ClassificationFuturistic Trends in Networks and Computing TechnologiesProceedings of the International Conference on Big Data, IoT, and Machine LearningSimulation and Analysis of Mathematical Methods in Real-Time Engineering ApplicationsDeep Learning Techniques for Biomedical and Health InformaticsDeployable Machine Learning for Security Defense\"}, {\"paperId\": \"785cf19124280da2fc2b8c2617f0fea8716e5b72\", \"abstract\": \"Series Preface. Foreword. Prologue. Creativity and Computers M. Boden. Part I: Foundational Issues. Introduction: On Having a Mind of Your Own T. Dartnall. Creativity, Thought and Representational Redescription T. Dartnall. Connectionism and Cognitive Flexibility A. Clark. Re-representation and Emergent Information in Three Cases of Problem Solving D. Peterson. Psychological Issues in Modelling Creativity R. Wales, S. Thornton. Godel's Theorem and Creativity G. Priest. Machine Predictability Versus Human Creativity R. McDonough. Part 2: Creativity and Cognition. Introduction: Creativity and Cognition G.S. Halford, R. Levinson. Tensor Models: A Creative Basis for Memory Retrieval and Analogical Mapping J. Wiles, G.S. Halford, J.E.M. Stewart, M.S. Humphreys, J.D. Bain, W.H. Wilson. Experience-Based Creativity R. Levinson. Creative Proof Planning T. Walsh. Clues to Creativity M. Harney. Part 3: Creativity and Connectionism. Introduction: Creativity, Connectionism and Guided Walks C. Thornton. Creativity, Chaos and Artificial Intelligence A. Dekker, P. Farrow. The Evolution of Connectionist Networks Xin Yao. Why Connectionist Learning Algorithms Need to be More Creative C. Thornton. Part 4: Creativity and Design. Introduction: Creativity and Design J.S. Gero. Computational Models of Creative Design Processes J.S. Gero. A Model of Creative Design Using a Genetic Metaphor L. Alem, M.L. Maher. Lateral Translation in Design G. Schmitt, Shen-Guan Shih. Creativity, Invention and the Computational Metaphor: Prolegomenon to a Case Study S. Dasgupta. Part 5: HumanCreativity Enhancement. Introduction: Computer-Based Systems that Support Creativity E. Edmonds. Cybernetic Serendipity Revisited E. Edmonds. Amplifying Designers' Creativity with Domain-Oriented Design Environments G. Fischer, K. Nakakoji. Creativity in Social Sciences: the Computer Enhancement of Qualitative Data Analysis T. Richards, L. Richards. Cognitive Support and the Rhythm of Design M. Sharples. Epilogue. How Could a Copycat ever be Creative? D. Hofstadter. Index of Names. Index of Subjects.\"}, {\"paperId\": \"03c331f7186aeb6c5c58c2f4911888bacfdb2840\", \"abstract\": null}, {\"paperId\": \"1aa6e2cb5f295c49ba7ef6654706bdf173efd4b5\", \"abstract\": \"This article investigates the impact of big data on the actuarial sector. The growing fields of applications of data analytics and data mining raise the ability for insurance companies to conduct more accurate policy pricing by incorporating a broader variety of data due to increased data availability. The analyzed areas of this paper span from automobile insurance policy pricing, mortality and healthcare modeling to estimation of harvest-, climate- and cyber risk as well as assessment of catastrophe risk such as storms, hurricanes, tornadoes, geomagnetic events, earthquakes, floods, and fires. We evaluate the current use of big data in these contexts and how the utilization of data analytics and data mining contribute to the prediction capabilities and accuracy of policy premium pricing of insurance companies. We find a high penetration of insurance policy pricing in almost all actuarial fields except in the modeling and pricing of cyber security risk due to lack of data in this area and prevailing data asymmetries, for which we identify the application of artificial intelligence, in particular machine learning techniques, as a possible solution to improve policy pricing accuracy and results.\"}, {\"paperId\": \"084dbe6a5a37820b9aeb63da012c7b98f58252c6\", \"abstract\": \"Importance\\nThe recent advances in the field of machine learning have raised expectations that computer-aided diagnosis will become the standard for the diagnosis of melanoma.\\n\\n\\nObjective\\nTo critically review the current literature and compare the diagnostic accuracy of computer-aided diagnosis with that of human experts.\\n\\n\\nData Sources\\nThe MEDLINE, arXiv, and PubMed Central databases were searched to identify eligible studies published between January 1, 2002, and December 31, 2018.\\n\\n\\nStudy Selection\\nStudies that reported on the accuracy of automated systems for melanoma were selected. Search terms included melanoma, diagnosis, detection, computer aided, and artificial intelligence.\\n\\n\\nData Extraction and Synthesis\\nEvaluation of the risk of bias was performed using the QUADAS-2 tool, and quality assessment was based on predefined criteria. Data were analyzed from February 1 to March 10, 2019.\\n\\n\\nMain Outcomes and Measures\\nSummary estimates of sensitivity and specificity and summary receiver operating characteristic curves were the primary outcomes.\\n\\n\\nResults\\nThe literature search yielded 1694 potentially eligible studies, of which 132 were included and 70 offered sufficient information for a quantitative analysis. Most studies came from the field of computer science. Prospective clinical studies were rare. Combining the results for automated systems gave a melanoma sensitivity of 0.74 (95% CI, 0.66-0.80) and a specificity of 0.84 (95% CI, 0.79-0.88). Sensitivity was lower in studies that used independent test sets than in those that did not (0.51; 95% CI, 0.34-0.69 vs 0.82; 95% CI, 0.77-0.86; P\\u2009<\\u2009.001); however, the specificity was similar (0.83; 95% CI, 0.71-0.91 vs 0.85; 95% CI, 0.80-0.88; P\\u2009=\\u2009.67). In comparison with dermatologists' diagnosis, computer-aided diagnosis showed similar sensitivities and a 10 percentage points lower specificity, but the difference was not statistically significant. Studies were heterogeneous and substantial risk of bias was found in all but 4 of the 70 studies included in the quantitative analysis.\\n\\n\\nConclusions and Relevance\\nAlthough the accuracy of computer-aided diagnosis for melanoma detection is comparable to that of experts, the real-world applicability of these systems is unknown and potentially limited owing to overfitting and the risk of bias of the studies at hand.\"}, {\"paperId\": \"7449a22f467cd8bb217719d8408c6724cda9fdcd\", \"abstract\": \"The explosion of advancements in artificial intelligence, sensor technologies, and wireless communication activates ubiquitous sensing through distributed sensors. These sensors are various domains of networks that lead us to smart systems in healthcare, transportation, environment, and other relevant branches/networks. Having collaborative interaction among the smart systems connects end-user devices to each other which enables achieving a new integrated entity called Smart Cities. The goal of this study is to provide a comprehensive survey of data analytics in smart cities. In this paper, we aim to focus on one of the smart cities important branches, namely Smart Mobility, and its positive ample impact on the smart cities decision-making process. Intelligent decision-making systems in smart mobility offer many advantages such as saving energy, relaying city traffic, and more importantly, reducing air pollution by offering real-time useful information and imperative knowledge. Making a decision in smart cities in time is challenging due to various and high dimensional factors and parameters, which are not frequently collected. In this paper, we first address current challenges in smart cities and provide an overview of potential solutions to these challenges. Then, we offer a framework of these solutions, called universal smart cities decision making, with three main sections of data capturing, data analysis, and decision making to optimize the smart mobility within smart cities. With this framework, we elaborate on fundamental concepts of big data, machine learning, and deep leaning algorithms that have been applied to smart cities and discuss the role of these algorithms in decision making for smart mobility in smart cities. keywords: Smart cities, Smart Mobility, Making Decision, Artificial Intelligence, Data Science, Machine Learning, Deep Learning.\"}, {\"paperId\": \"156510dd3d38813784f8b9cb6b380be899eec83a\", \"abstract\": \"THE ETHICAL ALGORITHM: The Science of Socially Aware Algorithm Design by Michael Kearns and Aaron Roth. New York: Oxford University Press, 2019. 232 pages. Hardcover; $24.95. ISBN: 9780190948207. *Can an algorithm be ethical? That question appears to be similar to asking if a hammer can be ethical. Isn't the ethics solely related to how the hammer is used? Using it to build a house seems ethical; using it to harm another person would be immoral. *That line of thinking would be appropriate if the algorithm were something as simple as a sorting routine. If we sort the list of names in a wedding guest book so that the thank-you cards can be sent more systematically, its use would be acceptable; sorting a list of email addresses by education level in order to target people with a scam would be immoral. *The algorithms under consideration in The Ethical Algorithm are of a different nature, and the ethical issues are more complex. These algorithms are of fairly recent origin. They arise as we try to make use of vast collections of data to make more-accurate decisions: for example, using income, credit history, current debt level, and education level to approve or disapprove a loan application. A second example would be the use of high school GPA, ACT or SAT scores, and extra-curricular activities to determine college admissions. *The algorithms under consideration use machine-learning techniques (a branch of artificial intelligence) to look at the success rates of past student admissions and instruct the machine-learning algorithm to determine a set of criteria that successfully distinguish (with minimal errors) between those past students who graduated and those who didn't. That set of criteria (called a \\\"model\\\") can then be used to predict the success of future applicants. *The ethical component is important because such machine-learning algorithms optimize with particular goals as targets. And there tend to be unintended consequences--such as higher rates of rejection of applicants of color who would actually have succeeded. The solution to this problem requires more than just adding social equity goals as part of what is to be optimized--although that is an important step. *The authors advocate the development of precise definitions of the social goals we seek, and then the development of algorithmic techniques that help produce those goals. One important example is the social goal of privacy. What follows leaves out many important ideas found in the book, but illustrates the key points. Kearns and Roth cite the release in the mid-1990s of a dataset containing medical records for all state employees of Massachusetts. The dataset was intended for the use of medical researchers. The governor assured the employees that identifying information had been removed--names, social security numbers, and addresses. Two weeks later, Latanya Sweeney, a PhD student at MIT, sent the governor his medical records from that dataset. It cost her $20 to legally purchase the voter rolls for the city of Cambridge, MA. She then correlated that with other publicly available information to eliminate every other person from the medical dataset other than the governor himself. *Achieving data privacy is not as simple as was originally thought. To make progress, a good definition of privacy is needed. One useful definition is the notion of differential privacy: \\\"nothing about an individual should be learnable from a dataset that cannot be learned from the same dataset but with the individual's data removed\\\" (p. 36). This needs to also prevent identification by merging multiple datasets (for example, the medical records from several hospitals from which we might be able to identify an individual by looking for intersections on a few key attributes such as age, gender, and illness). One way to achieve this goal is to add randomness to the data. This can be done in a manner in which the probability of determining an individual changes very little by adding or removing that person's data to/from the dataset. *A very clever technique for adding this random noise can be found in a randomized response, an idea introduced in the 1960s to get accurate information in polls about sensitive topics (such as, \\\"have you cheated on your taxes?\\\"). The respondent is told to flip a coin. If it is a head, answer truthfully. If it is a tail, flip a second time and answer \\\"yes\\\" if it is a head and \\\"no\\\" if it is a tail. Suppose the true proportion of people who cheat on their taxes is p. Some pretty simple math shows that with a sufficiently large sample size (larger than needed for surveys that are less sensitive), the measured proportion, m, of \\\"yes\\\" responses will be close to m = \\u00bc + \\u00bd p. We can then approximate p as 2m - \\u00bd, and still give individuals reasonable deniability. If I answer \\\"yes\\\" and a hacker finds my record, there is still a 25% chance that my true answer is \\\"no.\\\" My privacy has been effectively protected. So we can achieve reasonable privacy at the cost of needing a larger dataset. *This short book discusses privacy, fairness, multiplayer games (such as using apps to direct your morning commute), pitfalls in scientific research, accountability, the singularity (a future time where machines might become \\\"smarter\\\" than humans), and more. Sufficient detail is given so that the reader can understand the ideas and the fundamental aspects of the algorithms without requiring a degree in mathematics or computer science. *One of the fundamental issues driving the need for ethical algorithms is the unintended consequences that result from well-intended choices. This is not a new phenomenon--Lot made a choice based on the data he had available: \\\"Lot looked about him, and saw that the plain of the Jordan was well watered everywhere like the garden of the Lord, like the land of Egypt ...\\\" Genesis 13:10 (NRSV). But by choosing that apparently desirable location, Lot brought harm to his family. *I have often pondered the command of Jesus in Matthew 10:16 where he instructs us to \\\"be wise as serpents and innocent as doves.\\\" Perhaps one way to apply this command is to be wise as we are devising algorithms to make sure that they do no harm. We should be willing to give up some efficiency in order to achieve more equitable results. *Reviewed by Eric Gossett, Department of Mathematics and Computer Science, Bethel University, St. Paul, MN 55112.\"}, {\"paperId\": \"91befa9bb10685d87ff0154516a594e35a55676c\", \"abstract\": \"he conference attracted 320 attendees from over 90 different academic, industrial, and government institutions. Of the 150 papers submitted, 49 were accepted for publication in the conference proceedings (available from Morgan Kaufmann). Of the 49 papers, 20 were presented in three days of plenary sessions during the conference, with the remainder presented at a poster session. Three invited talks were included that reviewed important subfields of machine learning: genetic algorithms, connectionist learning, and formal models of learning. The conference also featured discussion sessions on topics of particular interest to subgroups of the attendees. The discussion topics covered empirical approaches to learning, the sharing of machine-learning data and programs, explanation-based learning, and genetic algorithms. In addition, two receptions were held to provide further opportunity for interaction among conference attendees. The conference was supported by registration fees and grants from the Office of Naval Research (ONR) Computer Sciences Division, the ONR Cognitive Science Program, and the American Association for Artificial Intelligence.\"}, {\"paperId\": \"0ee3803919fe314e0e2542a4427bfe305332a061\", \"abstract\": \"COMPUTATIONAL ENGINEERING AND DESIGN CENTER SCHOOL OF ENGINEERING SCIENCES Doctor of Philosophy Artificial Intelligence Technologies in Complex Engineering Design by Yew Soon Ong Engineering design optimization is an emerging technology whose application both tends to shorten design-cycle time and finds new designs that are not only feasible, but also nearer to optimum, based on specified design criteria. Its gain in attention in the field of complex designs is fuelled by advancing computing power now allowing increasingly accurate analysis codes to be deployed. Unfortunately, the optimization of complex engineering design problems remains a difficult task, due to the complexity of the cost surfaces and the human expertise necessary in order to achieve high quality results. This research is concerned with the effective use of past experiences and chronicled data from previous designs to mitigate some of the limitations of present engineering design optimization process. In particular, the present work leverages well established artificial intelligence technologies and extends recent theoretical and empirical advances, particularly in machine learning, adaptive hybrid evolutionary computation, surrogate modeling, radial basis functions and transductive inference, to mitigate the issues of i) choice of optimization methods and ii) dealing with expensive design problems. The resulting approaches are studied using commonly employed benchmark functions. Further demonstrations on realistic aerodynamic aircraft and ship design problems reveal that the proposed techniques not only generate robust design performance, they can also greatly decrease the cost of design space search and arrive at better designs as compared to conventional approaches.\"}, {\"paperId\": \"14a71bc3850a5f206ff67553f7469ffd29e7070b\", \"abstract\": null}, {\"paperId\": \"3cf7c86124c125a66380afe343706a57d7c740d7\", \"abstract\": \"Huge efforts are being made by computer scientists and statisticians to design and implement algorithms and techniques for efficient storage, management, processing, and analysis of biological database. Data mining is an emerging area of computational intelligence that offers new theories, techniques and tools for processing large volumes of data (Sriraam, Natasha & Kaur, Data mining approaches for kidney dialysis treatment, 2006). The data mining and statistical learning techniques were used to discover consistent and useful patterns in large datasets. These techniques are used in a computational biology and bioinformatics fields. Computational biology and bioinformatics seeks to solve biological problems by combining aspects of biology, computer science, mathematics, and other disciplines (Adams, Matheson & Pruim, BLASTED: Integrating biology and computation, 2008). The main focus of this study is to expand understanding of how biologists, medical practitioners and scientists would benefit from data mining and statistical learning techniques in prediction of breast cancer survivability and prognosis using R statistical computing tool and Weka machine learning tool. In this dissertation, data mining and statistical learning techniques were applied to breast cancer datasets for survival analysis. The breast cancer dataset from University of California, Irvine (UCI) machine learning database system and National Cancer Institute (NCI) biological database system were used for prediction and comparative study of the data mining and statistical learning techniques. The results of the classifiers or models were mixed, logistic regression did outperform decision tree, SVM, AdaBoost, Bagging and naive Bayes algorithms based on accuracy. However, the artificial neural network showed slight improvement over logistic regression, and the decision tree resulted in slightly higher classification accuracy over AdaBoost, Bagging and naive Bayes' models in terms of accuracy. Results were mixed as to which algorithm is the most optimal model, and it appeared that the performance of each algorithm depends on the size, high dimensionality of data representation and cleanliness of the dataset.\"}, {\"paperId\": \"e355d0a9bc338660c1de467329707224f5a26065\", \"abstract\": \"Detecting quality in large unstructured datasets requires capacities far beyond the limits of human perception and communicability and, as a result, there is an emerging trend towards increasingly complex analytic solutions in data science to cope with this problem. This new trend towards analytic complexity represents a severe challenge for the principle of parsimony (Occam\\u2019s razor) in science. This review article combines insight from various domains such as physics, computational science, data engineering, and cognitive science to review the specific properties of big data. Problems for detecting data quality without losing the principle of parsimony are then highlighted on the basis of specific examples. Computational building block approaches for data clustering can help to deal with large unstructured datasets in minimized computation time, and meaning can be extracted rapidly from large sets of unstructured image or video data parsimoniously through relatively simple unsupervised machine learning algorithms. Why we still massively lack in expertise for exploiting big data wisely to extract relevant information for specific tasks, recognize patterns and generate new information, or simply store and further process large amounts of sensor data is then reviewed, and examples illustrating why we need subjective views and pragmatic methods to analyze big data contents are brought forward. The review concludes on how cultural differences between East and West are likely to affect the course of big data analytics, and the development of increasingly autonomous artificial intelligence (AI) aimed at coping with the big data deluge in the near future.\"}, {\"paperId\": \"5bcd8c5def120c19d0ef8eb04731582e4adcb9bc\", \"abstract\": \"In recent years emotion detection in text has become more popular due to its potential applications in fields such as psychology, marketing, political science, and artificial intelligence, among others. While opinion mining is a well-established task with many standard data sets and well-defined methodologies, emotion mining has received less attention due to its complexity. In particular, the annotated gold standard resources available are not enough. In order to address this shortage, we present a multilingual emotion data set based on different events that took place in April 2019. We collected tweets from the Twitter platform. Then one of seven emotions, six Ekman\\u2019s basic emotions plus the \\u201cneutral or other emotions\\u201d, was labeled on each tweet by 3 Amazon MTurkers. A total of 8,409 in Spanish and 7,303 in English were labeled. In addition, each tweet was also labeled as offensive or no offensive. We report some linguistic statistics about the data set in order to observe the difference between English and Spanish speakers when they express emotions related to the same events. Moreover, in order to validate the effectiveness of the data set, we also propose a machine learning approach for automatically detecting emotions in tweets for both languages, English and Spanish.\"}, {\"paperId\": \"dd75e7fb1378aa2706a6304f40177ee8cef01704\", \"abstract\": null}, {\"paperId\": \"9498406ce1b4de9dfff86ceb92dd6ef68e062d5e\", \"abstract\": \"Artificial Cognitive Architecture with Self-Learning and Self-Optimization CapabilitiesPattern Recognition and Classification in Time Series DataComputational Intelligence Techniques and Their Applications to Software Engineering ProblemsEvolutionary Computation with Intelligent SystemsArtificial Intelligence for Smart and Sustainable Energy Systems and ApplicationsOptimizing Medicine Residency Training ProgramsData Mining: Concepts, Methodologies, Tools, and ApplicationsApplied Big Data Analytics in Operations ManagementOptimizing Decision Making in the Apparel Supply Chain Using Artificial Intelligence (AI)Multidisciplinary Perspectives on Telecommunications, Wireless Systems, and Mobile ComputingHybrid Computational IntelligenceInnovative Collaborative Practice and Reflection in Patient EducationHealth Care Delivery and Clinical Science: Concepts, Methodologies, Tools, and ApplicationsBig Data Analytics in HIV/AIDS ResearchOn Unified Computational Intelligence: Neural Networks, Dynamic Programming, and Applications to Economics and FinanceEmerging Theory and Practice in NeuroprostheticsApplications of Computational Intelligence in Data-Driven TradingArtificial Intelligence in EducationHandbook of Research on Demand-Driven Web Services: Theory, Technologies, and ApplicationsSecurity, Trust, and Regulatory Aspects of Cloud Computing in Business EnvironmentsMultidisciplinary Computational Intelligence Techniques: Applications in Business, Engineering, and MedicineApplied Computational Intelligence and Soft Computing in EngineeringAdvances in Machine Learning and Computational IntelligenceMultidisciplinary Computational AnatomyEncyclopedia of Business Analytics and OptimizationEncyclopedia of Information Science and Technology, Third EditionMedical Diagnosis Using Artificial Neural NetworksIncorporating Nature-Inspired Paradigms in Computational ApplicationsOptimizing Contemporary Application and Processes in Open Source SoftwareCross-Cultural Training and Teamwork in HealthcareBiologically Rationalized Computing Techniques For Image Processing ApplicationsFlipping Health Care through Retail Clinics and Convenient Care ModelsComputational Intelligence in Aerospace SciencesMathematical Modeling, Computational Intelligence Techniques and Renewable EnergyApplications of Computational Intelligence in Multi-Disciplinary ResearchAdvances in Computational Intelligence TechniquesImpact of Smart Technologies and Artificial Intelligence (AI) Paving Path Towards Interdisciplinary Research in the Fields of Engineering, Arts, Humanities, Commerce, Economics, Social Sciences, Law and Management Challenges and OpportunitiesAdvanced Research on Biologically Inspired Cognitive ArchitecturesFinite Element Model Updating Using Computational Intelligence TechniquesMultidisciplinary Functions of Blockchain Technology in AI and IoT Applications This book gathers selected high-quality papers presented at the International Conference on Machine Learning and Computational Intelligence (ICMLCI-2019), jointly organized by Kunming University of Science and Technology and the Interscience Research Network, Bhubaneswar, India, from April 6 to 7, 2019. Addressing virtually all aspects of intelligent systems, soft computing and machine learning, the topics covered include: prediction; data mining; information retrieval; game playing; robotics; learning methods; pattern visualization; automated knowledge acquisition; fuzzy, stochastic and probabilistic computing; neural computing; big data; social networks and applications of soft computing in various areas.As is true of most technological fields, the software industry is constantly advancing and becoming more accessible to a wider range of people. The advancement and accessibility of these systems creates a need for understanding and research into their development. Optimizing Contemporary Application and Processes in Open Source Software is a critical scholarly resource that examines the prevalence of open source software systems as well as the advancement and development of these systems. Featuring coverage on a wide range of topics such as machine learning, empirical software engineering and management, and open source, this book is geared toward academicians, practitioners, and researchers seeking current and relevant research on the advancement and prevalence of open source software systems.Many techniques have been developed to control the variety of dynamic systems. To develop those control techniques, it is fundamental to know the mathematical relations between the system inputs and outputs. Incorporating Nature-Inspired Paradigms in Computational Applications is a critical scholarly resource that examines the application of nature-inspired paradigms on system identification. Featuring coverage on a broad range of topics such as biogeographic computation, evolutionary control systems, and natural computing, this book is geared towards IT professionals, engineers, computer scientists, academicians, researchers, and graduate-level students seeking current research on the application of nature-inspired paradigms.This book is intended for practitioners seeking an overview of different computational intelligence techniques with aerospace applications, and for newcomers looking for fundamental information with advanced examples. It provides a look into the world of computational intelligence, detailing techniques across four main areas of aerospace sciences: robotics, multidisciplinary design, aerodynamics, and space.Practitioners in apparel manufacturing and retailing enterprises in the fashion industry, ranging from senior to front line management, constantly face complex and critical decisions. There has been growing interest in the use of artificial intelligence (AI) techniques to enhance this process, and a number of AI techniques have already been successfully applied to apparel production and retailing. Optimizing decision making in the apparel supply chain using artificial intelligence (AI): From production to retail provides detailed coverage of these techniques, outlining how they are used to assist decision makers in tackling key supply chain problems. Key decision points in the apparel supply chain and the fundamentals of artificial intelligence techniques are the focus of the opening chapters, before the book proceeds to discuss the use of neural networks, genetic algorithms, fuzzy set theory and extreme learning machines for intelligent sales forecasting and intelligent product cross-selling systems. Helps the reader gain an understanding of the key decision points in the apparel supply chain Discusses the fundamentals of artificial intelligence techniques for apparel management techniques Considers the use of neural networks in selecting the location of apparel manufacturing\"}, {\"paperId\": \"ec1b20dde1199d5b74506cd652c5046594776ab3\", \"abstract\": \"Field Programmable Gate Array (FPGA) is a general purpose programmable logic device that can be configured by a customer after manufacturing to perform from a simple logic gate operations to complex systems on chip or even artificial intelligence systems. Scientific publications related to FPGA started in 1992 and, up to now, we found more than 70,000 documents in the two leading scientific databases (Scopus and Clarivative Web of Science). These publications show the vast range of applications based on FPGAs, from the new mechanism that enables the magnetic suspension system for the kilogram redefinition, to the Mars rovers\\u2019 navigation systems. This paper reviews the top FPGAs\\u2019 applications by a scientometric analysis in ScientoPy, covering publications related to FPGAs from 1992 to 2018. Here we found the top 150 applications that we divided into the following categories: digital control, communication interfaces, networking, computer security, cryptography techniques, machine learning, digital signal processing, image and video processing, big data, computer algorithms and other applications. Also, we present an evolution and trend analysis of the related applications.\"}, {\"paperId\": \"6c4636fe14779555e0b4e499de18d930bd8fcbbe\", \"abstract\": \"Over the last decade, for the first time, substantial efforts have been directed at the development of dedicated in silico platforms for drug repurposing, including initiatives targeting cancers and conditions as diverse as cryptosporidiosis, dengue, dental caries, diabetes, herpes, lupus, malaria, tuberculosis and Covid-19 related respiratory disease. This review outlines some of the exciting advances in the specific applications of in silico approaches to the challenge of drug repurposing and focuses particularly on where these efforts have resulted in the development of generic platform technologies of broad value to researchers involved in programmatic drug repurposing work. Recent advances in molecular docking methodologies and validation approaches, and their combination with machine learning or deep learning approaches are continually enhancing the precision of repurposing efforts. The meaningful integration of better understanding of molecular mechanisms with molecular pathway data and knowledge of disease networks is widening the scope for discovery of repurposing opportunities. The power of Artificial Intelligence is being gainfully exploited to advance progress in an integrated science that extends from the sub-atomic to the whole system level. There are many promising emerging developments but there are remaining challenges to be overcome in the successful integration of the new advances in useful platforms. In conclusion, the essential component requirements for development of powerful and well optimised drug repurposing screening platforms are discussed.\"}, {\"paperId\": \"429099d344334e6020a7cb349e5a7c4de5c1a76e\", \"abstract\": \"Summary form only given. Intelligence science is a cross-discipline that dedicates to joint research on basic theory and technology of intelligence by brain science, cognitive science, artificial intelligence and others. Brain science explores the essence of brain, research on the principle and model of natural intelligence in molecular, cell and behavior level. Cognitive science studies human mental activity, such as perception, learning, memory, thinking, consciousness etc. In order to implement machine intelligence, Artificial intelligence attempts simulation, extension and expansion of human intelligence using artificial methodology and technology. The above three disciplines work together to explore new concept, new theory, new methodology. It will be successful and create a brilliant future in the 21 century. Brain science points out that perceptive lobes have special function separately, the occipital lobe processes the visual information, the temporal lobe processes auditory information, the parietal lobe processes the information from the somatic sensors. All of three lobes deal with information perceived from the physical world. Each lobe is covered with cortex where the bodies of neurons are located. Cortex consists of primary, intermediate and advanced areas at least. Information is processed in the primary area first, then is passed to intermediate and advanced areas. Comparing with computer system, the brain is the same as hardware and the mind looks like software. Most work in cognitive science assumes that the mind has mental representations analogous to computer data structures, and computational procedures similar to computational algorithms. Connectionists have proposed novel ideas to use neurons and their connections as inspirations for data structures, and neuron firing and spreading activation as inspirations for algorithms. Cognitive science then works with a complex 3-way analogy among the mind, the brain, and computers. Mind, brain, and computation can each be used to suggest new ideas about the others. There is no single computational model of mind, since different kinds of computers and programming approaches suggest different ways in which the mind might work. The mind contains perception, rational, consciousness and emotion. The long-term scientific goal of artificial intelligence is human-level intelligence. In this lecture, we will discuss basic research topics related to intelligence science, such as learning, memory, thought, language, consciousness etc. We also report the recent progresses containing: visual perception; introspective learning; linguistic cognition; consciousness model; and platform of agent-grid intelligence\"}, {\"paperId\": \"573e8ff895c5771bc348265872ebf351e18109a1\", \"abstract\": null}]}\n",
            "\n",
            "{\"total\": 102806, \"offset\": 900, \"next\": 1000, \"data\": [{\"paperId\": \"e1a1df41edcd3541002e388ff40c261ba72fa118\", \"abstract\": \"Increasingly, the life sciences rely on data science, an emerging discipline in which visualization plays a critical role. Visualization is particularly important with challenging data from cutting-edge experimental techniques, such as 3D genomics, spatial transcriptomics, 3D proteomics, epiproteomics, high-throughput imaging, and metagenomics. Data visualization also plays an increasing role in how research is communicated. Some scientists still think of data visualization as optional; however, as more realize it is an essential tool for revealing insights buried in complex data, bioinformatics visualization is emerging as a subdiscipline. This article outlines current and future grand challenges in bioinformatics data visualization, and announces the first publication venue dedicated to this subdiscipline. Over the past two decades, life science data have increased rapidly in volume and complexity, with the result that data analysis is often the major bottleneck (O\\u2019Donoghue et al., 2010a). For example, \\u201cAll major genomics breakthroughs so far have been accompanied by the development of groundbreaking statistical and computational methods\\u201d (Green et al., 2020). Thus, in the remaining decades of the 21st century, life scientists will become increasingly reliant on the emerging tools and methods of data science (Blei and Smyth, 2017; Altman and Levitt, 2018). One of these methods is data visualization (a.k.a. DataVis), which plays a critical role in transforming data and analysis outcomes into insight (Card et al., 1999). Data visualization involves analysis, design, and rendering, as well as observation and cognitive processing (Figure 1). Some scientists think of DataVis as an optional step mostly aimed at aesthetics \\u2014 however, there is growing recognition that it is an essential tool in the analysis of complex data; two indicators of this recognition are the recent sales of DataVis companies Looker and Tableau for US$3B and $16B, respectively. Currently, however, most attention is focused on another aspect of data science, namely, the use of machine learning to develop artificial intelligence systems. Such systems have recently led to exciting advances in the life sciences (e.g., Callaway, 2020a)\\u2014 but also to some hyperbole. Clearly, machine learning methods are increasingly critical for research; but these methods also have limitations (Challen et al., 2019; Heaven, 2019; Yu and Kohane, 2019). More fundamentally, automated methods are insufficient, since analysis outcomes must be observed and understood by an analyst before insight can occur (Figure 1). Most analysts use data visualization as an integral part of their cognitive processes\\u2014especially important is manual validation, which involves checking for errors and outliers in raw data, and for wrong assumptions used in automated analysis methods (Anscombe, 1973). Automated data analysis (including machine learning) and data visualization are just components of the larger goal of data science, which the eminent computer scientist Fred Brooks argues should focus on \\u2018Intelligence Amplification\\u2019 (a.k.a. I.A.) \\u2014 i.e., on amplifying our abilities to manage more Edited by: Barbora Kozlikova, Masaryk University, Czechia\"}, {\"paperId\": \"20a8aa0295c43b5f553155b470c643d1708a7eaf\", \"abstract\": null}, {\"paperId\": \"6be0cf52f2081f5d0f6b6e241e4e171d19d96b82\", \"abstract\": \"The Robotic Process Automation (RPA) is a new wave of future technologies. Robotic Process Automation is one of the most advanced technologies in the area of computers science, electronic and communications, mechanical engineering, and information technology. It is a combination of both hardware and software, networking and automation for doing things very simple. In this light, the research manuscript investigated the secondary data - which is available on google, academic and research databases. The investigation went for totally 6 months, i.e., 1-1-2018 to 30-6-2018. A very few empirical articles, white papers, blogs and were found RPA and came across to compose this research manuscript. This study is exploratory in nature because of the contemporary phenomenon. The keywords used in searching of the database were Robotic Process Automation, RPA, Robots, Artificial Intelligence, Blue Prism. The study finally discovered that Robots and Robotic Process Automation technologies are becoming compulsory as a part to do business operations in organizations across the globe. Robotic Process Automation can bring immediate value to the core business processes including employee payroll, employee status changes, new hire recruitment, and onboarding, accounts receivable and payable, invoice processing, inventory management, report creation, software installations, data migration, and vendor onboarding etc. to name a few applications. Besides, the Robotic Process Automation has abundant applications including healthcare and pharmaceuticals, financial services, outsourcing, retail, telecom, energy and utilities, real estate and FMCG and many more sectors. To put in the right place of RPA in business operations, their many allied technologies are working at the background level, artificial intelligence, machine learning, deep learning, data analytics, HR analytics, virtual reality (second life), home automation, blockchain technologies, 4D printing etc. Moreover, it covers the content of different start-ups companies and existing companies - their RPA applications used across the world. This manuscript will be a good guideline for the academicians, researchers, students, and practitioners to get an overall idea.\"}, {\"paperId\": \"8d8d905742f8cf30019482accf9c4de3d2cd65fa\", \"abstract\": \"Graph matching is a fundamental problem in theoretical computer science and artificial intelligence, and lays the foundation for many computer vision and machine learning tasks. Approximate algorithms are necessary for graph matching due to its NP-complete nature. Inspired by the usage in network-related tasks, random walk is generalized to graph matching as a type of approximate algorithm. However, it may be inappropriate for the previous random walk-based graph matching algorithms to utilize continuous techniques without considering the discrete property. In this paper, we propose a novel random walk-based graph matching algorithm by incorporating both continuous and discrete constraints in the optimization process. Specifically, after interpreting graph matching by random walk, the continuous constraints are directly embedded in the random walk constraint in each iteration. Further, both the assignment matrix (vector) and the pairwise similarity measure between graphs are iteratively updated according the discrete constraints, which automatically leads the continuous solution to the discrete domain. Comparisons on both synthetic and real-world data demonstrate the effectiveness of the proposed algorithm.\"}, {\"paperId\": \"ea155231e5cda1ba8093c06cbd60c837fb5cef71\", \"abstract\": \"ABSTRACT While Industry 4.0 has been trending in practice and research, operations management studies in this area remain nascent. Our intent is to understand the current state of research in Industry 4.0 in different disciplines and deduce insights and opportunities for future research in operations management. In this paper, we provide a focused analysis to examine the state-of-the-art research in Industry 4.0. To learn about researchers\\u2019 perspectives about Industry 4.0, we conducted a large-scale, cross-disciplinary and global survey on Industry 4.0 topics among researchers in industrial engineering, operations management, operations research, control and data science at the 9th IFAC MIM 2019 Conference in Berlin in August 2019. By using our survey findings and literature analysis, we build structural and conceptual frameworks to understand the current state of knowledge and to propose future research opportunities for operations management scholars. Glossary of Abbreviations AGV: Automated guided vehicle; AI: Artificial intelligence; APS: Advanced planning system: a wide variety of software tools and techniques, with many applications in manufacturing and logistics (including the service sector); BDA: Big data analytics; CAS: Complex adaptive system: a system composed of many interacting parts that evolve and adapt over time; CIM: Computer integrated manufacturing; CPFR: Collaborative planning, forecasting and replenishment; CPS: Cyber-physical system: a seamless integration of computation and physical components; DAMCLS: Decision analysis, modelling, control and learning systems; ERP: Enterprise resource planning; FMS: Flexible manufacturing system; I4.0: Industry 4.0; IFAC: International Federation of Automatic Control: a federation is concerned with the impact of control technology on society; IME: Industrial and mechanical engineering; IoT: Internet-of-Things; IT: Information technology; M2M: Machine-to-machine; MAS: Multi-agent system: a loosely coupled network of software agents that interact to solve problems that are beyond the individual capacities or knowledge of each problem solver; OR: Operations research; RFID: Radio frequency identification: a technology that uses electromagnetic fields to automatically identify and track tags attached to objects; RMS: Reconfigurable manufacturing system: a manufacturing system that can change and evolve rapidly in order to adjust its productivity capacity and functionality; OM: Operations management; T&T: Track and trace system; VCA: VOS viewer co-occurrence analysis: a software tool for visualising bibliometric networks; VMI: Vendor-managed inventory.\"}, {\"paperId\": \"528ccd8ddb58cd676fd427a10e255476d1e8a985\", \"abstract\": \"The series \\\"Advances in Intelligent Systems and Computing\\\" contains publications on theory, applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually all disciplines such as engineering, natural sciences, computer and information science, ICT, economics, business, e-commerce, environment, healthcare, life science are covered. The list of topics spans all the areas of modern intelligent systems and computing such as: computational intelligence, soft computing including neural networks, fuzzy systems, evolutionary computing and the fusion of these paradigms, social intelligence, ambient intelligence, computational neuroscience, artificial life, virtual worlds and society, cognitive science and systems, Perception and Vision, DNA and immune based systems, self-organizing and adaptive systems, e-Learning and teaching, human-centered and human-centric computing, recommender systems, intelligent control, robotics and mechatronics including human-machine teaming, knowledge-based paradigms, learning paradigms, machine ethics, intelligent data analysis, knowledge management, intelligent agents, intelligent decision making and support, intelligent network security, trust management, interactive entertainment, Web intelligence and multimedia.    The publications within \\\"Advances in Intelligent Systems and Computing\\\" are primarily proceedings of important conferences, symposia and congresses. They cover significant recent developments in the field, both of a foundational and applicable character. An important characteristic feature of the series is the short publication time and world-wide distribution. This permits a rapid and broad dissemination of research results.  ** Indexing: The books of this series are submitted to ISI Proceedings, EICompendex, DBLP, SCOPUS, Google Scholar and Springerlink **\"}, {\"paperId\": \"5fd3dfa90fc979aea925635ff44bc8b07f76afaf\", \"abstract\": \"\\n Digitalization and Artificial Intelligence have impacted the oil and gas industry. Drilling of wells, predictive maintenance and digital fields are examples for the use of these technologies. In hydrocarbon production forecasting, numerical reservoir models and \\\"digital twins\\\" of reservoirs have been used for decades. However, increasing computing power and Artificial Intelligence recently enabled oil and gas companies to generate \\\"digital siblings\\\" of reservoirs (model ensembles) covering the uncertainty range in static data (e.g. petrophysics, geological structure), dynamic data (e.g. oil or gas properties) and economics (Capital Expenditures, Operating Expenditures). Machine Learning and Artificial Intelligence are applied to condition the model ensembles to measured data and improve hydrocarbon production forecasting under uncertainty.\\n The model ensembles can be used for quantitative decision making under uncertainty. This allows companies to shorten the time for field (re-)development planning and to develop into learning organizations for decision making.\\n These developments require companies to change the way of working in hydrocarbon production forecasting and decision analysis. Additional skills need to be developed in companies to embrace digitalization. Data science - which is considered a key skill in digitalization - has not been identified as crucial in skills development of oil and gas companies in the past. However, for data driven decision making, advanced data analytics skills and data science skills are a pre-requisite. To overcome this skill gap, staff needs to be trained and graduates with data science and profound physical and chemical skills need to be hired.\\n Furthermore, skills development has to address the challenge of incorrect use of Machine Learning technologies and the risks of Artificial Intelligence leading to erroneous optimizations. In particular interpretability of AI needs to be covered in skills development.\"}, {\"paperId\": \"f4be4aa22ccf3c71b8e32e5780aa15fe39125f3a\", \"abstract\": \"It is envisioned that significant improvements in medical capabilities may be required to meet formidable conditions expected in future military conflicts and global events such as the COVID-19 pandemic. Similar challenges may exist for large-scale humanitarian assistance missions and civilian mass casualty events that do not conform to prior assumptions for care delivery including evacuation within the golden hour and availability of large medical footprints in non-traditional and field settings. The importance of standardization and foundational infrastructure for medical devices, sensors, and data management is presented in order to achieve safe, and effective medical systems that deliver dramatic advances in functionality made possible by Artificial Intelligence and Machine Learning (AI/ML). The concept of autonomous, artificial intelligence based learning systems for medical support in military Multi-Domain Operations (MDO) to meet evolving demands is presented. Drivers towards greater use of Artificial Intelligence (AI) and Medical Autonomy to solve anticipated gaps in forward resuscitative and stabilization care, as well as associated relevance and implications for the management of civilian disasters are introduced. Finally, the central role of application architecture and robust technology frameworks necessary to advance the state of the science are discussed.\"}, {\"paperId\": \"aa4f3540971b416400a5d95274a8215938677afc\", \"abstract\": \"Knowledge engineering paradigms (KEPs) deal with the development of intelligent systems in which reasoning and knowledge play pivotal role. Recently, KEPs receive increasing attention within the fields of smart education. Researchers have been used the knowledge engineering (KE) techniques, approaches and methodologies to develop a smart tutoring systems (STSs). The main characteristics of such systems are the ability of reasoning, inference and based on static and heuristic knowledge. On the other side, the convergence of artificial intelligence (AI), web science (WS) and data science (DS) is enabling the creation of a new generation of web-based smart systems for all educational and learning tasks. This paper discusses the KEPs techniques and tools for developing the smart educational and learning systems. Four most popular paradigms are discussed and analyzed namely; case-based reasoning, ontological engineering, data mining and intelligent agents. The main objective of this study is to determine and exploration the benefits and advantages of such computational paradigms to increase the effectiveness and enhancing the efficiency of the smart tutoring systems. Moreover, the paper addresses the challenges faced by the application developers and knowledge engineers in developing and deploying such systems. In addition to institutional and organizational aspects of smart educational technologies development and application. Key-Words: Knowledge engineering and management, Artificial intelligence in education, Smart tutoring systems, Computational intelligence, Machine learning\"}, {\"paperId\": \"087a3c950aa7966ea94aad50c1d949efccae8c59\", \"abstract\": null}, {\"paperId\": \"075a40670f5bdb425139eb4a2c0e0ef9d3f6a4d9\", \"abstract\": \"Our mobility is an important daily requirement so much so that any disruption to it severely degrades our perceived quality of life. Studies in gait and human movement sciences, therefore, play a significant role in maintaining the well-being of our mobility. Current gait analysis involves numerous interdependent gait parameters that are difficult to adequately interpret due to the large volume of recorded data and lengthy assessment times in gait laboratories. A proposed solution to these problems is computational intelligence (CI), which is an emerging paradigm in biomedical engineering most notably in pathology detection and prosthesis design. The integration of CI technology in gait systems facilitates studies in disorders caused by lower limb defects, cerebral disorders, and aging effects by learning data relationships through a combination of signal processing and machine learning techniques. Learning paradigms, such as supervised learning, unsupervised learning, and fuzzy and evolutionary algorithms, provide advanced modeling capabilities for biomechanical systems that in the past have relied heavily on statistical analysis. CI offers the ability to investigate nonlinear data relationships, enhance data interpretation, design more efficient diagnostic methods, and extrapolate model functionality. These are envisioned to result in more cost-effective, efficient, and easy-to-use systems, which would address global shortages in medical personnel and rising medical costs. This paper surveys current signal processing and CI methodologies followed by gait applications ranging from normal gait studies and disorder detection to artificial gait simulation. We review recent systems focusing on the existing challenges and issues involved in making them successful. We also examine new research in sensor technologies for gait that could be combined with these intelligent systems to develop more effective healthcare solutions.\"}, {\"paperId\": \"e211ec0fdaee8bec696475eaffae05af32222b9b\", \"abstract\": \"Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs\\u2014for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.\"}, {\"paperId\": \"1d4b2cd7026e95530b9a29b2d9b207421f5dc421\", \"abstract\": \"Data analytics has become an important part of companies in industry, leading to an increase in the demand for analytics experts. Data analytics is also a central aspect of trends such as \\u201cBig Data\\u201d, \\u201cData Science\\u201d, \\u201cArtificial Intelligence\\u201d. As such, for potential analytics professionals to have relevant job skills, educational institutions need to have an up-to-date picture of the demanded job skills. We studied the recent past to better anticipate future developments of analytics job skills through a study based on text mining using job advertisements from 2014 and 2019 summing up to a total of 17,282 advertisements. We investigated how these trends evolved by looking at meaning of words using associated words as well as topics obtained from topic modeling. Our longitudinal study reveals a shift in prevalence and meaning of analytics trends in the industry. It shows a small shift from business oriented job skills towards analytical and technical ones. This finding is accompanied by increasing popularity of \\u201cBig Data\\u201d, in contrast with the reduced popularity of \\u201cBusiness Intelligence\\u201d. Additionally, we observed how \\u201cMachine Learning\\u201d is becoming more strongly associated with the field of data science itself.\"}, {\"paperId\": \"9361dfb0f38d80dadf373b6cda02d44b8ce43867\", \"abstract\": \"In today's increasingly connected world, graph mining plays a pivotal role in many real-world application domains, including social network analysis, recommendations, marketing and financial security. Tremendous efforts have been made to develop a wide range of computational models. However, recent studies have revealed that many widely-applied graph mining models could suffer from potential discrimination. Fairness on graph mining aims to develop strategies in order to mitigate bias introduced/amplified during the mining process. The unique challenges of enforcing fairness on graph mining include (1) theoretical challenge on non-IID nature of graph data, which may invalidate the basic assumption behind many existing studies in fair machine learning, and (2) algorithmic challenge on the dilemma of balancing model accuracy and fairness. This tutorial aims to (1) present a comprehensive review of state-of-the-art techniques in fairness on graph mining and (2) identify the open challenges and future trends. In particular, we start with reviewing the background, problem definitions, unique challenges and related problems; then we will focus on an in-depth overview of (1) recent techniques in enforcing group fairness, individual fairness and other fairness notions in the context of graph mining, and (2) future directions in studying algorithmic fairness on graphs. We believe this tutorial could be attractive to researchers and practitioners in areas including data mining, artificial intelligence, social science and beneficial to a plethora of real-world application domains.\"}, {\"paperId\": \"499475516ac0321e5a752fd5521979359a3f6573\", \"abstract\": \"\\nPurpose\\nThe purpose of this paper is to examine the artificial intelligence (AI) methodologies to fight against money laundering crimes in Colombia.\\n\\n\\nDesign/methodology/approach\\nThis paper examines Colombian money laundering situations with some methodologies of network science to apply AI tools.\\n\\n\\nFindings\\nThis paper identifies the suspicious operations with AI methodologies, which are not common by number, quantity or characteristics within the economic or financial system and normal practices of companies or industries.\\n\\n\\nResearch limitations/implications\\nAccess to financial institutions\\u2019 data was the most difficult element for research because affect the implementation of a set of different algorithms and network science methodologies.\\n\\n\\nPractical implications\\nThis paper tries to reduce the social and economic implications from money laundering (ML) that result from illegal activities and different crimes against inhabitants, governments, public resources and financial systems.\\n\\n\\nSocial implications\\nThis paper proposes a software architecture methodology to fight against ML and financial crime networks in Colombia which are common in different countries. These methodologies complement legal structure and regulatory framework.\\n\\n\\nOriginality/value\\nThe contribution of this paper is how within the flow already regulated by financial institutions to manage the ML risk, AI can be used to minimize and identify this kind of risk. For this reason, the authors propose to use the graph analysis methodology for monitoring and identifying the behavior of different ML patterns with machine learning techniques and network science methodologies. These methodologies complement legal structure and regulatory framework.\\n\"}, {\"paperId\": \"94ff21888cc1b38c55270ed225524ce8770a7853\", \"abstract\": \"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. SI systems are typically made up of a population of simple agents interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of \\u201cintelligent\\u201d global behavior, unknown to the individual agents. Natural examples of SI include ant colonies, bird flocking, animal herding, bacterial growth, and fish schooling. \\n \\nResearch in SI started in the late 1980s. Besides the applications to conventional optimization problems, SI can be employed in library materials acquisition, communications, medical dataset classification, dynamic control, heating system planning, moving objects tracking, and prediction. Indeed, SI can be applied to a variety of fields in fundamental research, engineering, industries, and social sciences. \\n \\nThe main objective of this special issue is to provide the readers with a collection of high quality research articles that address the broad challenges in application aspects of swarm intelligence and reflect the emerging trends in state-of-the-art algorithms. \\n \\nThe special issue received 42 high quality submissions from different countries all over the world. All submitted papers followed the same standard (peer-reviewed by at least three independent reviewers) as applied to regular submissions to \\u201cthis journal\\u201d. Due to the limited space, 15 papers were finally included. The primary guideline was to demonstrate the wide scope of SI algorithms and applications in various aspects. Besides, mathematically oriented papers with promising potential in practical problems were also included. \\n \\nThe paper authored by Y.-L. Wu et al. (National Chiao Tung University and Ming Chuan University) presents an integer programming model of the studied problem by considering how to select materials in order to maximize the average preference and the budget execution rate under some practical restrictions including departmental budget and limitation of the number of materials in each category and each language. They propose a discrete particle swarm optimization (DPSO) with scout particles, design an initialization algorithm and a penalty function to cope with the constraints, and employ the scout particles to enhance the exploration within the solution space. \\n \\nIn the paper by Z. Yin et al. (Harbin Institute of Technology), they propose an efficient multiuser detector based on a suboptimal code mapping multiuser detector and artificial bee colony algorithm (SCM-ABC-MUD) and implement the proposed algorithm in direct-sequence ultrawideband (DS-UWB) systems under the additive white Gaussian noise (AWGN) channel. \\n \\nM. S. Uzer et al. (Selcuk University) offer a hybrid approach that uses the artificial bee colony (ABC) algorithm for feature selection and support vector machines for classification. For the diagnosis of hepatitis, liver disorders, and diabetes datasets from the UCI database, the proposed system reached classification accuracies of 94.92%, 74.81%, and 79.29%, respectively. \\n \\nAnother paper is by M. Karakose (Firat University) and U. Cigdem (Gaziosmanpasa University). It proposes a new approach for improvement of DNA computing with adaptive parameters towards the desired goal using quantum-behaved particle swarm optimization (QPSO). Experimental results obtained with MATLAB and FPGA demonstrate ability to provide effective optimization, considerable convergence speed, and high accuracy according to DNA computing algorithm. \\n \\nIn the paper by Y. Celik (Karamanoglu Mehmetbey University) and E. Ulker (Selcuk University), their research proposes an improved marriage in honey bees optimization (IMBO) by adding Levy flight algorithm for queen mating flight and neighboring for worker drone improving. The IMBO algorithm's performance and its success are tested on the well-known six unconstrained test functions and compared with other metaheuristic optimization algorithms. \\n \\nM. Baygin (Ardahan University) and M. Karakose (Firat University) study a new approach of immune system-based optimal estimate for dynamic control of group elevator systems. The method is mainly based on estimation of optimal way by optimizing all calls with genetic, immune system and DNA computing algorithms, and it is evaluated with a fuzzy system. With dynamic and adaptive control approach in this study, a significant progress on group elevator control systems has been achieved in terms of time and energy efficiency according to traditional methods. \\n \\nThe paper by M. Karakose (Firat University) proposes a reinforcement-learning based artificial immune classifier. The proposed new approach has many contributions according to other methods in the literature such as effectiveness, less memory cell, high accuracy, speed, and data adaptability. Some benchmark data and remote image data are used for experimental results. The comparative results with supervised/unsupervised based artificial immune system, negative selection classifier, and resource limited artificial immune classifier are given to demonstrate the effectiveness of the proposed new method. \\n \\nIn their paper, T. J. Choi et al. (Sungkyunkwan University) and (Daegu Gyeongbuk Institute of Science and Technology) present an adaptive parameter control DE algorithm. The control parameters of each individual are adapted based on the average of successfully evolved individuals' parameter values using the Cauchy distribution. The experimental results show that their proposed algorithm is more robust than the standard DE algorithm and several state-of-the-art adaptive DE algorithms in solving various unimodal and multimodal problems. \\n \\nIn the paper by R.-J. Ma et al. (Southwest Jiaotong University and CSR Qishuyan Institute Co., Ltd.), the authors present an integral mathematical model and particle swarm optimization (PSO) algorithm based on the life cycle cost (LCC) approach for the heating system planning (HSP) problem. The results show that the improved particle swarm optimization (IPSO) algorithm can more preferably solve the HSP problem than PSO algorithm. \\n \\nIn the paper by M. Tang et al. (National University of Defense Technology and Universite Pierre et Marie Curie), they report that the flocking has some negative effects on the human, as the infectious disease H7N9 will easily be transmitted from the denser flocking birds to the human. Their paper focuses on the H7N9 virus transmission in the flocking birds and from the flocking birds to the human. Some interesting results have been shown: (1) only some simple rules could result in an emergence such as the flocking; (2) the minimum distance between birds could affect H7N9 virus transmission in the flocking birds and even affect the virus transmissions from the flocking birds to the human. \\n \\nY. Wang et al. (China University of Petroleum) present a memory-based multiagent coevolution algorithm for robust tracking the moving objects. Each agent can remember, retrieve, or forget the appearance of the object through its own memory system by its own experience. Experimental results show that their proposed method can deal with large appearance changes and heavy occlusions when tracking a moving object. \\n \\nThe paper by Q. Ni and J. Deng (Southeast University and Soochow University) analyzes the performance of PSO with the proposed random topologies and explores the relationship between population topology and the performance of PSO from the perspective of graph theory characteristics in population topologies. Further, in a relatively new PSO variant which named logistic dynamic particle optimization, an extensive simulation study is presented to discuss the effectiveness of the random topology and the design strategies of population topology. \\n \\nY. Zhou and H. Zheng (Guangxi University for Nationalities, Guangxi Key Laboratory of Hybrid Computation and IC Design Analysis) propose a novel complex valued cuckoo search algorithm. They use complex-valued encoding to expand the information of nest individuals and denote the gene of individuals by plurality. The value of independent variables for objective function is determined by modules, and a sign of them is determined by angles. The position of nest is divided into real part gene and imaginary gene. Six typical functions are tested, and the usefulness of the proposed algorithm is verified. \\n \\nThe paper by R. Alwee et al. (Universiti Teknologi Malaysia) introduces a hybrid model that combines support vector regression (SVR) and autoregressive integrated moving average (ARIMA) to be applied in crime rates forecasting. Particle swarm optimization is used to estimate the parameters of the SVR and ARIMA models. The experimental results show that their proposed hybrid model is able to produce more accurate forecasting results as compared to the individual models. \\n \\nFinally, K. S. Lim et al. (Universiti Teknologi Malaysia, Universiti Malaysia Pahang, and University of Malaya) describe an improved Vector Evaluated Particle Swarm Optimization algorithm by incorporating the nondominated solutions as the guidance for a swarm rather than using the best solution from another swarm. The results suggest that the improved Vector Evaluated Particle Swarm Optimization algorithm has impressive performance compared with the conventional Vector Evaluated Particle Swarm Optimization algorithm.\"}, {\"paperId\": \"5b5d6867e7a69cae38003c0c6fcb4c621f8d4af4\", \"abstract\": null}, {\"paperId\": \"8467534990215ef80b005bc9e4df5a1a2b2e7745\", \"abstract\": \"Data Mining with R: Learning with Case Studies, Second Edition uses practical examples to illustrate the power of R and data mining. Providing an extensive update to the best-selling first edition, this new edition is divided into two parts. The first part will feature introductory material, including a new chapter that provides an introduction to data mining, to complement the already existing introduction to R. The second part includes case studies, and the new edition strongly revises the R code of the case studies making it more up-to-date with recent packages that have emerged in R. The book does not assume any prior knowledge about R. Readers who are new to R and data mining should be able to follow the case studies, and they are designed to be self-contained so the reader can start anywhere in the document. The book is accompanied by a set of freely available R source files that can be obtained at the books web site. These files include all the code used in the case studies, and they facilitate the \\\"do-it-yourself\\\" approach followed in the book. Designed for users of data analysis tools, as well as researchers and developers, the book should be useful for anyone interested in entering the \\\"world\\\" of R and data mining. About the Author Lus Torgo is an associate professor in the Department of Computer Science at the University of Porto in Portugal. He teaches Data Mining in R in the NYU Stern School of Business MS in Business Analytics program. An active researcher in machine learning and data mining for more than 20 years, Dr. Torgo is also a researcher in the Laboratory of Artificial Intelligence and Data Analysis (LIAAD) of INESC Porto LA.\"}, {\"paperId\": \"3487eb1cb7a29fd2effc4f45f41f738cfe158ef9\", \"abstract\": \"P racticing chemists solve problems via \\u201cchemical intuition\\u201d, a quality that lets them skip intermediate details and get to the essential result, even if the outcome is counterintuitive to the uninitiated. There is no human shortcut to building this intuition; chemists hone their skills through years of experience of learning and memorizing patterns of molecular structure and reactivity. It is in this spirit that Vijay Pande and co-workers propose in \\u201cLow Data Drug Discovery with One-Shot Learning\\u201d in this issue of ACS Central Science a computational approach for chemical prediction by learning from a low number of examples. The paper touches on many central themes that are relevant to the intersection of the three main components of computation in chemistry: molecular representation, chemical space exploration, and machine learning to accelerate computation. For discovering new molecules, the enormity of chemical space cannot be understated; the number of \\u201csmall\\u201d to \\u201cmedium\\u201d sized molecules is estimated to be in the range of 10 to 10180, a number that is a hundred orders of magnitude larger than the number of atoms in the visible universe. With just a considerably small number of examples, chemists are able to distinguish and assess the potential function of a molecule for a given task. For example, we recently created a \\u201cMolecular Tinder\\u201d application that helped us in the design of molecules for organic displays. In analogy to the dating application, Molecular Tinder was a voting system that allowed us to harvest information from experimentalists who voted \\u201cYes\\u201d, \\u201cNo\\u201d, or \\u201cMaybe\\u201d on the synthesizability of molecules. Voting results allowed us to design algorithms that preferentially generated molecules with practical synthesic routes that were eventually synthesized and tested in real devices. Another very important aspect of human intuition is \\u201ctransferability\\u201d, which enables the generalization of knowledge learned in a particular domain to untested domains. Everyone who has passed an undergraduate organic chemistry test had to show that their brain is able to generalize from one domain to the other. This is a much more challenging task for a computer. We are sometimes able to predict with varying degrees of success these properties using quantum chemistry calculations, but when these simulations are involved, supralinear computational scaling laws hinder the application of most common algorithms to complex molecules. Therefore, to cover chemical space efficiently, we cannot go unaided by intuition if we ever hope to explore it for successful molecular design. It is often thought in the artificial intelligence (AI) community that any human decision that can be done in a matter of a few seconds, can be in theory, learned and automated by a computer. There have been many recent examples where deep learning is solving increasingly complex tasks and getting closer to the performance of humans, even surpassing it in certain tasks such as the game Go with AlphaGo. This progress has been propelled mainly by two factors: broader availability of data and cheaper\"}, {\"paperId\": \"3f9ead904f9671b39ffe46641343c88423016999\", \"abstract\": \"The term Industry 4.0, coined to be the fourth industrial revolution, refers to a higher level of automation for operational productivity and efficiency by connecting virtual and physical worlds in an industry. With Industry 4.0 being unable to address and meet increased drive of personalization, the term Industry 5.0 was coined for addressing personalized manufacturing and empowering humans in manufacturing processes. The onset of the term Industry 5.0 is observed to have various views of how it is defined and what constitutes the reconciliation between humans and machines. This serves as the motivation of this paper in identifying and analyzing the various themes and research trends of what Industry 5.0 is using text mining tools and techniques. Toward this, the abstracts of 196 published papers based on the keyword \\u201cIndustry 5.0\\u201d search in IEEE, science direct and MDPI data bases were extracted. Data cleaning and preprocessing were performed for further analysis to apply text mining techniques of key terms extraction and frequency analysis. Further topic mining i.e., unsupervised machine learning method was used for exploring the data. It is observed that the terms artificial intelligence (AI), big data, supply chain, digital transformation, machine learning, internet of things (IoT), are among the most often used and among several enablers that have been identified by researchers to drive Industry 5.0. Five major themes of Industry 5.0 addressing, supply chain evaluation and optimization, enterprise innovation and digitization, smart and sustainable manufacturing, transformation driven by IoT, AI, and Big Data, and Human-machine connectivity were classified among the published literature, highlighting the research themes that can be further explored. It is observed that the theme of Industry 5.0 as a gateway towards human machine connectivity and co-existence is gaining more interest among the research community in the recent years.\"}, {\"paperId\": \"4aee62165b7b25052aaa7ffa96dbeb6fe32a2cb5\", \"abstract\": \"Gravitational wave astronomy has set in motion a scientific revolution. To further enhance the science reach of this emergent field, there is a pressing need to increase the depth and speed of the gravitational wave algorithms that have enabled these groundbreaking discoveries. To contribute to this effort, we introduce Deep Filtering, a new highly scalable method for end-to-end time-series signal processing, based on a system of two deep convolutional neural networks, which we designed for classification and regression to rapidly detect and estimate parameters of signals in highly noisy time-series data streams. We demonstrate a novel training scheme with gradually increasing noise levels, and a transfer learning procedure between the two networks. We showcase the application of this method for the detection and parameter estimation of gravitational waves from binary black hole mergers. Our results indicate that Deep Filtering significantly outperforms conventional machine learning techniques, achieves similar performance compared to matched-filtering while being several orders of magnitude faster thus allowing real-time processing of raw big data with minimal resources. More importantly, Deep Filtering extends the range of gravitational wave signals that can be detected with ground-based gravitational wave detectors. This framework leverages recent advances in artificial intelligence algorithms and emerging hardware architectures, such as deep-learning-optimized GPUs, to facilitate real-time searches of gravitational wave sources and their electromagnetic and astro-particle counterparts.\"}, {\"paperId\": \"915b2fc052e85c5aaae45d1eeebec0a3bea4f776\", \"abstract\": \"The series \\\"Advances in Intelligent Systems and Computing\\\" contains publications on theory, applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually all disciplines such as engineering, natural sciences, computer and information science, ICT, economics, business, e-commerce, environment, healthcare, life science are covered. The list of topics spans all the areas of modern intelligent systems and computing such as: computational intelligence, soft computing including neural networks, fuzzy systems, evolutionary computing and the fusion of these paradigms, social intelligence, ambient intelligence, computational neuroscience, artificial life, virtual worlds and society, cognitive science and systems, Perception and Vision, DNA and immune based systems, self-organizing and adaptive systems, e-Learning and teaching, human-centered and human-centric computing, recommender systems, intelligent control, robotics and mechatronics including human-machine teaming, knowledge-based paradigms, learning paradigms, machine ethics, intelligent data analysis, knowledge management, intelligent agents, intelligent decision making and support, intelligent network security, trust management, interactive entertainment, Web intelligence and multimedia.    The publications within \\\"Advances in Intelligent Systems and Computing\\\" are primarily proceedings of important conferences, symposia and congresses. They cover significant recent developments in the field, both of a foundational and applicable character. An important characteristic feature of the series is the short publication time and world-wide distribution. This permits a rapid and broad dissemination of research results.   Indexed by DBLP, EI Compendex, INSPEC, WTI Frankfurt eG, zbMATH, Japanese Science and Technology Agency (JST), SCImago.  All books published in the series are submitted for consideration in Web of Science.\"}, {\"paperId\": \"91a4db306e5c435ec967ffc6c73fde0dda9efcdd\", \"abstract\": null}, {\"paperId\": \"80d2c2b4d6e9271dbd83f12d46e77cc9a3791e17\", \"abstract\": \"Imitation is a powerful mechanism whereby knowledge may be transferred between agents (both biological and artificial). Key problems on the topic of imitation have emerged in various areas close to artificial intelligence, including the cognitive and social sciences, animal behavior, robotics, human-computer interaction, embodied intelligence, software engineering, programming by example and machine learning. Artificial systems used to study imitation can both test models of imitation derived from observational or neurobiological data on imitation in animals and then apply them to different kinds of nonbiological systems ranging from robots to software agents. A crucial problem in imitation is the correspondence problem, mapping action sequences of the demonstrator and the imitator agent. This problem becomes particularly obvious when the two agents do not share the same embodiment and affordances. This paper describes a new general imitation mechanism called ALICE (action learning for imitation via correspondence between embodiments) that specifically addresses the correspondence problem. The mechanism is implemented and its efficacy illustrated on the \\\"chessworld\\\" testbed that was created to study imitation from an agent-based perspective, i.e., by a particular agent in a particular environment.\"}, {\"paperId\": \"75c6e4babee72f36adf9d018d7f4e81358c4f630\", \"abstract\": null}, {\"paperId\": \"d11806f468790a103cc9dcbf1d68354bb2254c49\", \"abstract\": \"The ecological condition of the world's waterways continues to decline under increasing pollution, human land use intensification, and/or demand for water abstraction. This is occurring despite the fact that freshwater ecologists and other water scientists have been investigating these environmental concerns for many years. Freshwater science has made considerable advances understanding the causes of this ecological decline, but we still appear to be further from halting that decline than ever before. Perhaps the scientific solutions are clear but political, social, legal or economic constraints intervene? Irrespective of the reasons, in my opinion freshwater science is failing to deal effectively with this environmental crisis. I believe that artificial intelligence devices and machine learning software may offer potential for dealing with the environmental crisis facing the world's freshwater. There are numerous, free and easy to use software packages that would enable freshwater ecologists to better understand some of the complex, nonlinear relationships in their data, and to potentially make better predictions about the effects of stressors and/or how best to mitigate them. I see a not too distant future where these devices will take over direct management of river reaches to hopefully provide more effective balancing of economic and environmental needs for water. I would like to encourage more scientists to embrace the ease and power of machine learning as a way to better interpret collected data, or at least provide an alternative perspective that may prove useful. WIREs Water 2015, 2:595\\u2013600. doi: 10.1002/wat2.1102\"}, {\"paperId\": \"0a1bf925a43c3e506c51cf487e07b07944b3243c\", \"abstract\": \"Industry 4.0 and its applications are one of the most important issues of countries that want to keep their competitiveness in the field of scientific, technological and innovation in today's world. The Countries that are not able to adapt to Industry 4.0 processes will not be able to develop scientifically and technologically or keep up with current innovation processes. One of the aims of this paper is to explain the concept of industry 4.0 to our readers in a comprehensible manner and to show how much science education is actually related to industry 4.0.\\nIn a general sense, Industry 4.0 or in other words, the 4th Industrial Revolution is an expression that includes many modern automation processes, large databases that communicate with each other, robotic devices and dark factories, in short, quality production technologies. In addition, Industry 4.0 includes modern technologies such as additive manufacturing, autonomous robots, big data, artificial intelligence, augmented reality, system integration, internet of things, cybersecurity, cloud computing, machine learning, deep learning etc. These technologies play an important role especially in the formation of new smart factories.\"}, {\"paperId\": \"7293d7440d89fc038d7e9ee8d1460cc9b3759478\", \"abstract\": \"Remarkable advances in computation and data storage and the ready availability of huge data sets have been the keys to the growth of the new disciplines of data mining and machine learning, while the enormous success of the Human Genome Project has opened up the field of bioinformatics. These exciting developments, which led to the introduction of many innovative statistical tools for high-dimensional data analysis, are described here in detail. The author takes a broad perspective; for the first time in a book on multivariate analysis, nonlinear methods are discussed in detail as well as linear methods. Techniques covered range from traditional multivariate methods, such as multiple regression, principal components, canonical variates, linear discriminant analysis, factor analysis, clustering, multidimensional scaling, and correspondence analysis, to the newer methods of density estimation, projection pursuit, neural networks, multivariate reduced-rank regression, nonlinear manifold learning, bagging, boosting, random forests, independent component analysis, support vector machines, and classification and regression trees. Another unique feature of this book is the discussion of database management systems. This book is appropriate for advanced undergraduate students, graduate students, and researchers in statistics, computer science, artificial intelligence, psychology, cognitive sciences, business, medicine, bioinformatics, and engineering. Familiarity with multivariable calculus, linear algebra, and probability and statistics is required. The book presents a carefully-integrated mixture of theory and applications, and of classical and modern multivariate statistical techniques, including Bayesian methods. There are over 60 interesting data sets used as examples in the book, over 200 exercises, and many color illustrations and photographs.\"}, {\"paperId\": \"5c93520a1b0edbba774420cabb9511704e00e535\", \"abstract\": \"In this study, the mathematical principles of rough sets theory are explained and a sample application about rule discovery from a decision table by using different algorithms in rough sets theory is presented. Data mining and usage of the useful patterns that reside in the databases have become a very important research area because of the rapid developments in both computer hardware and software industries. In parallel with the rapid increase in the data stored in the databases, effective use of the data is becoming a problem. To discover the rules or interesting and useful patterns from these stored data, data mining techniques are used. If data is incomplete or inaccurate, the results extracted from the database during the data discovery phase would be inconsistent and meaningless. Rough sets theory is a new mathematical approach used in the intelligent data analysis and data mining if data is uncertain or incomplete. This approach is of great importance in cognitive science and artificial intelligence, especially in machine learning, decision analysis, expert systems and inductive reasoning. There are many advantages of rough set approach in intelligent data analysis. Some of these advantages are being suitable for parallel processing, finding minimal data sets, supplying effective algorithms to discover hidden patterns in data, valuation of the meaningfulness of the data, producing decision rule set from data, being easy to understand and the results obtained can be interpreted clearly. In the last years, rough sets theory is widely used in different areas like engineering, banking and finance. In the last decades, the size of the data stored in the databases of the organizations has been growing each day and therefore we face difficulties about obtaining the valuable data. Databases are a collection of relational and non-recurring data to meet the demands of the organizations. Because the data stored in the databases is growing each day, it is getting harder for the users to reach the accurate and useful information. In the last few years, because of the rapid developments in both computer hardware and software industries, the increase in the storage capacities of huge databases, the data mining and the usage of the useful patterns that reside in the databases, became a very important research area. To discover the rules or interesting and useful patterns among these stored data in the databases, data mining techniques are used. Storing huge amount of increasing data in the databases, which is called information explosion, it is necessary to transform these data into necessary and useful information. Using conventional statistics techniques fail to satisfy the\"}, {\"paperId\": \"f39c32723d4693278ffde32e1abc5f3f3370ea70\", \"abstract\": \"Credibility, as the general concept covering trustworthiness and expertise, but also quality and reliability, is strongly debated in philosophy, psychology, and sociology, and its adoption in computer science is therefore fraught with difficulties. Yet its importance has grown in the information access community because of two complementing factors: on one hand, it is relatively difficult to precisely point to the source of a piece of information, and on the other hand, complex algorithms, statistical machine learning, artificial intelligence, make decisions on behalf of the users, with little oversight from the users themselves.This survey presents a detailed analysis of existing credibility models from different information seeking research areas, with focus on the Web and its pervasive social component. It shows that there is a very rich body of work pertaining to different aspects and interpretations of credibility, particularly for different types of textual content e.g., Web sites, blogs, tweets, but also to other modalities videos, images, audio and topics e.g., health care. After an introduction placing credibility in the context of other sciences and relating it to trust, we argue for a quartic decomposition of credibility: expertise and trustworthiness, well documented in the literature and predominantly related to information source, and quality and reliability, raised to the status of equal partners because the source is often impossible to detect, and predominantly related to the content.The second half of the survey provides the reader with access points to the literature, grouped by research interests. Section 3 reviews general research directions: the factors that contribute to credibility assessment in human consumers of information; the models used to combine these factors; the methods to predict credibility. A smaller section is dedicated to informing users about the credibility learned from the data. Sections 4, 5, and 6 go further into details, with domain-specific credibility, social media credibility, and multimedia credibility, respectively. While each of them is best understood in the context of Sections 1 and 2, they can be read independently of each other.The last section of this survey addresses a topic not commonly considered under \\\"credibility\\\": the credibility of the system itself, independent of the data creators. This is a topic of particular importance in domains where the user is professionally motivated and where there are no concerns about the credibility of the data e.g. e-discovery and patent search. While there is little explicit work in this direction, we argue that this is an open research direction that is worthy of future exploration.Finally, as an additional help to the reader, an appendix lists the existing test collections that cater specifically to some aspect of credibility.Overall, this review will provide the reader with an organised and comprehensive reference guide to the state of the art and the problems at hand, rather than a final answer to the question of what credibility is for computer science. Even within the relatively limited scope of an exact science, such an answer is not possible for a concept that is itself widely debated in philosophy and social sciences.\"}, {\"paperId\": \"9e50644454ffc5c823caeb660bcd90c759218a28\", \"abstract\": \": Graph theory, soft computing and machine learning are being used in our daily life problems in the field of science involving mathematics, optimization and decision sciences. Zadeh introduced the idea of fuzzy set. This idea helped Kauffman to present the concept of fuzzy graph. Molodtsov presented the idea of soft set. Using this concept, Thumbakara et al. discovered a novel idea of soft graphs and Akram et al. discussed the fundamentals of soft graphs. Smarandache conceptualized the hypersoft set (HS) which is the generalization of soft set. Hypersoft set transforms single attribute function to multi-attribute function. In this study, the existing concept of soft graph is extended to HS-graph and some of its rudiments like HS-subgraph, not HS-graph, HS-complete graph, HS-tree, etc., are conceptualized with the help of graphical representation and illustrative examples. Moreover, some theoretic operations are discussed with generalized results on hypersoft set. This study will help the researchers in multi-dimensional fields involving artificial intelligence (AI), soft computing, graph theory and networking, data sciences, etc. Hypersoft graph (HS-graph), Hypersoft complete graph (HSC-graph), Hypersoft subgraph (HS-subgraph), Hypersoft tree (HS-tree).\"}, {\"paperId\": \"68ff3f2219ab53461e8353ed58f0efe7d66e28d4\", \"abstract\": \"Word vector representations have a long tradition in several research fields, such as cognitive science or computational linguistics. They have been used to represent the meaning of various units of natural languages, including, among others, words, phrases, and sentences. Before the deep learning tsunami, count-based vector space models had been successfully used in computational linguistics to represent the semantics of natural languages. However, the rise of neural networks in NLP popularized the use of word embeddings, which are now applied as pre-trained vectors in most machine learning architectures. This book, written by Mohammad Taher Pilehvar and Jose Camacho-Collados, provides a comprehensive and easy-to-read review of the theory and advances in vector models for NLP, focusing specially on semantic representations and their applications. It is a great introduction to different types of embeddings and the background and motivations behind them. In this sense, the authors adequately present the most relevant concepts and approaches that have been used to build vector representations. They also keep track of the most recent advances of this vibrant and fast-evolving area of research, discussing cross-lingual representations and current language models based on the Transformer. Therefore, this is a useful book for researchers interested in computational methods for semantic representations and artificial intelligence. Although some basic knowledge of machine learning may be necessary to follow a few topics, the book includes clear illustrations and explanations, which make it accessible to a wide range of readers. Apart from the preface and the conclusions, the book is organized into eight chapters. In the first two, the authors introduce some of the core ideas of NLP and artificial neural networks, respectively, discussing several concepts that will be useful throughout the book. Then, Chapters 3 to 6 present different types of vector representations at the lexical level (word embeddings, graph embeddings, sense embeddings, and contextualized embeddings), followed by a brief chapter (7) about sentence and document embeddings. For each specific topic, the book includes methods and data sets to assess the quality of the embeddings. Finally, Chapter 8 raises ethical issues involved\"}, {\"paperId\": \"1af10257cde8968e6b8733435fa4c49c2e5fd88e\", \"abstract\": \"Today's modern world is filled with uncertainties and contradictions. As artificial intelligence (AI) advances, machines are frequently expected to mimic the human brain and, consequently, face the conflicts associated with this task. To overcome them, feature engineering has emerged as the field of science responsible for turning raw data into relevant input information, setting up classifiers in the fused digital signal processing (DSP) and pattern recognition (PR) domain. Despite the ongoing efforts to improve feature learning, handcrafted extraction still plays a very important role. In this context, a careful choice of features is extremely relevant for creating an accurate classification. This article sheds light on the problem of feature quality by using a nonclassical logical system capable of handling conflictive situations. It is known as paraconsistent logic (PL).\"}, {\"paperId\": \"589b9ce274bfc0e34475b10dc5c0b9c3e01b66e4\", \"abstract\": null}, {\"paperId\": \"bcefcd43386dc73b9504de724a11e39f055ca120\", \"abstract\": \"A major limitation of conventional web sites is their unorganized and isolated contents, which is created mainly for human consumption. This limitation can be addressed by organizing and publishing data, using powerful formats that add structure and meaning to the content of web pages and link related data to one another. Computers can \\\"understand\\\" such data better, which can be useful for task automation. The web sites that provide semantics (meaning) to software agents form the Semantic Web, the Artificial Intelligence extension of the World Wide Web. In contrast to the conventional Web (the \\\"Web of Documents\\\"), the Semantic Web includes the \\\"Web of Data\\\", which connects \\\"things\\\" (representing real-world humans and objects) rather than documents meaningless to computers. Mastering Structured Data on the Semantic Web explains the practical aspects and the theory behind the Semantic Web and how structured data, such as HTML5 Microdata and JSON-LD, can be used to improve your sites performance on next-generation Search Engine Result Pages and be displayed on Google Knowledge Panels. You will learn how to represent arbitrary fields of human knowledge in a machine-interpretable form using the Resource Description Framework (RDF), the cornerstone of the Semantic Web. You will see how to store and manipulate RDF data in purpose-built graph databases such as triplestores and quadstores, that are exploited in Internet marketing, social media, and data mining, in the form of Big Data applications such as the Google Knowledge Graph, Wikidata, or Facebooks Social Graph. With the constantly increasing user expectations in web services and applications, Semantic Web standards gain more popularity. This book will familiarize you with the leading controlled vocabularies and ontologies and explain how to represent your own concepts. After learning the principles of Linked Data, the five-star deployment scheme, and the Open Data concept, you will be able to create and interlink five-star Linked Open Data, and merge your RDF graphs to the LOD Cloud. The book also covers the most important tools for generating, storing, extracting, and visualizing RDF data, including, but not limited to, Protg, TopBraid Composer, Sindice, Apache Marmotta, Callimachus, and Tabulator. You will learn to implement Apache Jena and Sesame in popular IDEs such as Eclipse and NetBeans, and use these APIs for rapid Semantic Web application development. Mastering Structured Data on the Semantic Web demonstrates how to represent and connect structured data to reach a wider audience, encourage data reuse, and provide content that can be automatically processed with full certainty. As a result, your web contents will be integral parts of the next revolution of the Web. What youll learn Extend your markup with machine-readable annotations and get your data to the Google Knowledge Graph Represent real-world objects and persons with machine-interpretable code Develop Semantic Web applications in JavaReuse and interlink structured data and create LOD datasets Who this book is for The book is intended for web developers and SEO experts who want to learn state-of-the-art Search Engine Optimization methods using machine-readable annotations and machine-interpretable Linked Data definitions. The book will also benefit researchers interested in automatic knowledge discovery. As a textbook on Semantic Web standards powered by graph theory and mathematical logic, the book could also be used as a reference work for computer science graduates and Semantic Web researchers.\"}, {\"paperId\": \"1ae97c3c6bc7dd445c55fb6f29d3dd7d80535466\", \"abstract\": null}, {\"paperId\": \"1de081b42f91ff1a1a3358ed90a05a535284c87f\", \"abstract\": \"Machine learning is advancing towards a data-science approach, implying a necessity to a line of investigation to divulge the knowledge learnt by deep neuronal networks. Limiting the comparison among networks merely to a predefined intelligent ability, according to ground truth, does not suffice, it should be associated with innate similarity of these artificial entities. Here, we analysed multiple instances of an identical architecture trained to classify objects in static images (CIFAR and ImageNet data sets). We evaluated the performance of the networks under various distortions and compared it to the intrinsic similarity between their constituent kernels. While we expected a close correspondence between these two measures, we observed a puzzling phenomenon. Pairs of networks whose kernels' weights are over 99.9% correlated can exhibit significantly different performances, yet other pairs with no correlation can reach quite compatible levels of performance. We show implications of this for transfer learning, and argue its importance in our general understanding of what intelligence is, whether natural or artificial.\"}, {\"paperId\": \"aa9b97f058aaa8798fa1c68e1f500fcc22484a7e\", \"abstract\": \"This book can be presented in two different ways; introducing a particular methodology to build adaptive Web sites and; presenting the main concepts behind Web mining and then applying them to adaptive Web sites. In this case, adaptive Web sites is the case study to exemplify the tools introduced in the text. The authors start by introducing the Web and motivating the need for adaptive Web sites. The second chapter introduces the main concepts behind a Web site: its operation, its associated data and structure, user sessions, etc. Chapter three explains the Web mining process and the tools to analyze Web data, mainly focused in machine learning. The fourth chapter looks at how to store and manage data. Chapter five looks at the three main and different mining tasks: content, links and usage. The following chapter covers Web personalization, a crucial topic if we want to adapt our site to specific groups of people. Chapter seven shows how to use information extraction techniques to find user behavior patterns. The subsequent chapter explains how to acquire and maintain knowledge extracted from the previous phase. Finally, chapter nine contains the case study where all the previous concepts are applied to present a framework to build adaptive Web sites. In other words, the authors have taken care of writing a self-contained book for people that want to learn and apply personalization and adaptation in Web sites. This is commendable considering the large and increasing bibliography in these and related topics. The writing is easy to follow and although the coverage is not exhaustive, the main concepts and topics are all covered.IOS Press is an international science, technical and medical publisher of high-quality books for academics, scientists, and professionals in all fields. Some of the areas we publish in: -Biomedicine -Oncology -Artificial intelligence -Databases and information systems -Maritime engineering -Nanotechnology -Geoengineering -All aspects of physics -E-governance -E-commerce -The knowledge economy -Urban studies -Arms control -Understanding and responding to terrorism -Medical informatics -Computer Sciences\"}, {\"paperId\": \"875c77b0daf65f2db77b48e784cb68fb312edea3\", \"abstract\": null}, {\"paperId\": \"e70aa415863e6e347497398c5ba422a1198d0e47\", \"abstract\": \"The impacts of climate change on water resources management as well as the increasing severe natural disasters over the last decades have caught global attention. Reliable and accurate hydrological forecasts are essential for efficient water resources management and the mitigation of natural disasters. While the notorious nonlinear hydrological processes make accurate forecasts a very challenging task, it requires advanced techniques to build accurate forecast models and reliable management systems. One of the newest techniques for modelling complex systems is artificial intelligence (AI). AI can replicate the way humans learn and has the great capability to efficiently extract crucial information from large amounts of data to solve complex problems. The fourteen research papers published in this Special Issue contribute significantly to the uncertainty assessment of operational hydrologic forecasting under changing environmental conditions and the promotion of water resources management by using the latest advanced techniques, such as AI techniques. The fourteen contributions across four major research areas: (1) machine learning approaches to hydrologic forecasting; (2) uncertainty analysis and assessment on hydrological modelling under changing environments; (3) AI techniques for optimizing multi-objective reservoir operation; and (4) adaption strategies of extreme hydrological events for hazard mitigation. The papers published in this issue can not only advance water sciences but can also support policy makers toward more sustainable and effective water resources management.\"}, {\"paperId\": \"4e11ebdc3e4f30d774458fab9e4b45ff0d0aa971\", \"abstract\": null}, {\"paperId\": \"5f233f3de3667147b711b735cf97c498c74488f5\", \"abstract\": \"Computer vision is one of the fields of computer science that is one of the most powerful and persuasive types of artificial intelligence. It is similar to the human vision system, as it enables computers to recognize and process objects in pictures and videos in the same way as humans do. Computer vision technology has rapidly evolved in many fields and contributed to solving many problems, as computer vision contributed to self-driving cars, and cars were able to understand their surroundings. The cameras record video from different angles around the car, then a computer vision system gets images from the video, and then processes the images in real-time to find roadside ends, detect other cars, and read traffic lights, pedestrians, and objects. Computer vision also contributed to facial recognition; this technology enables computers to match images of people\\u2019s faces to their identities. which these algorithms detect facial features in images and then compare them with databases. Computer vision also play important role in Healthcare, in which algorithms can help automate tasks such as detecting Breast cancer, finding symptoms in x-ray, cancerous moles in skin images, and MRI scans. Computer vision also contributed to many fields such as image classification, object discovery, motion recognition, subject tracking, and medicine. The rapid development of artificial intelligence is making machine learning more important in his field of research. Use algorithms to find out every bit of data and predict the outcome. This has become an important key to unlocking the door to AI. If we had looked to deep learning concept, we find deep learning is a subset of machine learning, algorithms inspired by structure and function of the human brain called artificial neural networks, learn from large amounts of data. Deep learning algorithm perform a task repeatedly, each time tweak it a little to improve the outcome. So, the development of computer vision was due to deep learning. Now we'll take a tour around the convolution neural networks, let us say that convolutional neural networks are one of the most powerful supervised deep learning models (abbreviated as CNN or ConvNet). This name ;convolutional ; is a token from a mathematical linear operation between matrixes called convolution. CNN structure can be used in a variety of real-world problems including, computer vision, image recognition, natural language processing (NLP), anomaly detection, video analysis, drug discovery, recommender systems, health risk assessment, and time-series forecasting. If we look at convolutional neural networks, we see that CNN are similar to normal neural networks, the only difference between CNN and ANN is that CNNs are used in the field of pattern recognition within images mainly. This allows us to encode the features of an image into the structure, making the network more suitable for image-focused tasks, with reducing the parameters required to set-up the model. One of the advantages of CNN that it has an excellent performance in machine learning problems. So, we will use CNN as a classifier for image classification. So, the objective of this paper is that we will talk in detail about image classification in the following sections.\"}, {\"paperId\": \"7ffe4b48367a808512c2e4f8bfb88a37aa1f39ce\", \"abstract\": \"The inherent nature of energy, i.e., physicality, sociality and informatization, implies the inevitable and intensive interaction between energy systems and social systems. From this perspective, we define U+201C social energy U+201D as a complex sociotechnical system of energy systems, social systems and the derived artificial virtual systems which characterize the intense intersystem and intra-system interactions. The recent advancement in intelligent technology, including artificial intelligence and machine learning technologies, sensing and communication in Internet of Things technologies, and massive high performance computing and extreme-scale data analytics technologies, enables the possibility of substantial advancement in socio-technical system optimization, scheduling, control and management. In this paper, we provide a discussion on the nature of energy, and then propose the concept and intention of social energy systems for electrical power. A general methodology of establishing and investigating social energy is proposed, which is based on the ACP approach, i.e., U+201C artificial systems U+201D U+0028 A U+0029, U+201C computational experiments U+201D U+0028 C U+0029 and U+201C parallel execution U+201D U+0028 P U+0029, and parallel system methodology. A case study on the University of Denver U+0028 DU U+0029 campus grid is provided and studied to demonstrate the social energy concept. In the concluding remarks, we discuss the technical pathway, in both social and nature sciences, to social energy, and our vision on its future.\"}, {\"paperId\": \"20638c76d97dbc029557068191d5ba51be38e598\", \"abstract\": \"Words such as fairness, accountability, transparency, bias, \\u201dexplanation\\u201d and many others suffer from linguistic conflation. Scholars across different fields study disparate topics using distinct approaches, outlooks, and methods, while using shared terminology to describe distinct ideas. Even seemingly straightforward terms such as \\u201dalgorithm\\u201d, \\u201dartificial intelligence\\u201d, and \\u201dmachine learning\\u201d have murky boundaries and contested histories. Research at the interface of software systems and their human context (as well as practical policymaking and specifically drafting and interpreting the law) necessarily engages concepts across disciplines. However, because scholars and practitioners in different disciplines use the same words to mean different things, it can be challenging to advance understanding in a way that affects research or practice in different communities. Instead, faced with the question of how to describe concepts precisely, scholars and practitioners often double down on their existing disciplinary preconceptions, believing that resorting to their particular approach to rigor will surely convince those of different backgrounds. In an effort to build community around research in fairness, accountability and transparency, this tutorial presents the fruits of our research into the use of vocabulary by different stakeholder communities. One approach to facilitating better communication when terms are not available that cross cultural boundaries is to describe concepts. Covering core technical ideas in machine learning and data science for a broad audience, we present a set of core terms from our research and describe example scenarios in which\"}, {\"paperId\": \"ae31bf4f5f2d0c94c004883af44c605335c2e85b\", \"abstract\": \"This article outlines a research agenda for intelligent systems that will result in fundamental new capabilities for understanding the Earth system, based on requirements from Earth, ocean, polar, atmospheric, and geospace sciences. In order to meet the challenges presented by complex geosciences phenomena with uncertain, intermittent, sparse, multi-resolution, and multi-scale data, new approaches must be developed to incorporate existing scientific knowledge and the user\\u2019s context into intelligent systems. This will result in a new generation of knowledge-rich intelligent systems that will require significant research in artificial intelligence: 1) In knowledge representation, capturing scientific knowledge in the form of physical, geological, chemical, biological, and ecological processes will push the limits of the state of the art; 2) In sensing and robotics, that scientific knowledge should be used to prioritize and collect data; 3) In information integration, all those processes need to form a \\u201csystem of systems\\u201d where all data and knowledge are interconnected; 4) In machine learning, algorithms need to be enriched with models of the relevant physical, geological, chemical, biological, and ecological processes; and 5) In user interfaces, interaction modalities must be guided by a knowledgerich user model that provides context for interactions with scientists. This research agenda in knowledge-rich intelligent systems will be essential to unlock much needed integrated discoveries to chart our planet\\u2019s history and future. Communications of the ACM, January 2019, Vol. 62 No. 1, Pages 76-84. DOI: 10.1145/3192335 CACM 62(1), 2019 Y. Gil et al.\"}, {\"paperId\": \"77b765afb00ac4093d3b47a967d40de12ae2b92b\", \"abstract\": null}, {\"paperId\": \"b1c72cb07ee91d38f38254ba93ebebbb08100541\", \"abstract\": null}, {\"paperId\": \"c71ff5b083e8f44f978469b65fbef352c11eef73\", \"abstract\": \"The main aim of this Lecture is to discuss the main role of the Cognitive science in many applications, and mainly within the (Information Security Field). The lecture has the main topics: \\u2010Future Parameters and Advanced Technologies. \\u2010Main Features towards Cognitive \\u2010The Internet of Things \\u2010Modeling Cognitive Security \\u2010Some important parameters \\u2010International Scientific support \\u2010Users - Customers \\u2010Decision makers \\u2010Academicals Side From 1990 up, the Future parameters: {Wireless, Smartness (Artificial Intelligence), Computing Techniques, Informatics, Convergence (in Technology), Virtually, Multidisciplinary, Electronic (commerce; government, Economy), Cyber, Software Defined, Chaos / Randomness, Nano, Cognition}. Also some concentration on wireless generation 1G up to 6G, SDN and CRN. And concentration on the new computing techniques and their important applications. Computational Type of Cognitive Science: Ubiquitous Computing, Social computing, Affective computing, Cognitive Computing, Soft Computing, Cloud Computing, Mobile Computing, - etc. The main aspect of the lecture is related to cognitive science, which is the interdisciplinary study of: {Mind and Intelligence, Philosophy, Psychology, Artificial intelligence, Neuroscience, Education linguistics, and Anthropology}. The Cognitive Neuroscience is an Academic Field concerned with the scientific study of the Biological processes. It addresses the questions of: how psychological/cognitive activities are affected or controlled by neural circuits in the brain. Cognitive Security is characterized by technology that is able to understand, reason and learn. A much greater scale of Key point's relevant security data is now accessible with cognitive systems. The new approach of the cognitive security based on the following important facts: \\u2010Use of cognitive systems to analyze security trends and distill huge volumes of structured and unstructured data into information, and then into actionable knowledge. \\u2010Use of security technologies, techniques and processes that support cognitive systems' having the highest level of context and accuracy. \\u2010Cognitive security is to add cognition by exploiting technologies such as: machine learning, knowledge representation, network control and management, etc., while solving security problems. The Cognitive cryptography: This New scientific area is marked by a new generation of information systems, focused on developing Intelligent Cryptographic Protocols and procedures utilizing cognitive information processing approaches. Such systems use: the Semantic Analysis of encrypted data to select the most appropriate method of its encryption. The Cognitive Cryptography based on: Cryptographers Personal Data, Semantic Meaning of Information, and Context of Procedure.\"}, {\"paperId\": \"d9bbbfb252732bdcd9f184c2b963b180f624df36\", \"abstract\": \"In the classic setting of unsupervised domain adaptation (UDA), the labeled source data are available in the training phase. However, in many real-world scenarios, owing to some reasons such as privacy protection and information security, the source data is inaccessible, and only a model trained on the source domain is available. This paper proposes a novel deep clustering method for this challenging task. Aiming at the dynamical clustering at feature-level, we introduce extra constraints hidden in the geometric structure between data to assist the process. Concretely, we propose a geometry-based constraint, named semantic consistency on the nearest neighborhood (SCNNH), and use it to encourage robust clustering. To reach this goal, we construct the nearest neighborhood for every target data and take it as the fundamental clustering unit by building our objective on the geometry. Also, we develop a more SCNNH-compliant structure with an additional semantic credibility constraint, named semantic hyper-nearest neighborhood (SHNNH). After that, we extend our method to this new geometry. Extensive experiments on three challenging UDA datasets indicate that our method achieves state-of-the-art results. The proposed method has significant improvement on all datasets (as we adopt SHNNH, This work is partly funded by the German Research Foundation and the National Natural Science Foundation of China in the Crossmodal Learning project under contract Sonderforschungsbereich Transregio 169, the Hamburg Landesforschungsf\\u00f6rderungsprojekt Cross, the National Natural Science Foundation of China (61773083); Horizon2020 RISE project STEP2DYNA (691154); the National Key R&D Program of China (2018YFE0203900, 2020YFB1313600); the National Natural Science Foundation of China (U1813202, 61773093); the Shanghai Artificial Intelligence Innovation Development Special Support Project, R & D and Industrialization (3920365001); the Sichuan Science and Technology Program (2020YFG0476); Open Project of State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China (KFKT2021B39). (Corresponding authors: Jianwei Zhang.) Song Tang are with the Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China; the State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, China; the Technical Aspects of Multimodal Systems (TAMS) Group, Department of Informatics, Universit\\u00e4t Hamburg, Hamburg, Germany. (e-mail: tntechlab@hotmail.com) Yan Yang are with the Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China Zhiyuan Ma are with the Institute of Machine Intelligence, University of Shanghai for Science and Technology, Shanghai, China; the State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China. Fanyu Zeng is with the Engineering Research Center of Wideband Wireless Communication Technology, Ministry of Education, Nanjing University of Posts and Telecommunications, Nanjing, China. Shuzhi Sam Ge is with the Department of Electrical and Computer Engineering, National University of Singapore, Singapore. Changshui Zhang is with the Department of Automation, Tsinghua University, Beijing, China. Norman Hendrich and Jianwei Zhang are with the Technical Aspects of Multimodal Systems (TAMS) Group, Department of Informatics, Universit\\u00e4t Hamburg, Hamburg, Germany. the average accuracy increases by over 3.0% on the large-scaled dataset). Code is available at https://github.com/tntek/N2DCX.\"}, {\"paperId\": \"e260332242fa8e115e933403f29f87aa491426ca\", \"abstract\": \"The Aim of the paper is to reduce the number of hazardous events on railway tracks by developing a method of prediction of rare hazardous failures based on processing of large amounts of data on each kilometre of track obtained in real time from diagnostics systems. Hazardous failures are rare events; the set of variate values of the number of such events for an individual kilometre of track per year is: [0, 1]. However, for a railway network as a whole the yearly number of such events is in the dozens and efficient management requires the transition from the estimation of the probability of hazardous failure occurrence to the identification of the most probable location of failure. Methods . The problem of identification of rare, but hazardous possible events out of hundreds of thousands of records of non-critical railway track parameter divergences cannot be solved by conventional means of statistical processing. Hazardous events are predicted using the above statistics and artificial intelligence. Big Data and Data Science technology is used. Such technology includes methods of machine learning that enable item classification based on characteristics (features, predicates) and known cases of undesired event occurrence. The application of various algorithms of machine learning is demonstrated using the example of prediction of track superstructure failures using records collected between 2014 and 2019 on the Kuybyshevskaya Railway. Findings and conclusions . The result of facility ranking is the conclusion regarding the location of the most probable hazardous failure of railway track. That conclusion is based on the correspondence analysis between the actual characteristics of an item and conditions of its operation and the cases of adverse events and cases of their non-occurrence. The practical value of this paper consists in the fact that the proposed set of methods and means can be considered as an integral part of the track maintenance decision-making system. It can be easily adapted for online operation and integrated into the automated measurement system installed on a vehicle.\"}, {\"paperId\": \"2ea68e7aa26280ebff3bbe54f4e606a96685264d\", \"abstract\": \"Contemporary practitioners and scientists more and more frequently highlight the extraordinarily rapid process of implementation of new technologies \\u2013 including those based on artificial intelligence \\u2013 and unpredictable consequences of such actions. Therefore, it is important to be an active participant in the debate on the relation between human and modern technologies, a debate based on interdisciplinary scientific knowledge. The article refers to selected ideas related to knowledge management, organisational learning, knowledge area, or innovation environment. The challenge which social science researchers face, next to examining the theoretical aspects, is the application of various calculation methods and new technologies to make quicker and easier decisions in social contexts \\u2013 with regard to various groups of people, e.g. employees, customers, or voters. Apart from the new methods, another serious challenge is to raise social awareness regarding the digital responsibility in certain groups such as managers or, more generally, employers and employees. The responsibility of the elite and scientific authorities should consist in instilling awareness in one another and approaching the new phenomenon with care. Potential threats may completely change our civilisation. The presented discussion is based on literature study which included selected theories and reports of research centres and scientific bodies. A particularly interesting case study discussed in this article includes TOP CDR initiative and a report prepared by SW RESEARCH agency in cooperation with Procontent public relations and digital marketing agency. The conclusions of this report indicate that corporate digital responsibility (CDR) may be a pioneering area for in-depth empirical studies. The nature of the topic, despite being clearly related to DOI . /IJREL. . . . International Journal of Research in E-learning Vol. ( ), , pp. \\u2013 Ma\\u0142gorzata Suchacka 6 sociology, requires interdisciplinary approach and cooperation of numerous circles, not only scientific ones. K e y w o r d s: corporate social responsibility, corporate digital responsibility, technology, artificial intelligence Towards Corporate Digital Responsibility \\u2013 Future or Nowadays Challenges? Contemporary scientific authorities and, more and more frequently, political leaders highlight new kinds of threats to global labour market posed by automation and mass implementation of solutions based on artificial intelligence (AI). Development of new technologies, robotics, and process automation threatens current workplaces in both industry and the service sector. These processes may create social unrest, and their consequences are difficult to foresee due to the dynamic nature of their progression. The aim of the article is to characterise new challenges in corporate digital responsibility and new research areas which emerge in that field for social sciences. The author will identify certain theoretical aspects and potential consequences related to threats posed by the development of new technologies, artificial intelligence, automation, and digitalisation of social environment on a large scale. Selected thematically, relevant reports of scientific bodies, employers\\u2019 organisations, and companies collaborating with scientific circles will be analysed. The author will analyse in particular the TOP CDR initiative, which is the first project of this kind in Poland, focused on digitally responsible enterprises. A Few Words about Methods The analysis is based on theoretical considerations substantiated by selected research data. The theoretical themes referred to are part of the author\\u2019s selective attempt to indicate significant areas of possible future research. The scope of the analysis is largely based on literature study of selected concepts and is therefore significantly limited. All empirical remarks refer to existing data, reports of research centres and scientific bodies. The analysis will also include the TOP CDR report prepared by SW RESEARCH agency in cooperation with Procontent public relations and digital marketing agency. Corporate Digital Responsibility. New Challenges to the Social Sciences 7 Development of Technologies in the Field of Artificial Intelligence \\u2013 Responsibility and Challenges in Social Context For many years scientists and experts from virtually all scientific fields have been discussing the relationship between human and technology, which is developing at an increasing rate. These considerations include not only new ways of learning or human reaction to resulting changes in the reality, but also possible social processes which occur or will occur in the future due to technologisation and increasing presence of machines and robots in everyday life. The challenge which social science researchers face is the application of various calculation methods and new technologies to make quicker and easier decisions in social contexts with regard to various groups of people, e.g. employees, customers, or voters. This allows to gather data faster and identify digital traces of human activity \\u2013 either in social networks or in information obtained during behavioural studies using mobile phones. Access to this type of data provides the ability to stimulate various behaviours. Specialised software used on this kind of data has been perfected at an increasingly fast rate since the 1990s towards study of methods of AI operation. Initially, data compilation software was created to build databases using a specific type of reasoning mechanisms. Nearly 70 years have passed since the first widely recognised definition of artificial intelligence was presented by Alan Turing in 1950. At the time, artificial intelligence was understood, for the purpose of the conducted experiment, as an ability of a machine to perform cognitive tasks effectively without making human interrogator realise that that the respondent is a machine (Turing, 1950). Nowadays, programmers are focused on the creation of intelligent behaviour patterns which may be utilised in computer software. The goal is to develop a model allowing machine to imitate sophisticated human manifestations of intelligence: making decisions under uncertainty, analysis and synthesis of natural languages, conducting logical reasoning, diagnosis, expertise and participation in logic-based games such as chess. Machines already have achieved the ability to learn and perfect their behaviour on the basis of new experience. Using algorithms and specific data, the machines can, through the process of induction, transition from supervised learning to unsupervised learning (Russell & Norvig, 2003). More and more often, people are being replaced by machines, devices, and appropriate software, all through learning specific forms of response based on the output data. Nowadays, the ethical question should be a top priority for scientists, because many people might be really hurt by these algorithms (Suchacka & Hor\\u00e1kov\\u00e1, 2019, p. 917). Ma\\u0142gorzata Suchacka 8 This has specific consequences \\u2013 chances and threats. At the threshold of revolution initiated by introduction of artificial intelligence into various areas of socioeconomic life, an increasing number of socially sensitive practitioners and scientists call for the need to create a complex strategy of AI development. The analysis of selected AI development programmes conducted by Digital Poland Foundation in 2018 points to differences in approach to this matter between various countries. Depending on the government\\u2019s policy, emphasis is placed on retaining scientific leadership and development of basic research around AI (France), ensuring national security, order and monitoring behaviour of the citizens (China), maintaining leadership in robotics, increasing the level of industrialisation and supporting ageing society (Japan). The report characterises world\\u2019s most prominent centres of innovation and highlights the need to promote economy based on knowledge, cooperation, and sharing experience with the support of the regional and national level authorities (Digital Poland Foundation, 2018). The aware and responsible decision-makers should create conditions favourable for close integration of the worlds of science and business and accelerated commercialisation of the results of their cooperation. The responsibility of the elites and scientific authorities should consist in raising awareness about AI and taking the new phenomenon seriously. Potential threats may completely change our civilisation. The research conducted on the matter is still focused primarily on technical and IT issues, despite the fact that great minds of our times like Bill Gates, Elon Musk, and Stephen Hawking have been warning us against development of a model of artificial intelligence able to continuously improve itself. It is difficult to image what is becoming the reality \\u2013 machine surpassing human. The most controversial is the use of artificial intelligence in the army [armed forces], from rockets or jets to all kinds of infrastructure control systems. At this stage, it is assumed that people are in control, not threatened by computers deciding anything themselves. It will be this way until the artificial intelligence begins to modify its goals. Even if it is possible for the machine to become self-conscious, it will still have to set tasks for itself and find a justification for them, and that part is not immediately obvious. At the moment, intelligent technologies assist us with acquiring knowledge quickly, learning new behaviors outside the traditional system of education, which, however, should not be completely eliminated. A constant reflection, inherently sociological, should accompany these technological changes, for the algorithms behind ethical actions are the very traits of humanity and it is quite difficult to assume that the machine will accept them or will develop them itself without error of proper access path (Suc\"}, {\"paperId\": \"cc2a5a9d07e46484422cc7628486f31f4b2fe074\", \"abstract\": \"Adaptive and Natural Computing AlgorithmsAdvances in Knowledge Discovery and Data MiningLectures on Gaussian ProcessesAn introduction to continuity and related topics for general Gaussian ProcessesLarge Deviations for Gaussian QueuesGaussian Markov Random FieldsAsymptotic Methods in the Theory of Gaussian Processes and FieldsModelling and Control of Dynamic Systems Using Gaussian Process ModelsAn Introduction to Continuity, Extrema, and Related Topics for General Gaussian ProcessesThe Gaussian Approximation PotentialAn Introduction to Continuity, Extrema, and Related Topics for General Gaussian ProcessesStochastic Analysis for Gaussian Random Processes and FieldsMachine LearningNeural Networks and Machine LearningBayesian Time Series ModelsGraphical Models for Machine Learning and Digital CommunicationMarkov Processes, Gaussian Processes, and Local TimesThe Generic ChainingTime Series AnalysisProbability in Banach SpacesDark DataMachine LearningSurrogatesIntroduction and Implementations of the Kalman FilterMachine Learning and Knowledge Discovery in DatabasesLearning Kernel ClassifiersThe Concentration of Measure PhenomenonBayesian Data Analysis, Third EditionKernels for Vector-Valued FunctionsAdvanced Lectures on Machine LearningGaussian Processes for Machine LearningEfficient Reinforcement Learning Using Gaussian ProcessesGaussian Process Regression Analysis for Functional DataGaussian Processes on TreesBayesian Learning for Neural NetworksReinforcement Learning and Optimal ControlInterpolation of Spatial DataQuantum Processes Systems, and InformationHigh-Dimensional ProbabilityIntroduction to Empirical Processes and Semiparametric Inference The three volume proceedings LNAI 11906 \\u2013 11908 constitutes the refereed proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2019, held in W\\u00fcrzburg, Germany, in September 2019. The total of 130 regular papers presented in these volumes was carefully reviewed and selected from 733 submissions; there are 10 papers in the demo track. The contributions were organized in topical sections named as follows: Part I: pattern mining; clustering, anomaly and outlier detection, and autoencoders; dimensionality reduction and feature selection; social networks and graphs; decision trees, interpretability, and causality; strings and streams; privacy and security; optimization. Part II: supervised learning; multi-label learning; large-scale learning; deep learning; probabilistic models; natural language processing. Part III: reinforcement learning and bandits; ranking; applied data science: computer vision and explanation; applied data science: healthcare; applied data science: e-commerce, finance, and advertising; applied data science: rich data; applied data science: applications; demo track. Chapter \\\"Incorporating Dependencies in Spectral Kernels for Gaussian Processes\\\" is available open access under a Creative Commons Attribution 4.0 International License via link.springer.com.This book is devoted to a systematic analysis of asymptotic behavior of distributions of various typical functionals of Gaussian random variables and fields. The text begins with an extended introduction, which explains fundamental ideas and sketches the basic methods fully presented later in the book. Good approximate formulas and sharp estimates of the remainders are obtained for a large class of Gaussian and similar processes. The author devotes special attention to the development of asymptotic analysis methods, emphasizing the method of comparison, the double-sum method and the method of moments. The author has added an extended introduction and has significantly revised the text for this translation, particularly the material on the double-sum method.This monograph opens up new horizons for engineers and researchers in academia and in industry dealing with or interested in new developments in the field of system identification and control. It emphasizes guidelines for working solutions and practical advice for their implementation rather than the theoretical background of Gaussian process (GP) models. The book demonstrates the potential of this recent development in probabilistic machine-learning methods and gives the reader an intuitive understanding of the topic. The current state of the art is treated along with possible future directions for research. Systems control design relies on mathematical models and these may be developed from measurement data. This process of system identification, when based on GP models, can play an integral part of control design in data-based control and its description as such is an essential aspect of the text. The background of GP regression is introduced first with system identification and incorporation of prior knowledge then leading into full-blown control. The book is illustrated by extensive use of examples, line drawings, and graphical presentation of computer-simulation results and plant measurements. The research results presented are applied in real-life case studies drawn from successful applications including: a gas\\u2013liquid separator control; urban-traffic signal modelling and reconstruction; and prediction of atmospheric ozone concentration. A MATLAB\\u00ae toolbox, for identification and simulation of dynamic GP models is provided for download.Sensor data fusion is the process of combining error-prone, heterogeneous, incomplete, and ambiguous data to gather a higher level of situational awareness. In principle, all living creatures are fusing information from their complementary senses to coordinate their actions and to detect and localize danger. In sensor data fusion, this process is transferred to electronic systems, which rely on some \\\"awareness\\\" of what is happening in certain areas of interest. By means of probability theory and statistics, it is possible to model the relationship between the state space and the sensor data. The number of ingredients of the resulting Kalman filter is limited, but its applications are not.Kosorok\\u2019s brilliant text provides a self-contained introduction to empirical processes and semiparametric inference. These powerful research techniques are surprisingly useful for developing methods of statistical inference for complex models and in understanding the properties of such methods. This is an authoritative text that covers all the bases, and also a friendly and gradual introduction to the area. The book can be used as research reference and textbook.The two-volume set LNAI 12084 and 12085 constitutes the thoroughly refereed proceedings of the 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020, which was due to be held in Singapore, in May 2020. The conference was held virtually due to the COVID-19 pandemic. The 135 full papers presented were carefully reviewed and selected from 628 submissions. The papers present new ideas, original research results, and practical development experiences from all KDD related areas, including data mining, data warehousing, machine learning, artificial intelligence, databases, statistics, knowledge engineering, visualization, decision-making systems, and the emerging applications. They are organized in the following topical sections: recommender systems; classification; clustering; mining social networks; representation learning and embedding; mining behavioral data; deep learning; feature extraction and selection; human, domain, organizational and social factors in data mining; mining sequential data; mining imbalanced data; association; privacy and security; supervised learning; novel algorithms; mining multimedia/multi-dimensional data; application; mining graph and network data; anomaly detection and analytics; mining spatial, temporal, unstructured and semi-structured data; sentiment analysis; statistical/graphical model; multi-source/distributed/parallel/cloud computing.Isoperimetric, measure concentration and random\"}, {\"paperId\": \"eee63abdfa9e3456f85f31c4244ddc5750ea5790\", \"abstract\": \"Intelligent Personal Assistance are poised to become the primary and most significant Human Computer Interface in the near future. This is attributed to advancements in Artificial Intelligence, Machine Learning, Internet of Things, Natural Language Processing and Data Sciences. A range of generic as well as specialized IPAs are being researched and developed by industry. Despite security and privacy issues, the adoption and utility of these agents is increasing. This paper proposes a singular adaptive multi-role IPA (SAM-IPA) that goes beyond scheduling and search facilities to handling multidimensional IoT as well as application data. SAM-IPA will not only act as a singular HCI responsible for delegation of multifarious tasks on behalf of the user but will also increase awareness via IoT and adapt based on ML over BigData. The proposed SAM-IPA will leverage the application interface communication mechanisms and technology potentials in the foreseeable horizon thereby drawing concrete findings while identifying research areas needing deliberation.\"}, {\"paperId\": \"4da1b767ed29ec12babb8ff1b3a15412be770f6e\", \"abstract\": null}, {\"paperId\": \"cc346fd1a983540759d721575ed970d0a9669b57\", \"abstract\": \"From the Publisher: \\nInductive Logic Programming is a new research area situated in machine learning and logic programming, two subfields of artificial intelligence. The goal of inductive logic programming is to develop theories, techniques and tools for inducing hypotheses from observations using the representations from computational logic. Inductive Logic Programming has a high potential for applications in data mining, automated scientific discovery, knowledge discovery in databases, as well as automatic programming. This book provides a detailed state-of-the-art overview of Inductive Logic Programming as well as a collection of recent technical contributions to Inductive Logic Programming. The state-of-the-art overview is based on - among others - the succesful ESPRIT basic research project no. 6020 on Inductive Logic Programming, funded by the European Commission from 1992 till 1995. It highlights some of the most important recent results within Inductive Logic Programming and can be used as a thorough introduction to the field. This book is relevant to students, researchers and practitioners of artificial intelligence and computer science, especially those concerned with machine learning, data mining and computational logic.\"}, {\"paperId\": \"5fb83d9c8c76f5b175bb9a4638bbbd1c0931ca98\", \"abstract\": null}, {\"paperId\": \"621053fbd5d64afe6615b693feb48fbba41ee624\", \"abstract\": \"Sentient tools--powered by incredible advances in artificial intelligence, deep learning, and data mining--represent the next stage of intelligent, aware, and social machines designed specifically to work with people. The Web extra at https://youtu.be/XtMFZ0sDzIk is an audio recording in which Science Fiction Prototyping editor Brian David Johnson talks with Richard Sear, vice president of consulting at Frost and Sullivan, about the next stage of intelligent, aware, and social machines designed specifically to work with people.\"}, {\"paperId\": \"02c989cba99e41da3b79245ba6fc6bce7af5e45a\", \"abstract\": \"Abstract Plants, and the biological systems around them, are key to the future health of the planet and its inhabitants. The Plant Science Decadal Vision 2020\\u20132030 frames our ability to perform vital and far\\u2010reaching research in plant systems sciences, essential to how we value participants and apply emerging technologies. We outline a comprehensive vision for addressing some of our most pressing global problems through discovery, practical applications, and education. The Decadal Vision was developed by the participants at the Plant Summit 2019, a community event organized by the Plant Science Research Network. The Decadal Vision describes a holistic vision for the next decade of plant science that blends recommendations for research, people, and technology. Going beyond discoveries and applications, we, the plant science community, must implement bold, innovative changes to research cultures and training paradigms in this era of automation, virtualization, and the looming shadow of climate change. Our vision and hopes for the next decade are encapsulated in the phrase reimagining the potential of plants for a healthy and sustainable future. The Decadal Vision recognizes the vital intersection of human and scientific elements and demands an integrated implementation of strategies for research (Goals 1\\u20134), people (Goals 5 and 6), and technology (Goals 7 and 8). This report is intended to help inspire and guide the research community, scientific societies, federal funding agencies, private philanthropies, corporations, educators, entrepreneurs, and early career researchers over the next 10 years. The research encompass experimental and computational approaches to understanding and predicting ecosystem behavior; novel production systems for food, feed, and fiber with greater crop diversity, efficiency, productivity, and resilience that improve ecosystem health; approaches to realize the potential for advances in nutrition, discovery and engineering of plant\\u2010based medicines, and \\\"green infrastructure.\\\" Launching the Transparent Plant will use experimental and computational approaches to break down the phytobiome into a \\\"parts store\\\" that supports tinkering and supports query, prediction, and rapid\\u2010response problem solving. Equity, diversity, and inclusion are indispensable cornerstones of realizing our vision. We make recommendations around funding and systems that support customized professional development. Plant systems are frequently taken for granted therefore we make recommendations to improve plant awareness and community science programs to increase understanding of scientific research. We prioritize emerging technologies, focusing on non\\u2010invasive imaging, sensors, and plug\\u2010and\\u2010play portable lab technologies, coupled with enabling computational advances. Plant systems science will benefit from data management and future advances in automation, machine learning, natural language processing, and artificial intelligence\\u2010assisted data integration, pattern identification, and decision making. Implementation of this vision will transform plant systems science and ripple outwards through society and across the globe. Beyond deepening our biological understanding, we envision entirely new applications. We further anticipate a wave of diversification of plant systems practitioners while stimulating community engagement, underpinning increasing entrepreneurship. This surge of engagement and knowledge will help satisfy and stoke people's natural curiosity about the future, and their desire to prepare for it, as they seek fuller information about food, health, climate and ecological systems.\"}, {\"paperId\": \"3ed4ac25312ffbfb2df490ee128f1a82ddc9616e\", \"abstract\": \"We present a novel approach to fault detection and Physical Asset Health Management (PAHM) called Logical Analysis of Data (LAD). LAD is a supervised learning, artificial intelligence, data mining technique that possesses distinctive advantages which proved to be of use in PAHM. This approach has been introduced by a group of researchers at Rutgers University in the USA, in the medical field, and it was adapted and used in the field of PAHM for the first time by our group of researchers. Unlike the traditional approaches that are either based on mathematical and statistical models or neural network modelling, this approach is based only on the advancement in the field of computer science, namely the speed of computation and the volume of data that can be processed, and proceeds in a logical manner like the human brain. That is why its potential for future applications seems promising. LAD does not assume that the data belong to a specific statistical distribution and therefore does not require statistical analysis of data prior to or after its use. Unlike the statistically based techniques, correlations and dependence between features or variables do not have any effect on LAD's per formance. LAD can handle this phenomenon, and moreover, it gives physical explanation to it. Unlike other data analysis techniques, such as neural networks and support vector machines, LAD is a transparent method; the output of LAD can be traced back to the specific root causes that resulted in the categorization of a specific observation into a certain class. The interpretability of all the results and of all the steps of calculation is a clear advantage that maintenance experts can use in order to find answers to their questions. LAD detects and diagnoses complex phenomena such as equipment fault due to the interaction of multiple factors. Moreover, LAD is not based and does not need data on failure events. Thus, for physical assets that have a lifetime of 25 or 30 years, degradation can still be analyzed and maintenance decision can still be taken. During the learning process, LAD analyzes the collected data that represents readings of some physical asset's features such as vibration signals, oil analysis, temperature, etc..., which are taken while the asset is in different states, for example new, failing, deteriorating, etc... It then generates patterns that can be easily interpreted and translated into meaningful physical rules. LAD automatically extracts features and generates patterns from the readings and, accordingly, classifies the asset into a certain state based on the patterns generated. The learning function of cbmLAD results in an accumulation and preservation of expert knowledge that can be used at any time by the user, even if the human expertise is lost due to retirement or resignation. It thus becomes a powerful knowledge management tool that automates and conserves knowledge. In the testing process LAD can detect and diagnose the asset's state based on the generated knowledge during the learning process. This process of learning and pattern generation serve to reinforce the theoretical knowledge and uncover new knowledge about a certain diagnostic problem in PAHM. LAD has been applied successful in different situations that will be presented in the following sections.\"}, {\"paperId\": \"46fd480d4866c8081b668584ffa4c6bb07df92f5\", \"abstract\": \"Studies on human mobility have a long history with increasingly strong interdisciplinary connections across social science, environmental science, information and technology, computer science, engineering, and health science. However, what is lacking in the current research is a synthesis of the studies to identify the evolutional pathways and future research directions. To address this gap, we conduct a systematic review of human mobility-related studies published from 1990 to 2020. Drawing on the selected publications retrieved from the Web of Science, we provide a bibliometric analysis and network visualisation using CiteSpace and VOSviewer on the number of publications and year published, authors and their countries and afflictions, citations, topics, abstracts, keywords, and journals. Our findings show that human mobility-related studies have become increasingly interdisciplinary and multi-dimensional, which have been strengthened by the use of the so-called \\u2018big data\\u2019 from multiple sources, the development of computer technologies, the innovation of modelling approaches, and the novel applications in various areas. Based on our synthesis of the work by top cited authors we identify four directions for future research relating to data sources, modelling methods, applications, and technologies. We advocate for more in-depth research on human mobility using multi-source big data, improving modelling methods and integrating advanced technologies including artificial intelligence, and machine and deep learning to address real-world problems and contribute to social good.\"}, {\"paperId\": \"71a3bd6bb6b7c1757b15a364a7075f793d365b45\", \"abstract\": \"The knowledge discovery and data mining (KDD) field draws on findings from statistics, databases, and artificial intelligence to construct tools that let users gain insight from massive data sets. People in business, science, medicine, academia, and government collect such data sets, and several commercial packages now offer general-purpose KDD tools. An important KDD goal is to \\\"turn data into knowledge\\\". For example, knowledge acquired through such methods on a medical database could be published in a medical journal. Knowledge acquired from analyzing a financial or marketing database could revise business practice and influence a management school's curriculum. In addition, some US laws require reasons for rejecting a loan application, which knowledge from the KDD could provide. Occasionally, however, you must explain the learned decision criteria to a court, as in the recent lawsuit Blue Mountain filed against Microsoft for a mail filter that classified electronic greeting cards as spam mail. We expect more from knowledge discovery tools than simply creating accurate models as in machine learning, statistics, and pattern recognition. We can fully realize the benefits of data mining by paying attention to the cognitive factors that make the resulting models coherent, credible, easy to use, and easy to communicate to others.\"}, {\"paperId\": \"14bf863c83828a4febba95de8849ffe6b3d0eb32\", \"abstract\": null}, {\"paperId\": \"9428a1d5f1b86b5c53de0a08c6024e686a20a3fc\", \"abstract\": null}, {\"paperId\": \"d0bd0e1aeaac26cb022564c1d931a63649e08994\", \"abstract\": null}, {\"paperId\": \"25d966d3ba48c755ae9b9c8802988c97f7a42994\", \"abstract\": \"we present ELEMZ, a machine learning system that induces classification rules from a set of data based on a heuristic search over a hypothesis space. ELEM2 is distinguished from other rule induction systems in three aspects. First, it uses a new heuristic function to guide the heuristic search. The function reflects the degree of relevance of an attribute-value pair to a target concept and leads to selection of the most relevant pairs for formulating rules. Second, ELEM2 handles inconsistent training examples by defining an unlearnable region of a concept based on the probability distribution of that concept in the training data. The unlearnable region is used as a stopping criterion for the concept learning process, which resolves conflicts without removing inconsistent examples. Third, ELEMS employs a new rule quality measure in its post-pruning process to prevent rules from overfitting the data. The rule quality formula measures the extent to which a rule can discriminate between the positive and negative examples of a class. We describe features of ELEMZ, its rule induction algorithm and its classification procedure. We report experimental results that compare ELEMZ with C4.5 and CN2 on a number of datasets. @ 2003 Elsevier Science Ltd. All rights reserved. Keywords-Machine learning, Rule induction, Classification, Data mining, Artificial intelligence.\"}, {\"paperId\": \"c5a96be1a4a77b07ea7b056948d0e7d1022edc22\", \"abstract\": \"The objective of this paper is to provide guidelines on information security data visualization and insights with repeatable process and examples on visualizing (communicating) information security data. Security data visualization can be used in many areas in information security. Security metrics, Security monitoring, anomaly detection, forensics, and malware analysis are examples where security data visualization can play a vital role and make us better security professionals. Security data visualization also plays key role in emerging fields such as data science, machine learning, and exploratory data analytics. There are many uses for security data visualization; so, in order to cover key aspects the paper is categorized in to two parts. The first category is communicating value. There is a well-known proverb \\u201ca picture is worth a thousand words\\u201d (Piqua Leader-Dispatch, One Look Is Worth A Thousand Words, 1913, p. 2) which explains this. The problem with traditional metrics is numbers and tables can be daunting and details can be missed easily. Visualizing it will enable the security team to highlight the salient points in the data. Security data visualization enables you to tell a story with the data. Information security is becoming a common topic in boardroom discussions and it is becoming more and more important that the value of information security is communicated to business leaders. The second category is finding anomalies using security data visualization. One of the key strengths of security teams is access to enterprise log data, meta-data, network traffic data, and netflow data. The challenge is finding and isolating the bad actors from legitimate traffic. The human mind, by evolution, is trained to identify patterns and anomalies using visualization. Security professionals can benefit by visualizing enterprise data to find anomalies and identify patterns which will be helpful in isolating events which might indicate compromise. Hopefully some of the examples will be useful to generate more ideas in this space and will be a valuable resource for all Information Security practitioners. Once security professionals get an understanding of using security data visualization it will open a whole new world and there is a possibility that this knowledge of security data science will have significant improvement on information security tasks. Security Data Visualization pingbalaji@gmail.com 1.0 Introduction Security data visualization can be used in many areas in information security. Security metrics, Security monitoring, anomaly detection, forensics, and malware analysis are examples where security data visualization can play a vital role and make us better security professionals. Till now security professionals were able to survive with Microsoft Excel and similar tools without in-depth knowledge in security data visualization. But security data visualization is becoming extremely important due to big data, machine learning and exploratory data analytics. Due to the volume of data in big data it is extremely impossible to find anomalies using traditional methods. First thing to do after a statistical computation is to understand the data visually. Recent generations of SIEM log collection and correlation solutions use big data analytics. Security data visualization plays a very vital part in analyzing the big data. Data science field is evolving at a rapid pace. Data visualization is important component of data science. Botnet Visualization Microsoft\\u2019s Digital Crimes Unit tapped The Office for Creative Research, a multidisciplinary digital design group based in New York, to come up with new ways of looking at one particular threat: botnets, the global networks of infected computers that cyber criminals enlist to do their bidding. OCR came up with a prototype tool called Specimen Box. Specimen Box offers many views including live display of botnet activity \\u201cwhich can be used to analyze botnet data\\u201d (\\\"#005: The Sight and Sound of CyberCrime\\\", o-c-r.org, 2014, para. 3). Security Data Visualization pingbalaji@gmail.com Reverse Engineering Security data visualization is used more and more in reverse engineering. \\u201cIn this engaging TED(TED is a platform for ideas worth spreading http://www.ted.com/) talk, Chris Domas shows how researchers use pattern recognition and reverse engineering (and pull a few allnighters) using visualization to understand a chunk of binary code whose purpose and contents they don't know.\\u201d( Domas, C. (n.d.). The 1s and 0s behind cyber warfare. Retrieved December 15, 2014, from http://www.ted.com/talks/chris_domas_the_1s_and_0s_behind_cyber_warfare, para. 1) Currently the information security practitioners are just scratching the surface in this area, additional security data visualization magic is captured in Appendix A for inspiring and invoking the curiosity and awe in security practitioners for utilizing the full potential of security data visualization in information security day-to-day jobs. Hopefully some of the examples will be useful to generate more ideas in this space and will be a valuable skill for all Information Security practitioners. Once security practitioners get an understanding of using security data visualization it will open a whole new world and there is a possibility that this knowledge of security data science will have significant improvement on information security tasks. Security Data Visualization pingbalaji@gmail.com 2.0 Security Data Visualization Skills Data science and security visualization require the skills described in the Venn diagram. It is the space where the hacking skills, statistical knowledge and domain knowledge meet. (Conway, D. (n.d.). The Data Science Venn Diagram. Retrieved November 29, 2014, from http://drewconway.com/zia/2013/3/26/the-data-science-venn-diagram) Substantive Expertise \\u2013 This is the security domain knowledge, which will enable the security practitioner to understand the data, determine what is expected and find anomalies or metrics from visualization. Hacking Skills \\u2013 Hacking skills are the skills from a data scientist language required for working with massive amount of data that should be acquired, cleaned and sanitized. Math & Statistics Knowledge \\u2013 This knowledge is critical to understand which tools to use, understand the spread and other characteristics to derive insight from the data. Security Data Visualization pingbalaji@gmail.com Security practitioners will be comfortable with domain knowledge and hacking skills. Statistics knowledge is one aspect that security practitioners have to understand to gain insight from data and also to ask the right questions to derive the right security visualization. One resource for statistical knowledge is an online free course \\u201cData to Insight: An Introduction to Data Analysis\\u201d (\\\"Data to Insight: An Introduction to Data Analysis The University of Auckland FutureLearn\\\", 2014). This course is a hands-on introduction to statistical data analysis that emphasizes fundamental concepts and practical skills. This course also introduces the tool iNZight. One aspect google looks at while recruiting engineers is their knowledge on statistics and probability. The reason might be that they need people who understand the basics in deriving value from data. Using statistics, probability in combination with machine learning/artificial intelligence there are lot of predictions based on the data in various fields. Data science field is evolving at a rapid pace. Data visualization is important component of data science. These techniques will soon be applied to information security field for better identification of bad actors. One of the most important advantages of data visualization is that all the resources on data visualization are publicly available for learning the key concepts. There are numerous Coursera and eDX courses available for free about data visualization. There is extensive material about R project with numerous examples from various experts. If enough time is dedicated, data visualization tools and R can be learned easily by security analysts. Security Data Visualization pingbalaji@gmail.com The key advantage for security analysts is that security analysts have access to security data like security metrics data, network traffic data, malware indicators of compromise data, and many more. Security domain expertise is very important before starting data visualization, starting with the right question and the domain expertise will enable to get good output using data visualization. By using data visualization techniques on security data, security analysts can gain be valuable insights on metrics and anomaly detection. Hopefully these insights can make security practitioners jobs easier. Security Data Visualization pingbalaji@gmail.com 3.0 Security Data Visualization Process At a very high level the security visualization process consists of below five steps: (Security Data Visualization process) The key steps involved in visualization are Step 1 \\u2013 Visualization Goals Step 2 Data Preparation phase Step 3 Exploration phase Step 4 Visualization phase Step 5 Feedback and fine-tune Visualization Goals\"}, {\"paperId\": \"7223965ba896ff82273c76fa8b2a94bee88cc9b0\", \"abstract\": \"The ability to retrieve accurate information from databases without an extensive knowledge of the contents and organization of each database is extremely beneficial to the dissemination and utilization of freight data. Advances in the artificial intelligence and information sciences provide an opportunity to develop query capturing algorithms to retrieve relevant keywords from freight-related natural language queries. The challenge is correctly identifying and classifying these keywords. On their own, current natural language processing algorithms are insufficient in performing this task for freight-related queries. High performance machine learning algorithms also require an annotated corpus of named entities which currently does not exist in the freight domain. This paper proposes a hybrid named entity recognition approach which draws on the individual strengths of models to correctly identify entities. The hybrid approach resulted in a greater precision for named entity recognition of freight entities-a key requirement for accurate information retrieval from freight data sources.\"}, {\"paperId\": \"1d14fc90040ffd6f2222d2eed6bbc12a8eef908c\", \"abstract\": \"Sometime next year, managers at the US Department of Energy\\u2019s (DOE) Argonne National Laboratory in Lemont, IL, will power up a calculating machine the size of 10 tennis courts and vault the country into a new age of computing. The $500-million mainframe, called Aurora, could become the world\\u2019s first \\u201cexascale\\u201d supercomputer, running an astounding 1018, or 1 quintillion, operations per second.\\n\\n\\n\\nRows of cabinets hold incredible processing power for one of the world's best supercomputers, Summit, at Oak Ridge National Laboratory in TN. Exascale computing will surpass these existing computers by leaps and bounds. Image credit: Flickr/Oak Ridge National Laboratory, licensed under CC BY 2.0.\\n\\n\\n\\nAurora is expected to have more than twice the peak performance of the current supercomputer record holder, a machine named Fugaku at the RIKEN Center for Computational Science in Kobe, Japan. Fugaku and its calculation kin serve a vital function in modern scientific advancement, performing simulations crucial for discoveries in a wide range of fields. But the transition to exascale will not be easy. \\u201cAs these machines grow, they become harder and harder to exploit efficiently,\\u201d says Danny Perez, a physicist at Los Alamos National Laboratory in NM. \\u201cWe have to change our computing paradigms, how we write our programs, and how we arrange computation and data management.\\u201d\\n\\nThat\\u2019s because supercomputers are complex beasts, consisting of cabinets containing hundreds of thousands of processors. For these processors to operate as a single entity, a supercomputer needs to pass data back and forth between its various parts, running huge numbers of computations at the same time, all while minimizing power consumption. Writing programs for such parallel computing is not easy, and theorists will need to leverage new tools such as machine learning and artificial intelligence to make scientific breakthroughs. Given these challenges, researchers have \\u2026\"}, {\"paperId\": \"5fdc8c4d8a5bef3320e185c59bbb80e8e15805c8\", \"abstract\": \"Over the years, collaborative mobility proved to be an important but challenging component of the smart cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of what-if decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment.\"}, {\"paperId\": \"0529bf255413205a0f6b45e3c8da5513fd119c37\", \"abstract\": null}, {\"paperId\": \"ff6715de6cc057cdc620ed1bedf6f87726479b82\", \"abstract\": \"Rough sets have been proposed for a very wide variety of applications. In particular, the rough set approach seems to be important for Artificial Intelligence and cognitive sciences, especially in machine learning, knowledge discovery, data mining, expert systems, approximate reasoning. The problem of imperfect knowledge has been tackled for a long time by philosophers, logicians and mathematicians. Recently it became also a crucial issue for computer scientists, particularly in the area of Artificial Intelligence. There are many approaches to the problem of how to understand and manipulate imperfect knowledge. Rough set theory has more advantage than fuzzy set and any other theory like probability theory etc. This paper presents some Rough Set theory concept and its applications over various fields.\"}, {\"paperId\": \"ff44ec94c322aac281296163e6f10983de61ad38\", \"abstract\": \"Abstract Aim To enable a world-leading research dataset of routinely collected clinical images linked to other routinely collected data from the whole Scottish national population. This includes more than 30 million different radiological examinations from a population of 5.4 million and >2 PB of data collected since 2010. Methods Scotland has a central archive of radiological data used to directly provide clinical care to patients. We have developed an architecture and platform to securely extract a copy of those data, link it to other clinical or social datasets, remove personal data to protect privacy, and make the resulting data available to researchers in a controlled Safe Haven environment. Results An extensive software platform has been developed to host, extract, and link data from cohorts to answer research questions. The platform has been tested on 5 different test cases and is currently being further enhanced to support 3 exemplar research projects. Conclusions The data available are from a range of radiological modalities and scanner types and were collected under different environmental conditions. These real-world, heterogenous data are valuable for training algorithms to support clinical decision making, especially for deep learning where large data volumes are required. The resource is now available for international research access. The platform and data can support new health research using artificial intelligence and machine learning technologies, as well as enabling discovery science.\"}, {\"paperId\": \"1212d459cc36366e3613fd17465f908d7eb47cc2\", \"abstract\": null}, {\"paperId\": \"59d4fe962e34ea2b80c55b5ffbfc000cbff61de1\", \"abstract\": \"Automatic cloud recognition promises significant improvements in Earth science remote sensing. At any time, more than half of Earth's surface is covered by clouds, obscuring images and atmospheric measurements. This is particularly problematic for CubeSats, a new generation of small, low\\u2010orbiting spacecraft with very limited communications bandwidth. Such spacecraft can use image analysis to autonomously select clear scenes for prioritized downlink. More agile spacecraft can also benefit from cloud screening by retargeting observations to cloud\\u2010free areas. This could significantly improve the science yield of instruments such as the Orbiting Carbon Observatory 3 mission. However, most existing cloud detection algorithms are not suitable for these applications, because they require calibrated and georectified spectral data, which is not typically available onboard. Here, we describe a statistical machine\\u2010learning method for real\\u2010time autonomous scene interpretation using a visible camera with no radiometric calibration. A random forest classifies cloud and clear pixels based on local patterns of image texture. We report on experimental evaluation of images from the International Space Station (ISS) and present results from a deployment onboard the IPEX spacecraft. This demonstrates actual execution in flight and provides some preliminary lessons learned about operational use. It is a rare example of a machine\\u2010learning system deployed to an autonomous spacecraft. To our knowledge, it is also the first instance of significant artificial intelligence deployed on board a CubeSat and the first ever deployment of visible image\\u2010based cloud screening onboard any operational spacecraft.\"}, {\"paperId\": \"c6b50daee0a95fe3e1b1d17ae3875f63ecd22887\", \"abstract\": \"Embodied voice-based agents, such as Amazon Echo, Google Home, and Jibo, are becoming increasingly present in the home environment. For most people, these agents represent their first experience living with artificial intelligence in such private and personal spaces. However, little is known about people's desires, preferences, and boundaries for these technologies. This thesis shares insights, learnings, methods, and tools from ajourney with 69 children, adults, and older adults to help democratize the design of voice-based agents for the home. In the first study, participants interact with and discover various voice-based agents to capture first impressions of the technology. In the second study, participants engage in longterm encounters with agents in their home to experience and reflect upon their preferences, desires and boundaries for these devices. Qualitative and quantitative data from interview transcripts, card sorting, and deployed cultural and technology probes is used to identify agent action preferences, sociotechnical themes, daily usage trends, personality preferences, and future \\\"wishes\\\" for agents. This work culminates with participants designing their dream agents for the home through a structured ideation process. Throughout this work, a series of participatory design tools and methods are developed, iterated upon, and implemented to create a language of engagement with participants. These methods and tools are shared as an open-source design kit for others seeking to explore the domain. Cynthia Breazeal, Thesis Supervisor Associate Professor of Media Arts & Sciences\"}, {\"paperId\": \"fee1e72a23203f00943db68ba9c0e444e12097aa\", \"abstract\": \"Hand it to you Our ability to grab, hold, and manipulate objects involves our dexterous hands, our sense of touch, and feedback from our eyes and muscles that allows us to maintain a controlled grip. Billard and Kragic review the progress made in robotics to emulate these functions. Systems have developed from simple, pinching grippers operating in a fully defined environment, to robots that can identify, select, and manipulate objects from a random collection. Further developments are emerging from advances in computer vision, computer processing capabilities, and tactile materials that give feedback to the robot. Science, this issue p. eaat8414 BACKGROUND Humans have a fantastic ability to manipulate objects of various shapes, sizes, and materials and can control the objects\\u2019 position in confined spaces with the advanced dexterity capabilities of our hands. Building machines inspired by human hands, with the functionality to autonomously pick up and manipulate objects, has always been an essential component of robotics. The first robot manipulators date back to the 1960s and are some of the first robotic devices ever constructed. In these early days, robotic manipulation consisted of carefully prescribed movement sequences that a robot would execute with no ability to adapt to a changing environment. As time passed, robots gradually gained the ability to automatically generate movement sequences, drawing on artificial intelligence and automated reasoning. Robots would stack boxes according to size, weight, and so forth, extending beyond geometric reasoning. This task also required robots to handle errors and uncertainty in sensing at run time, given that the slightest imprecision in the position and orientation of stacked boxes might cause the entire tower to topple. Methods from control theory also became instrumental for enabling robots to comply with the environment\\u2019s natural uncertainty by empowering them to adapt exerted forces upon contact. The ability to stably vary forces upon contact expanded robots\\u2019 manipulation repertoire to more-complex tasks, such as inserting pegs in holes or hammering. However, none of these actions truly demonstrated fine or in-hand manipulation capabilities, and they were commonly performed using simple two-fingered grippers. To enable multipurpose fine manipulation, roboticists focused their efforts on designing humanlike hands capable of using tools. Wielding a tool in-hand became a problem of its own, and a variety of advanced algorithms were developed to facilitate stable holding of objects and provide optimality guarantees. Because optimality was difficult to achieve in a stochastic environment, from the 1990s onward researchers aimed to increase the robustness of object manipulation at all levels. These efforts initiated the design of sensors and hardware for improved control of hand\\u2013object contacts. Studies that followed were focused on robust perception for coping with object occlusion and noisy measurements, as well as on adaptive control approaches to infer an object\\u2019s physical properties, so as to handle objects whose properties are unknown or change as a result of manipulation. ADVANCES Roboticists are still working to develop robots capable of sorting and packaging objects, chopping vegetables, and folding clothes in unstructured and dynamic environments. Robots used for modern manufacturing have accomplished some of these tasks in structured settings that still require fences between the robots and human operators to ensure safety. Ideally, robots should be able to work side by side with humans, offering their strength to carry heavy loads while presenting no danger. Over the past decade, robots have gained new levels of dexterity. This enhancement is due to breakthroughs in mechanics with sensors for perceiving touch along a robot\\u2019s body and new mechanics for soft actuation to offer natural compliance. Most notably, this development leverages the immense progress in machine learning to encapsulate models of uncertainty and support further advances in adaptive and robust control. Learning to manipulate in real-world settings is costly in terms of both time and hardware. To further elaborate on data-driven methods but avoid generating examples with real, physical systems, many researchers use simulation environments. Still, grasping and dexterous manipulation require a level of reality that existing simulators are not yet able to deliver\\u2014for example, in the case of modeling contacts for soft and deformable objects. Two roads are hence pursued: The first draws inspiration from the way humans acquire interaction skills and prompts robots to learn skills from observing humans performing complex manipulation. This allows robots to acquire manipulation capabilities in only a few trials. However, generalizing the acquired knowledge to apply to actions that differ from those previously demonstrated remains difficult. The second road constructs databases of real object manipulation, with the goal to better inform the simulators and generate examples that are as realistic as possible. Yet achieving realistic simulation of friction, material deformation, and other physical properties may not be possible anytime soon, and real experimental evaluation will be unavoidable for learning to manipulate highly deformable objects. OUTLOOK Despite many years of software and hardware development, achieving dexterous manipulation capabilities in robots remains an open problem\\u2014albeit an interesting one, given that it necessitates improved understanding of human grasping and manipulation techniques. We build robots to automate tasks but also to provide tools for humans to easily perform repetitive and dangerous tasks while avoiding harm. Achieving robust and flexible collaboration between humans and robots is hence the next major challenge. Fences that currently separate humans from robots will gradually disappear, and robots will start manipulating objects jointly with humans. To achieve this objective, robots must become smooth and trustable partners that interpret humans\\u2019 intentions and respond accordingly. Furthermore, robots must acquire a better understanding of how humans interact and must attain real-time adaptation capabilities. There is also a need to develop robots that are safe by design, with an emphasis on soft and lightweight structures as well as control and planning methodologies based on multisensory feedback. Holding two objects in one hand requires dexterity. Whereas a human can grab multiple objects at the same time (top), a robot (bottom) cannot yet achieve such dexterity. In this example, a human has placed the objects in the robot\\u2019s hand. PHOTOS: LEARNING ALGORITHMS AND SYSTEMS LABORATORY, EPFL Dexterous manipulation is one of the primary goals in robotics. Robots with this capability could sort and package objects, chop vegetables, and fold clothes. As robots come to work side by side with humans, they must also become human-aware. Over the past decade, research has made strides toward these goals. Progress has come from advances in visual and haptic perception and in mechanics in the form of soft actuators that offer a natural compliance. Most notably, immense progress in machine learning has been leveraged to encapsulate models of uncertainty and to support improvements in adaptive and robust control. Open questions remain in terms of how to enable robots to deal with the most unpredictable agent of all, the human.\"}, {\"paperId\": \"9848b6a17eb7d413df947b28622ccdb175397802\", \"abstract\": null}, {\"paperId\": \"e6c08ed103be4ac3a143ebee2226bf6a2553b73e\", \"abstract\": \"While the field of computer vision drives many of todays digital technologies and communication networks, the topic of color has emerged only recently in most computer vision applications. One of the most extensive works to date on color in computer vision, this book provides a complete set of tools for working with color in the field of image understanding.Based on the authors intense collaboration for more than a decade and drawing on the latest thinking in the field of computer science, the book integrates topics from color science and computer vision, clearly linking theories, techniques, machine learning, and applications. The fundamental basics, sample applications, and downloadable versions of the software and data sets are also included. Clear, thorough, and practical, Color in Computer Vision explains:Computer vision, including color-driven algorithms and quantitative results of various state-of-the-art methodsColor science topics such as color systems, color reflection mechanisms, color invariance, and color constancyDigital image processing, including edge detection, feature extraction, image segmentation, and image transformationsSignal processing techniques for the development of both image processing and machine learningRobotics and artificial intelligence, including such topics as supervised learning and classifiers for object and scene categorization Researchers and professionals in computer science, computer vision, color science, electrical engineering, and signal processing will learn how to implement color in computer vision applications and gain insight into future developments in this dynamic and expanding field.\"}, {\"paperId\": \"9ea75ece7acd8e26022bf21b6f2c0a8e55507f4f\", \"abstract\": \"This book seeks to develop an answer to the major question arising from the adoption of sophisticated data-science approaches within humanities research: are existing humanities methods compatible with computational thinking? Data-based and algorithmically powered methods present both new opportunities and new complications for humanists. This book takes as its founding assumption that the exploration and investigation of texts and data with sophisticated computational tools can serve the interpretative goals of humanists. At the same time, it assumes that these approaches cannot and will not obsolete other existing interpretive frameworks. Research involving computational methods, the book argues, should be subject to humanistic modes that deal with questions of power and infrastructure directed toward the field\\u2019s assumptions and practices. Arguing for a methodologically and ideologically self-aware critical digital humanities, the author contextualizes the digital humanities within the larger neo-liberalizing shifts of the contemporary university in order to resituate the field within a theoretically informed tradition of humanistic inquiry. Bringing the resources of critical theory to bear on computational methods enables humanists to construct an array of compelling and possible humanistic interpretations from multiple dimensions\\u2014from the ideological biases informing many commonly used algorithms to the complications of a historicist text mining, from examining the range of feature selection for sentiment analysis to the fantasies of human subjectless analysis activated by machine learning and artificial intelligence.\"}, {\"paperId\": \"951361f213a04b5ba9377130fc66ef9ad2898d3d\", \"abstract\": null}, {\"paperId\": \"430fdd8ee081ad3be05be60a77579a780cfd6cc2\", \"abstract\": \"Multi-component and high-entropy nitrides are a growing field with a promise of new functional materials. The interest in the field was sparked by the adjacent field of high-entropy and multi-component alloys, and the promise consists of both demonstrated properties and a possibly very large freedom for materials design. These promises, however, also come with new challenges connected to the vast available experimental space, which is inherent in multi-component materials. Traditional materials science methodologies will be slow to make appreciable progress in such an environment. A novel approach is needed to meet the challenges of the hyperdimensional compositional space. Recent developments within the fields of information technology can give materials science the tools needed. This Perspective article summarizes the state of the art in the field of multi-component nitride materials, focusing on coatings where solid solution phases with simple crystal structures are formed. Furthermore, it outlines the present research challenges that need to be addressed to move the field forward and suggests that there is a need to combine the traditional knowledge-driven materials science methodology with new data-driven methodologies. The latter would include advanced data-handling with artificial intelligence and machine learning to assist in the evaluation of large, shared datasets from both experimental and theoretical work. Such a change in the methodology will be a challenge but will be needed in order to fully realize the full potential of multi-component (nitride) materials.\"}, {\"paperId\": \"36e499c1a637ee76b76a840c8a69f33962cd74b9\", \"abstract\": \"Journal of Soft Computing and Decision Support Systems (eISSN: 2289-8603)\\u00a0has a wide scope by covering almost all the engineering and science applications of soft computing and decision support systems. The journal also publishes the papers related to the application of soft computing techniques for implementing and evaluating the decision support systems. It invites only original and interesting contributions from worldwide authors throughout the year.\\u00a0Journal of Soft Computing and Decision Support Systems publishes accepted paper through six regular issues in a year.\\u00a0The JSCDSS is published bi-monthly (Feb, Apr, Jun, Aug, Oct, Dec). Journal of Soft Computing and Decision Support Systems is open to all aspects of soft computing and decision support systems. Major fields of interest include Artificial Intelligence and Decision Support Systems Clinical Decision Support Systems Data Mining and Decision Support Systems Decision Support Systems and Big Data Decision Support Systems and Intelligent Systems Decision Support Systems for Complex Engineering Problems Decision Support Systems in Healthcare Electronic Commerce and Mobile Commerce Electronic Government Electronic Learning and Mobile Learning Expert Systems and Decision Support Systems Fuzzy Systems and Neural Networks Hybrid Intelligent Systems Hybrid Systems Using Neural Networks, Evolutionary Algorithms and Fuzzy Systems Industrial Engineering and Operations Research Intelligent Decision Support in Healthcare Information Systems and Services Intelligent Tutoring Systems Knowledge Management Systems Machine Learning and Decision Support Systems Mathematical Modeling and Decision Support Systems Multi-Criteria Decision-Making Technique for Engineering Problems Multi-Criteria Optimization Nano-Technology Modelling Soft Computing and Pattern Recognition Soft Computing \\u00a0and Decision Support Systems Soft Computing and Big Data Soft Computing and Internet Modeling, Communication and Networking, and Web Mining Soft Computing and Recommender Systems Soft Computing in Web Information Retrieval Web-based and Mobile Decision Support Systems\"}, {\"paperId\": \"59095fea05aad75ba462f7cfb30d57fba7be8e94\", \"abstract\": \"Today's audit profession is driving exciting and unprecedented changes that are fundamentally evolving the role of the auditor and how audits are performed. Breakthrough innovations in areas such as artificial intelligence, workflow automation, and data analytics are eliminating a number of the tedious and labor-intensive manual processes traditionally associated with an audit. More importantly, innovation is enabling auditors to deliver powerful insights that simply weren't possible before. These changes can enhance audit quality and deliver higher value for audit stakeholders--from clients and audit professionals to investors and the capital markets as a whole. (See the article \\\"How to Enable Audit Innovation,\\\" page 33, for details on how Deloitte nurtures in-house ideas to innovate audits.) INSIGHTS ENHANCE QUALITY Automation and other cutting-edge innovations reduce the amount of manual and time-consuming data collection required for an audit. But that's just the beginning. An even bigger benefit of audit innovation is the ability to generate new kinds of insights that increase the value of an audit and bring audit quality to a new level. Powered by innovative technologies and supported by a risk-based methodology, auditors now have more resources, tools, and time to strategically apply their most important skills--professional skepticism and judgment--to business issues, controls, and risks. What's more, auditors are armed with advanced analytical tools to provide deeper insights, including areas beyond the limits of a more traditional audit. For example, using the latest technologies, auditors can analyze complete data sets rather than samples. Advanced tools can be applied to all of a company's contracts related to an area of audit interest, or to metadata about an automated key control. This can reduce audit risk by making it less likely for an unusual transaction to slip through the cracks. Also, given the transformational nature of advanced technologies and analytics, innovative audit tools can readily reveal valuable insights about a business for clients to consider, such as operational inefficiencies and areas for potential improvement. INNOVATIONS RESHAPE THE AUDIT Technology and innovation are advancing at breakneck speed with unprecedented computing power to transform the audit. These advanced technologies, sometimes referred to as \\\"exponentials,\\\" represent technological breakthroughs at the intersection of information technology and science, and they are increasingly a driving force behind audit innovation. Here are some examples of how these exponential technologies and other forms of innovation are powering audits forward and promise a bright future for audit professionals: Artificial intelligence Artificial intelligence (AI) involves the theory and development of computer systems able to perform tasks that normally require human intelligence. Because AI technologies (also called cognitive technologies) can tackle many tasks performed traditionally by humans, they can enable an audit to avoid the typical trade-offs between speed and quality. Two AI technologies that are especially relevant to audit are natural language processing (NLP), which enables a system to read and understand key concepts in electronic documents, and machine learning, which enables a system to improve itself without being reprogrammed. As audit evidence increasingly becomes more digitized, these technologies, combined with workflow automation, enable auditors to do significantly more analysis in less time. This can allow auditors to spend more time on tasks that add more value to the audit. Workflow automation Through the creative use of technology, many audit activities that previously required time-consuming manual processing by auditors can now be automated. As a result, much of an audit's tedium can be reduced and enable analysis that is faster and more comprehensive. \\u2026\"}, {\"paperId\": \"b1cabd768bc49ae55b0c8bfffb7252c15d3ea039\", \"abstract\": \"The papers in these Proceedings were presented at the Seventeenth ACM Conference on Economics and Computation (EC'16), held July 24-28, 2016 in Maastricht, The Netherlands. Since 1999 the ACM Special Interest Group on Electronic Commerce (SIGecom) has sponsored EC, the leading scientific conference on advances in theory, systems, and applications at the interface of economics and computation, including applications to electronic commerce. The papers were selected by the program committee from among 242 submissions that were received by February 23, 2016. Paper submissions were invited in the following three non-exclusive focus areas: \\nTF: Theory and Foundations \\nAI: Artificial Intelligence and Applied Game Theory \\nEA: Experimental, Empirical, and Applications \\n \\n \\n \\nThe call for papers attracted 242 distinct submissions. Each paper was reviewed by at least three program committee members and two senior program committee members on the basis of significance, scientific novelty, technical quality, readability, and relevance to the conference. Following the tradition of recent iterations of the conference, the authors were asked to align their submission with one or two of the tracks. Of the total of 242 submissions, 136 indicated TF track, of which 48 were accepted, 29 indicated AI track, of which 10 were accepted, 18 indicated EA track, of which 5 were accepted, and 59 papers indicated two tracks, of which 17 papers were accepted. 44 of the accepted papers are published in these Proceedings. For the remaining 36, at the authors' request, only abstracts are included along with pointers to full working papers that the authors guarantee to be reliable for at least two years. This option accommodates the practices of fields outside of computer science in which conference publishing can preclude journal publishing. We expect that many of the papers in these Proceedings will appear in a more polished and complete form in scientific journals in the future. \\n \\nPapers were presented in parallel sessions with the exception of a plenary session for the following award-winning papers: \\nDeferred Acceptance with Compensation Chains, Piotr Dworczak (Best Paper with Student Lead Author Award) \\nWhich Is the Fairest (Rent Division) of Them All?, Kobi Gal, Moshe Mash, Ariel Procaccia and Yair Zick (Best Paper Award) \\n \\ntogether with the SIGecom Doctoral Dissertation Award talk by Inbal Talgam-Cohen. \\n \\nTo emphasize commonalities among the problems studied at EC, and to facilitate interchange at the conference, sessions were organized by topic rather than by focus area, and no indication of a paper's focus area(s) was given at the conference or appears in these proceedings. \\n \\nEC'16 featured the following plenary talks: \\nDynamic Pricing in a Labor Market: Surge Pricing and Flexible Work on the Uber Platform, by Keith Chen \\nIntrinsic Robustness of the Price of Anarchy, by Tim Roughgarden (Kalai Prize) \\nACM SIGecom Test of Time Award talk, by Michael Trick and Craig Tovey \\n \\nas well as a poster session that included 30 papers. \\n \\nIn addition to the main technical program, EC'16 also featured three workshops: \\nThe 2nd Workshop on the Interface between Algorithmic Game Theory and Data Sciences, organized by Richard Cole, Brad Larsen, Kevin Leyton-Brown, Balasubramanian Sivan, and Vasilis Syrgkanis \\nWorkshop on Economics of Cloud Computing, organized by Nikhil Devanur \\nThe 12th Workshop on Ad Auctions, organized by Nicole Immorlica, Hamid Nazerzadeh, and Sergei Vassilvitskii \\n \\n \\n \\nand five tutorials: \\nPractical Computation in Finite Games using Gambit, Game Theory Explorer, and SageMath, presented by Theodore Turocy, Albert Xin Jiang, Vincent Knight, Kevin Leyton-Brown, Rahul Savani, and Bernhard von Stengel \\nAlgorithmic Game Theory and Data Science, presented by Jamie Morgenstern and Vasilis Syrgkanis \\nElicitation and Machine Learning, presented by Rafael Frongillo and Bo Waggoner \\nDesign and Implementation of Combinatorial Prediction Markets, presented by Sebastien Lahaie and Miroslav Dudik \\nComputer Poker, presented by Sam Ganzfried and Marc Lanctot\"}, {\"paperId\": \"3d0eb69c024fea391d13e61b87b8af0d3a7145ca\", \"abstract\": null}, {\"paperId\": \"635fa87944549d4a31c8fbf4ca36e1b33478c329\", \"abstract\": \"This article sets out our perspective on how to begin the journey of decolonising computational fields, such as data and cognitive sciences. We see this struggle as requiring two basic steps: a) realisation that the present-day system has inherited, and still enacts, hostile, conservative, and oppressive behaviours and principles towards women of colour (WoC); and b) rejection of the idea that centering individual people is a solution to system-level problems. The longer we ignore these two steps, the more \\\"our\\\" academic system maintains its toxic structure, excludes, and harms Black women and other minoritised groups. This also keeps the door open to discredited pseudoscience, like eugenics and physiognomy. We propose that grappling with our fields' histories and heritage holds the key to avoiding mistakes of the past. For example, initiatives such as \\\"diversity boards\\\" can still be harmful because they superficially appear reformatory but nonetheless center whiteness and maintain the status quo. Building on the shoulders of many WoC's work, who have been paving the way, we hope to advance the dialogue required to build both a grass-roots and a top-down re-imagining of computational sciences -- including but not limited to psychology, neuroscience, cognitive science, computer science, data science, statistics, machine learning, and artificial intelligence. We aspire for these fields to progress away from their stagnant, sexist, and racist shared past into carving and maintaining an ecosystem where both a diverse demographics of researchers and scientific ideas that critically challenge the status quo are welcomed.\"}, {\"paperId\": \"2e2bfc4d9b1f2f8f8c9a223310030cdc20a54eb2\", \"abstract\": null}, {\"paperId\": \"0e1f4c4eda72de82367adae82cb92363ab834100\", \"abstract\": null}, {\"paperId\": \"646edcce300891224ddb0cc0fa3a38826e21ad3c\", \"abstract\": null}, {\"paperId\": \"199d8e0c1aa02abfd8a0b2eebc310c109035f8e7\", \"abstract\": null}, {\"paperId\": \"851c88708fee59194624d090589b89b4f4d356d7\", \"abstract\": \"Sign posts regarding the transition from volume to value are ubiquitous in health care. Current efforts to create value are guided by the triple aim: improve patient outcomes and enhance patient experience while decreasing the cost of care (1). This has prompted a shift toward population health management (PHM), which leverages data-driven technology, effective change management, and workflow optimization to promote sustainable care delivery models (2). Meaningful achievement of these goals represents the greatest challenge of our health care era. Radiology as a profession is no stranger to transformation and reinvention. The field has successfully navigated discrete epochs in its history beginning with Imaging 1.0, a period of discovery from 1920 to 1990 initiated by Wilhelm Roentgen and culminating in a technical revolution with advent of CT and MRI. The widespread picture archiving and communication system implementation in the 1990s ignited a dramatic surge in efficiency and productivity, and the profession flourished as a revenue center in a fee-for-service payment system. However, this era is referred to as the dark era of Imaging 2.0 because of the unintended consequences of increased isolation from our patients, health care teams, and referring providers. In recognition of this void in the evolving practice of radiology, the Imaging 3.0 campaign was launched to foster collaboration between radiologists, patients, referring physicians, and other key stakeholders in the pursuit of appropriate, evidence-based imaging and management (3). Born in a military setting, over-the-horizon radar systems were developed during the Cold War to improve safety by increasing time available for protective action by detecting threats long before traditional radar or line of sight. The rapidly changing health care environment and advent of technologies evoke an analogous need and vision for radiology. It is thus appropriate to speculate about the next era to shape our collective preparation and response. The breakneck pace of advances in data science and the inability of the legacy health care system to meet increasingly complex demands has led many thought leaders in the field to identify artificial intelligence (AI) as this next frontier. In general terms, AI allows machines to process information, recognize patterns, and refine existing models to learn, solve problems, and perform specific tasks. This iterative process relies on machine learning, which leverages self-correcting algorithms to interpret vast amounts of data and make rapid predictions (4). The concept of AI and uncertainty around its impact understandably create angst for some radiologists. After all, interpretation of medical images has recently become a Augmented Radiology: Looking Over the Horizon\"}, {\"paperId\": \"f4a4e0943f96317dfaf3a1e627b85d5ce22d2264\", \"abstract\": null}, {\"paperId\": \"93fb3b0d1b66d6fddbc603fc4f0de912059772fc\", \"abstract\": null}, {\"paperId\": \"cd7e0da4e2a87ce09b8acbdc9d61ec4d511a27fa\", \"abstract\": \"This Article proposes a novel and provocative analysis of judicial opinions that are published without indicating individual authorship. Our approach provides an unbiased, quantitative, and computer scientific answer to a problem that has long plagued legal commentators. * William Li is a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and a 2012 graduate of the Technology and Policy Program at the Massachusetts Institute of Technology (MIT). * Pablo Azar is a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT). * David Larochelle is an engineer at the Berkman Center for Internet & Society at Harvard University. * Phil Hill is a Fellow at the Berkman Center for Internet & Society at Harvard University and a 2013 J.D. Candidate at Harvard Law School. * James Cox was an associate with Jenner & Block LLP during drafting of this Article, and currently serves as an attorney for the United States government. * Robert C. Berwick is Professor of Computational Linguistics and Computer Science and Engineering in the Departments of Electrical Engineering and Computer Science and Brain and Cognitive Sciences, MIT. * Andrew W. Lo is the Charles E. and Susan T. Harris Professor at the MIT Sloan School of Management, Principal Investigator in the Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT), and a joint faculty in the MIT Electrical Engineering and Computer Science Department. \\u2020 We thank John Cox at MIT, Andy Sellars and Ryan Budish at the Berkman Center, and Philip C. Berwick at the Washington University in St. Louis Law School for their invaluable feedback, and Jayna Cummings for editorial assistance. 504 STANFORD TECHNOLOGY LAW REVIEW [Vol. 16:485 United States courts publish a shocking number of judicial opinions without divulging the author. Per curiam opinions, as traditionally and popularly conceived, are a means of quickly deciding uncontroversial cases in which all judges or justices are in agreement. Today, however, unattributed per curiam opinions often dispose of highly controversial issues, frequently over significant disagreement within the court. Obscuring authorship removes the sense of accountability for each decision\\u2019s outcome and the reasoning that led to it. Anonymity also makes it more difficult for scholars, historians, practitioners, political commentators, and\\u2014in the thirty-nine states with elected judges and justices\\u2014the electorate, to glean valuable information about legal decisionmakers and the way they make their decisions. The value of determining authorship for unsigned opinions has long been recognized but, until now, the methods of doing so have been cumbersome, imprecise, and altogether unsatisfactory. Our work uses natural language processing to predict authorship of judicial opinions that are unsigned or whose attribution is disputed. Using a dataset of Supreme Court opinions with known authorship, we identify key words and phrases that can, to a high degree of accuracy, predict authorship. Thus, our method makes accessible an important class of cases heretofore inaccessible. For illustrative purposes, we explain our process as applied to the Obamacare decision, in which the authorship of a joint dissent was subject to significant popular speculation. We conclude with a chart predicting the author of every unsigned per curiam opinion during the Roberts Court. INTRODUCTION....................................................................................................... 505 I. UNSIGNED OPINIONS ........................................................................................ 505 A. Historical Context of Unsigned Opinions .................................................. 506 B. Problems with Unsigned Opinions ............................................................. 508 C. Solving Attributional Questions the Old-Fashioned Way........................... 509 D. Solving Attributional Questions Algorithmically........................................ 510 II. TEST CASE: OBAMACARE................................................................................... 511 III. EXPERIMENTAL SETUP ..................................................................................... 514 A. Experimental Questions ............................................................................. 514 B. Data Preparation ....................................................................................... 515 C. Machine Learning System Overview .......................................................... 515 D. Design of Authorship Attribution System ................................................... 516 1. Document Representation ................................................................... 517 2. Model Selection ................................................................................... 518 3. Feature Selection ................................................................................ 520 IV. EMPIRICAL RESULTS AND DISCUSSION ............................................................. 522 A. Feature Sets and Classification Models ..................................................... 522 B. Comparison of Feature Selection Models .................................................. 522 C. Interpreting Authorship Attribution Model Scores ..................................... 523 D. Insights on Writing Styles ........................................................................... 524 E. Controlling for Clerks ................................................................................ 525 F. Authorship Prediction for Sebelius ............................................................ 526 G. Comparison to Predictions by Domain Experts ......................................... 527 H. Section-by-Section Analysis ....................................................................... 528 V. AUTHORSHIP PREDICTIONS FOR PER CURIAM OPINIONS OF THE ROBERTS COURT .............................................................................................................. 529 CONCLUSION .......................................................................................................... 533 Spring 2013] ALGORITHMIC ATTRIBUTION 505\"}, {\"paperId\": \"4fa0c20e5b9ce2f114b3d3d649873fa8037c8f8c\", \"abstract\": \"Recent and forthcoming advances in instrumentation, and giant new surveys, are creating astronomical data sets that are not amenable to the methods of analysis familiar to astronomers. Traditional methods are often inadequate not merely because of the size in bytes of the data sets, but also because of the complexity of modern data sets. Mathematical limitations of familiar algorithms and techniques in dealing with such data sets create a critical need for new paradigms for the representation, analysis and scientific visualization (as opposed to illustrative visualization) of heterogeneous, multiresolution data across application domains. Some of the problems presented by the new data sets have been addressed by other disciplines such as applied mathematics, statistics and machine learning and have been utilized by other sciences such as space-based geosciences. Unfortunately, valuable results pertaining to these problems are mostly to be found in publications outside of astronomy. Here we offer brief overviews of a number of concepts, techniques and developments that are vital to the analysis and visualization of complex datasets and images. One of the goals of this paper is to help bridge the gap between applied mathematics and artificial intelligence on the one side and astronomy on the other.\"}, {\"paperId\": \"ab95f9e5697a7619da9b4a8dbf81f338f2b2ec21\", \"abstract\": \"We consider the use of intelligent systems to address the long-standing medical problem of diagnostic differentiation between harmful (secondary) and benign (primary) headache conditions. In secondary headaches, conditions are caused by an underlying pathology, in contrast to primary headaches where the production of pain represents the sole constituent of the disorder. Conventional diagnostic paradigms carry an unacceptable risk of misdiagnosis, leaving patients open to potentially catastrophic consequences. Intelligent systems approaches, grounded in artificial intelligence, are adopted in this study as a potential means to unite contributions from multiple settings, including medicine, the life sciences, pervasive computation, sensor technologies, and autonomous intelligent agency, in the fight against headache uncertainty. In this paper, we therefore present the first steps in our research towards a data intensive, unified approach to headache dichotomisation. We begin by presenting a background to headache and its classification, followed by analysis of the space of confounding symptoms, in addition to the problem of primary and secondary condition discrimination. Finally, we proceed to report results of a preliminary case study, in which the epileptic seizure is considered as a manifestation of a headache confounding neuropathology. It was found that our classification approach, based on supervised machine learning, represents a promising direction, with a best area under curve test outcome of 0.915. We conclude that intelligent systems, in conjunction with biosignals, could be suitable for classification of a more general set of pathologies, while facilitating the medicalisation of arbitrary settings.\"}, {\"paperId\": \"c59f6b9b3644bd776433828aa88c6c0d00f42622\", \"abstract\": \"Expert systems are part of a general category of computer applications known as intelligence. Expert system are designed to solve complex problems. Expert Systems is a branch of AI designed to work within a particular domain. To solve expert-level problems, expert systems will need efficient access to a substantial domain knowledge base, and a reasoning mechanism to apply the knowledge to the problems they are given. Usually they will also need to be able to explain, to the users who rely on them, how they have reached their decisions. As an expert is a person who can solve a problem with the domain knowledge. This research paper introduces introduction, parts, application of expert system. and difference between forward chaining and Backward chaining and Exactly meaning of Chaining. ETL tools uses functionality to extract, transform and load data from one system into another system, but our expert advises they're not optimal for application-to-application communication. In artificial intelligence, an expert system is a computer system that emulates the decision-making ability of a human expert. The AI technology has become really advanced and its only matter of time when the machines will be able to learn almost anything. The machine learning algorithms are already very smart, however the Processing power has been a challenge in last decade .Now with the big data and distributed computing revolution this problem has become easy to solve. Many programmers and developers can start programming their own robots and other gadgets on their own. Artificial intelligence is a science and technology based on disciplines such as Computer Science, Biology, Psychology, Linguistics, Mathematics, and Engineering. A major thrust of AI is in the development of computer functions associated with human intelligence, such as reasoning, learning, and problem solving.\"}, {\"paperId\": \"9fd6621c52fbfff2778448515a83df000358597e\", \"abstract\": \"In 1989, B. F. Skinner told Joseph Rychlak that the greatest disappointment resulting from the 'cognitive revolution' was the turning of the human organism into a machine. Intrigued by this statement, Rychlak decided that after many years of formulation it was time to present his fundamentally teleological view of the human being, which he calls the 'logical learning theory' (LLT). In this new theoretical perspective the author re-presents such concepts as intention, purpose, and free will. Significant aspects of the 'mind-body' issue are explored here. Rychlak addresses teleological issues and provides a language for proper conceptualization. He uses experimental findings to support the notion of behavior as self-directed rather than mechanistic.In the process, Rychlak places LLT on the side of teleological explanation, in which concepts like free will, self-choice, purpose and intention are no longer dismissed. Rychlak compares LLT and existing formulations of behavior, including classical and operant conditioning, social learning theory, social constructionism, cognitive science, gestalt theories, and personality theories. Extensive research data and thorough discussions support Rychlak's theory. A glossary is also included. Joseph F. Rychlak is Maude C. Clarke Professor of humanistic psychology and professor of philosophy, Loyola University of Chicago. He is the author of \\\"Artificial Intelligence and \\\"Human Reason: A Teleological Critique\\\", \\\"The Psychology of Rigorous Humanism\\\", \\\"A Philosophy of Science for Personality Theory\\\", and \\\"Introduction to Personality and Psychotherapy\\\".\"}, {\"paperId\": \"55d0b6681645c91a4436a36d9aec4a1b3f542e97\", \"abstract\": \"Carbon nanotubes have an exciting array of applications which span mechanical, electrical, thermal and chemical/sensing. However, full exploitation is slowed by a lack of control over synthesis. Despite the two decades since the explosion of work in the area, progress in controlled production of nanotubes is impeded by our lack of understanding of the fundamental mechanisms of nucleation and growth. Our group has endeavored to develop a method that addresses the critical bottlenecks impeding the speed of research by taking advantage of advances in robotics, artificial intelligence, data sciences and in-situ/in-operando characterization. Our Autonomous Research System, ARES, is capable of designing, executing and evaluating its own CNT growth experiments. Artificial intelligence module based on random tree / genetic algorithm statistical approach analyses experimentally obtained kinetic parameters (rate, time constant, etc.) and proposes new experiments to achieve user-defined objective. These are then executed by ARES automatically and without human intervention, and fed back into the AI module to ensure machine learning. Recent experiments utilized maximum growth rate as an objective. The normalized difference between the objective and experimentally observed growth rates behaves in a fashion similar to what is typically seen in the control systems, with experimentally observed growth rate oscillating around the target. The convergence can be expressed via cumulative root mean square (RMS) of the rate difference. RMS increases initially (divergence), followed by consistent decrease after ~50 experiments, indicating convergence. That is, after some unsuccessful experimentation, ARES was better able to supply experimental conditions that achieved the objective growth rate. We take this as a clear demonstration of autonomous AI learning: convergence on the objective via closed-loop iterative experimentation without human intervention.\"}, {\"paperId\": \"eb9e1a33d206be45bd955504a05ae313a42e472e\", \"abstract\": \"The ability to recognize the plans and goals of other agents enables humans to reason about what other people are doing, why they are doing it, and what they will do next. This fundamental cognitive capability is also critical to interpersonal interactions because human communications presuppose an ability to understand the motivations of the participants and subjects of the discussion. As the complexity of human-machine interactions increases and automated systems become more intelligent, we strive to provide them with comparable intent recognition capabilities. Research addressing this problem is variously referred to as plan recognition, activity recognition, goal recognition, and intent recognition. This synergistic research area combines techniques from user modeling, computer vision, natural language understanding, probabilistic reasoning, and machine learning. Plan recognition algorithms play a crucial role in a wide variety of applications including smart homes, intelligent user interfaces, personal agent assistants, human-robot interaction, and video surveillance. Plan recognition research in computer science dates back at least thirty-five years, initially defined in a paper by Schmidt, Sridharan, and Goodson [64]. In the last ten years, significant new advances have been made on this problem by researchers in artificial intelligence and related areas. These advances have been driven by three primary factors: first, the pressing need for sophisticated and efficient plan recognition systems in a wide variety of applications; second, the development of new algorithmic techniques in probabilistic modeling, machine learning, and optimization (combined with more powerful computers to use these techniques); and finally, our increased ability to gather data about human activities. Recent research in the field is often divided into two subareas. \\u201cActivity recognition\\u201d focuses on the problem of dealing directly with noisy low-level data gathered by physical sensors such as cameras, wearable sensors, and instrumented user interfaces. The primary task in this space is to discover and extract interesting patterns in noisy sensory data that can be interpreted as meaningful activities. For example, an activity recognition system processing\"}]}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "# Write your code here\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "from google.colab import  drive\n",
        "drive.mount('/drive', force_remount=True)\n",
        "#Initializing the limit to 100 so that for everytime we hit the API we get only 100 records.\n",
        "limit=\"100\"\n",
        "#Declaring the URL\n",
        "url = \"https://api.semanticscholar.org/graph/v1/paper/search?query=machine%20learning+data%20science+artificial%20intelligence&fields=abstract&offset=\"\n",
        "appended_data = []\n",
        "for i in range(0,1000,100):\n",
        "  offset = str(i)\n",
        "  #Creating the dynamic url with offset changing for every iteration of loop\n",
        "  dynamic_url = url+offset+\"&limit=\"+limit\n",
        "  time.sleep(5)\n",
        "  #Calling get method from request library and sending url as parameter in get method to get the required data from API.\n",
        "  response = requests.get(dynamic_url,headers={\"x-api-key\":\"wMeN1WdWq75wh1hHr5CM34dDRJCaS5MQFD6s67kb\"})\n",
        "  #Converting the text response we got from the above to JSON format\n",
        "  print(response.text)\n",
        "  data_response = json.loads(response.text)['data']\n",
        "  #Converting the JSON data format to pandas Dataframe.\n",
        "  df = pd.DataFrame(data_response)\n",
        "  #df = df.dropna(axis=0,subset = ['abstract'])\n",
        "  #concatenating the dataframes for each offset to appended_data variable\n",
        "  appended_data.append(df)\n",
        "#print(appended_data[0])\n",
        "appended_data = pd.concat(appended_data,ignore_index=True)\n",
        "appended_data = pd.DataFrame(appended_data,columns=['paperId','abstract'])\n",
        "appended_data = appended_data.dropna(axis=0,subset = ['abstract'])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "# Convert the text data into a bag of words representation\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "doc_word_matrix = vectorizer.fit_transform(appended_data['abstract'])\n",
        "# Fit the LDA model to the data\n",
        "lda_model = LatentDirichletAllocation(n_components=10, random_state=0)\n",
        "lda_model.fit(doc_word_matrix)\n",
        "# Print the top 10 words in each topic\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    print(\"Topic #%d:\" % topic_idx)\n",
        "    print(\" \".join([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-11:-1]]))\n",
        "    print()\n",
        "\n",
        "# Cluster the documents\n",
        "kmeans_model = KMeans(n_clusters=10, random_state=0)\n",
        "doc_topic_matrix = lda_model.transform(doc_word_matrix)\n",
        "kmeans_model.fit(doc_topic_matrix)\n",
        "\n",
        "# Summarize and describe the topics for each cluster\n",
        "for cluster_idx in range(10):\n",
        "    print(\"Cluster #%d:\" % cluster_idx)\n",
        "    cluster_docs = appended_data[kmeans_model.labels_ == cluster_idx]\n",
        "    cluster_topics = doc_topic_matrix[kmeans_model.labels_ == cluster_idx].mean(axis=0)\n",
        "    top_topics = cluster_topics.argsort()[:-11:-1]\n",
        "    for topic_idx in top_topics:\n",
        "        print(\"Topic #%d: %f\" % (topic_idx, cluster_topics[topic_idx]))\n",
        "        print(\" \".join([vectorizer.get_feature_names_out()[i] for i in lda_model.components_[topic_idx].argsort()[:-11:-1]]))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGMnfpGWltyq",
        "outputId": "a16ddd18-b2b0-4ca5-d1d1-0dfdad83b1d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic #0:\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "\n",
            "Topic #1:\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "\n",
            "Topic #2:\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "\n",
            "Topic #3:\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "\n",
            "Topic #4:\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "\n",
            "Topic #5:\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "\n",
            "Topic #6:\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "\n",
            "Topic #7:\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "\n",
            "Topic #8:\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "\n",
            "Topic #9:\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster #0:\n",
            "Topic #4: 0.851462\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #0: 0.048671\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #1: 0.042033\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #9: 0.041038\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #3: 0.005494\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #6: 0.004557\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #7: 0.004031\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #5: 0.001050\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "Topic #2: 0.000832\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #8: 0.000832\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "\n",
            "Cluster #1:\n",
            "Topic #9: 0.936761\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #0: 0.013886\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #4: 0.008775\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #1: 0.008694\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #3: 0.008230\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #6: 0.007986\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #7: 0.006030\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #8: 0.005703\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #2: 0.002436\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #5: 0.001499\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "\n",
            "Cluster #2:\n",
            "Topic #1: 0.825864\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #9: 0.046902\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #0: 0.030970\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #4: 0.025071\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #6: 0.016224\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #5: 0.015931\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "Topic #7: 0.014514\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #2: 0.011702\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #8: 0.008432\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #3: 0.004390\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "\n",
            "Cluster #3:\n",
            "Topic #0: 0.765032\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #9: 0.074773\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #3: 0.034224\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #1: 0.032772\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #4: 0.026410\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #7: 0.019851\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #5: 0.017226\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "Topic #8: 0.014828\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #2: 0.007835\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #6: 0.007048\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "\n",
            "Cluster #4:\n",
            "Topic #7: 0.864621\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #0: 0.052960\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #9: 0.037812\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #1: 0.021240\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #3: 0.019301\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #2: 0.001066\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #4: 0.000948\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #8: 0.000684\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #6: 0.000684\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #5: 0.000684\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "\n",
            "Cluster #5:\n",
            "Topic #3: 0.767661\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #0: 0.069590\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #9: 0.068459\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #1: 0.038309\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #7: 0.015185\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #4: 0.013816\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #5: 0.008838\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "Topic #6: 0.008591\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #8: 0.007914\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #2: 0.001637\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "\n",
            "Cluster #6:\n",
            "Topic #8: 0.844022\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #9: 0.090574\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #4: 0.022496\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #1: 0.019787\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #3: 0.015760\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #0: 0.003709\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #7: 0.001366\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #2: 0.000762\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #6: 0.000762\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #5: 0.000762\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "\n",
            "Cluster #7:\n",
            "Topic #2: 0.753837\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #9: 0.091822\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #0: 0.066509\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #1: 0.044187\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #7: 0.030761\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #3: 0.010520\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #4: 0.000591\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #8: 0.000591\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #6: 0.000591\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #5: 0.000591\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "\n",
            "Cluster #8:\n",
            "Topic #6: 0.769713\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #9: 0.113976\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #0: 0.051743\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #7: 0.030028\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #1: 0.015223\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #3: 0.011293\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #4: 0.005829\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #2: 0.000732\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "Topic #8: 0.000732\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #5: 0.000732\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "\n",
            "Cluster #9:\n",
            "Topic #9: 0.524591\n",
            "learning data machine science intelligence artificial ai research deep computer\n",
            "Topic #0: 0.086924\n",
            "data ai ml science learning research artificial machine intelligence health\n",
            "Topic #4: 0.078348\n",
            "intelligence ai learning artificial machine science data human computer systems\n",
            "Topic #1: 0.071116\n",
            "data learning machine artificial intelligence materials algorithms models systems design\n",
            "Topic #5: 0.063820\n",
            "systems data intelligence human medical field artificial new algorithms big\n",
            "Topic #7: 0.056630\n",
            "ai data learning intelligence artificial machine research science health deep\n",
            "Topic #3: 0.053162\n",
            "data ai learning intelligence security artificial machine cancer information science\n",
            "Topic #6: 0.033328\n",
            "science systems data intelligence machine learning intelligent networks methods ml\n",
            "Topic #8: 0.019388\n",
            "intelligence artificial data machine new learning science social performance technologies\n",
            "Topic #2: 0.012694\n",
            "data mining fusion learning analysis machine algorithms using methods used\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Define a function to get sentiment score\n",
        "def get_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    sentiment = blob.sentiment.polarity\n",
        "    if sentiment > 0:\n",
        "        return 1\n",
        "    elif sentiment == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "# Apply the function to the DataFrame column and store the results in a new column\n",
        "appended_data['sentiment'] = appended_data['abstract'].apply(get_sentiment)"
      ],
      "metadata": {
        "id": "_UPqWsYesNH5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "appended_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "fLPZJV2Psahr",
        "outputId": "1bf50692-0e59-4b85-b864-991ffdbd7d8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      paperId  \\\n",
              "0    1e85606f75af68884122f0d1d8393c000e915194   \n",
              "2    72d3ddf1f7210d7e70144bbc09f770ec411fe909   \n",
              "3    a20c39f5439b84b57d883a21b698394d2347800d   \n",
              "4    ddb9ba8651e39d3c2e7a990c4b27049c20aff228   \n",
              "9    12d89245440d8c2a57c0741d10177189adf230d3   \n",
              "..                                        ...   \n",
              "995  ab95f9e5697a7619da9b4a8dbf81f338f2b2ec21   \n",
              "996  c59f6b9b3644bd776433828aa88c6c0d00f42622   \n",
              "997  9fd6621c52fbfff2778448515a83df000358597e   \n",
              "998  55d0b6681645c91a4436a36d9aec4a1b3f542e97   \n",
              "999  eb9e1a33d206be45bd955504a05ae313a42e472e   \n",
              "\n",
              "                                              abstract  sentiment  \n",
              "0    This Special Issue of the IEEE Transactions on...          1  \n",
              "2    Smarter applications are making better use of ...          1  \n",
              "3    Over the years, many clinical and engineering ...         -1  \n",
              "4    Machine learning (ML) is becoming capable of t...          1  \n",
              "9    Abstract The Petaflops supercomputer “Zhores” ...          1  \n",
              "..                                                 ...        ...  \n",
              "995  We consider the use of intelligent systems to ...          1  \n",
              "996  Expert systems are part of a general category ...          1  \n",
              "997  In 1989, B. F. Skinner told Joseph Rychlak tha...          1  \n",
              "998  Carbon nanotubes have an exciting array of app...          1  \n",
              "999  The ability to recognize the plans and goals o...          1  \n",
              "\n",
              "[775 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e074fd82-ff48-4a58-9f3d-60c349e66eb7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paperId</th>\n",
              "      <th>abstract</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1e85606f75af68884122f0d1d8393c000e915194</td>\n",
              "      <td>This Special Issue of the IEEE Transactions on...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>72d3ddf1f7210d7e70144bbc09f770ec411fe909</td>\n",
              "      <td>Smarter applications are making better use of ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a20c39f5439b84b57d883a21b698394d2347800d</td>\n",
              "      <td>Over the years, many clinical and engineering ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ddb9ba8651e39d3c2e7a990c4b27049c20aff228</td>\n",
              "      <td>Machine learning (ML) is becoming capable of t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>12d89245440d8c2a57c0741d10177189adf230d3</td>\n",
              "      <td>Abstract The Petaflops supercomputer “Zhores” ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>ab95f9e5697a7619da9b4a8dbf81f338f2b2ec21</td>\n",
              "      <td>We consider the use of intelligent systems to ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>c59f6b9b3644bd776433828aa88c6c0d00f42622</td>\n",
              "      <td>Expert systems are part of a general category ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>9fd6621c52fbfff2778448515a83df000358597e</td>\n",
              "      <td>In 1989, B. F. Skinner told Joseph Rychlak tha...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>55d0b6681645c91a4436a36d9aec4a1b3f542e97</td>\n",
              "      <td>Carbon nanotubes have an exciting array of app...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>eb9e1a33d206be45bd955504a05ae313a42e472e</td>\n",
              "      <td>The ability to recognize the plans and goals o...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>775 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e074fd82-ff48-4a58-9f3d-60c349e66eb7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e074fd82-ff48-4a58-9f3d-60c349e66eb7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e074fd82-ff48-4a58-9f3d-60c349e66eb7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "(1) Features used for sentiment classification and explain why you select these features.\n",
        "\n",
        "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vATjQNTY8buA"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(appended_data['abstract'], appended_data['sentiment'], test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "ZWCQPGEWqwL2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a bag of words representation of the data\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "train_features = vectorizer.fit_transform(train_data)\n",
        "test_features = vectorizer.transform(test_data)"
      ],
      "metadata": {
        "id": "Fmrrhbh4sjl-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Multinomial Naive Bayes classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_cv_scores = cross_val_score(nb_classifier, train_features, train_labels, cv=5)\n",
        "nb_classifier.fit(train_features, train_labels)\n",
        "nb_predictions = nb_classifier.predict(test_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AYwQ-v3sl5B",
        "outputId": "7a46208c-b5cf-44c5-c1eb-adfa370ae451"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Logistic Regression classifier\n",
        "lr_classifier = LogisticRegression(max_iter=1000)\n",
        "lr_cv_scores = cross_val_score(lr_classifier, train_features, train_labels, cv=5)\n",
        "lr_classifier.fit(train_features, train_labels)\n",
        "lr_predictions = lr_classifier.predict(test_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMWwaOpUsnSy",
        "outputId": "c61b0f48-7660-4302-8a3a-2c569952c505"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of the classifiers\n",
        "print(\"Multinomial Naive Bayes\")\n",
        "print(\"Accuracy: %.3f\" % accuracy_score(test_labels, nb_predictions))\n",
        "print(\"Precision: %.3f\" % precision_score(test_labels, nb_predictions, average='weighted'))\n",
        "print(\"Recall: %.3f\" % recall_score(test_labels, nb_predictions, average='weighted'))\n",
        "print(\"F1 Score: %.3f\" % f1_score(test_labels, nb_predictions, average='weighted'))\n",
        "print(\"Cross-Validation Scores:\", nb_cv_scores)\n",
        "print(\"Average Cross-Validation Score: %.3f\" % nb_cv_scores.mean())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUYM49xBsryc",
        "outputId": "f94c4cb6-1bf7-40c6-a111-e701ff271e98"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multinomial Naive Bayes\n",
            "Accuracy: 0.787\n",
            "Precision: 0.763\n",
            "Recall: 0.787\n",
            "F1 Score: 0.710\n",
            "Cross-Validation Scores: [0.76612903 0.77419355 0.77419355 0.75806452 0.78225806]\n",
            "Average Cross-Validation Score: 0.771\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logistic Regression\")\n",
        "print(\"Accuracy: %.3f\" % accuracy_score(test_labels, lr_predictions))\n",
        "print(\"Precision: %.3f\" % precision_score(test_labels, lr_predictions, average='weighted'))\n",
        "print(\"Recall: %.3f\" % recall_score(test_labels, lr_predictions, average='weighted'))\n",
        "print(\"F1 Score: %.3f\" % f1_score(test_labels, lr_predictions, average='weighted'))\n",
        "print(\"Cross-Validation Scores:\", lr_cv_scores)\n",
        "print(\"Average Cross-Validation Score: %.3f\" % lr_cv_scores.mean())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCn1uigTst_O",
        "outputId": "feff9955-ec30-4034-d093-6339bbf1aacf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression\n",
            "Accuracy: 0.813\n",
            "Precision: 0.794\n",
            "Recall: 0.813\n",
            "F1 Score: 0.796\n",
            "Cross-Validation Scores: [0.81451613 0.84677419 0.81451613 0.79032258 0.82258065]\n",
            "Average Cross-Validation Score: 0.818\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "XfvMKJjIXS5G",
        "outputId": "86c1500d-e9cb-4ffd-d6d5-d15c9854b8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-32b8ec33d7d5>:9: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  train.fillna(train.mean(), inplace=True)\n",
            "<ipython-input-32-32b8ec33d7d5>:10: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  test.fillna(test.mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  Alley  LotShape  \\\n",
              "0             60         3         65.0     8450       1      2         3   \n",
              "1             20         3         80.0     9600       1      2         3   \n",
              "2             60         3         68.0    11250       1      2         0   \n",
              "3             70         3         60.0     9550       1      2         0   \n",
              "4             60         3         84.0    14260       1      2         0   \n",
              "...          ...       ...          ...      ...     ...    ...       ...   \n",
              "1455          60         3         62.0     7917       1      2         3   \n",
              "1456          20         3         85.0    13175       1      2         3   \n",
              "1457          70         3         66.0     9042       1      2         3   \n",
              "1458          20         3         68.0     9717       1      2         3   \n",
              "1459          20         3         75.0     9937       1      2         3   \n",
              "\n",
              "      LandContour  Utilities  LotConfig  ...  PoolArea  PoolQC  Fence  \\\n",
              "0               3          0          4  ...         0       3      4   \n",
              "1               3          0          2  ...         0       3      4   \n",
              "2               3          0          4  ...         0       3      4   \n",
              "3               3          0          0  ...         0       3      4   \n",
              "4               3          0          2  ...         0       3      4   \n",
              "...           ...        ...        ...  ...       ...     ...    ...   \n",
              "1455            3          0          4  ...         0       3      4   \n",
              "1456            3          0          4  ...         0       3      2   \n",
              "1457            3          0          4  ...         0       3      0   \n",
              "1458            3          0          4  ...         0       3      4   \n",
              "1459            3          0          4  ...         0       3      4   \n",
              "\n",
              "      MiscFeature  MiscVal  MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
              "0               4        0       2    2008         8              4     208500  \n",
              "1               4        0       5    2007         8              4     181500  \n",
              "2               4        0       9    2008         8              4     223500  \n",
              "3               4        0       2    2006         8              0     140000  \n",
              "4               4        0      12    2008         8              4     250000  \n",
              "...           ...      ...     ...     ...       ...            ...        ...  \n",
              "1455            4        0       8    2007         8              4     175000  \n",
              "1456            4        0       2    2010         8              4     210000  \n",
              "1457            2     2500       5    2010         8              4     266500  \n",
              "1458            4        0       4    2010         8              4     142125  \n",
              "1459            4        0       6    2008         8              4     147500  \n",
              "\n",
              "[1460 rows x 80 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05e125a5-496d-418a-a980-d5bcc8aafa96\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>Alley</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>LandContour</th>\n",
              "      <th>Utilities</th>\n",
              "      <th>LotConfig</th>\n",
              "      <th>...</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>PoolQC</th>\n",
              "      <th>Fence</th>\n",
              "      <th>MiscFeature</th>\n",
              "      <th>MiscVal</th>\n",
              "      <th>MoSold</th>\n",
              "      <th>YrSold</th>\n",
              "      <th>SaleType</th>\n",
              "      <th>SaleCondition</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2008</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2007</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>2008</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>2008</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>60</td>\n",
              "      <td>3</td>\n",
              "      <td>62.0</td>\n",
              "      <td>7917</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2007</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>85.0</td>\n",
              "      <td>13175</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2010</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>210000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>70</td>\n",
              "      <td>3</td>\n",
              "      <td>66.0</td>\n",
              "      <td>9042</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2500</td>\n",
              "      <td>5</td>\n",
              "      <td>2010</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>266500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>68.0</td>\n",
              "      <td>9717</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>2010</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>142125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>75.0</td>\n",
              "      <td>9937</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2008</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>147500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 80 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05e125a5-496d-418a-a980-d5bcc8aafa96')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05e125a5-496d-418a-a980-d5bcc8aafa96 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05e125a5-496d-418a-a980-d5bcc8aafa96');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Write your code here\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "train = pd.read_csv('/content/drive/MyDrive/assignment4-question3-data/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/assignment4-question3-data/test.csv')\n",
        "\n",
        "# Fill missing values\n",
        "train.fillna(train.mean(), inplace=True)\n",
        "test.fillna(test.mean(), inplace=True)\n",
        "# create a label encoder object\n",
        "le = LabelEncoder()\n",
        "# Preprocessing\n",
        "train.drop(['Id'], axis=1, inplace=True)  # Remove Id column\n",
        "test.drop(['Id'], axis=1, inplace=True)\n",
        "train[['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']]=train[['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']].apply(le.fit_transform)\n",
        "test[['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']]=test[['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition']].apply(le.fit_transform)\n",
        "train\n",
        "#df = pd.get_dummies(train, columns=['MSZoning','Street','Alley','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood','Condition1','Condition2','BldgType','HouseStyle','RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','ExterQual','ExterCond','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','Heating','HeatingQC','CentralAir','Electrical','KitchenQual','Functional','FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split data into features and target\n",
        "train_X = train.iloc[:, :-1]  # All columns except last one (SalePrice)\n",
        "train_y = train.iloc[:, -1]  # SalePrice column"
      ],
      "metadata": {
        "id": "HYWPgA0QxdLd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(train_X, train_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "gsKj_u99xfC2",
        "outputId": "d164a219-d5b0-4116-ee1e-00a74905adc0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on test data\n",
        "test_X = test\n",
        "predictions = model.predict(test_X)"
      ],
      "metadata": {
        "id": "50V2mreY12L_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Evaluate model\n",
        "rmse = np.sqrt(mean_squared_error(train_y, model.predict(train_X)))\n",
        "r2 = r2_score(train_y, model.predict(train_X))\n",
        "\n",
        "print(\"RMSE: \", rmse)\n",
        "print(\"R2 score: \", r2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gtVckNw13JW",
        "outputId": "8e3c1005-bbee-4a21-ea9d-72fa3bf287f4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE:  30246.719624421326\n",
            "R2 score:  0.8549397945313106\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}