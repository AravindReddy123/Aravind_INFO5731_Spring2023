{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AravindReddy123/Aravind_INFO5731_Spring2023/blob/main/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2t_Gq3cMVV"
      },
      "source": [
        "# **The fourth in-class-exercise (40 points in total, 03/28/2022)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vszS_AFUcMVW"
      },
      "source": [
        "Question description: Please use the text corpus you collected in your last in-class-exercise for this exercise. Perform the following tasks:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PQIJbh5cMVX"
      },
      "source": [
        "## (1) (10 points) Generate K topics by using LDA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvM7gJqxcMVX"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models import CoherenceModel\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "documents = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "#documents = [...] # your list of documents\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Preprocess the documents\n",
        "stop_words = stopwords.words('english')\n",
        "preprocessed_documents = []\n",
        "for document in documents:\n",
        "    tokens = nltk.word_tokenize(document.lower())\n",
        "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    preprocessed_documents.append(tokens)\n",
        "\n",
        "# Create a dictionary and corpus\n",
        "dictionary = corpora.Dictionary(preprocessed_documents)\n",
        "corpus = [dictionary.doc2bow(document) for document in preprocessed_documents]\n",
        "\n",
        "# Compute coherence scores for different numbers of topics\n",
        "coherence_scores = []\n",
        "for k in range(2, 20):\n",
        "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k)\n",
        "    coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model_lda.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "    print(f\"Number of topics: {k}, coherence score: {coherence_score}\")\n",
        "\n",
        "# Select the value of K that maximizes the coherence score\n",
        "best_k = coherence_scores.index(max(coherence_scores)) + 2\n",
        "\n",
        "# Train the final LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=best_k)\n",
        "\n",
        "# Print the topics\n",
        "topics = lda_model.show_topics(num_topics=best_k, formatted=False)\n",
        "for i, topic in enumerate(topics):\n",
        "    print(f\"Topic {i}: {' '.join([word[0] for word in topic[1]])}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gWnQ7-VcMVY"
      },
      "source": [
        "## (2) (10 points) Generate K topics by using LSA, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSSS0C3VcMVY"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim import corpora, models\n",
        "from gensim.models import CoherenceModel\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load the corpus and preprocess the text\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "documents = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "texts = [[word for word in document.lower().split() if word not in stop_words] for document in documents]\n",
        "\n",
        "# Create a dictionary and a corpus\n",
        "dictionary = corpora.Dictionary(texts)\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "# Determine the optimal number of topics based on coherence score\n",
        "coherence_scores = []\n",
        "for k in range(1, 10):\n",
        "    lsa_model = models.LsiModel(corpus, num_topics=k, id2word=dictionary)\n",
        "    coherence_model = CoherenceModel(model=lsa_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_scores.append(coherence_model.get_coherence())\n",
        "\n",
        "optimal_k = coherence_scores.index(max(coherence_scores)) + 1\n",
        "\n",
        "# Generate K topics using LSA\n",
        "lsa_model = models.LsiModel(corpus, num_topics=optimal_k, id2word=dictionary)\n",
        "topics = lsa_model.print_topics(num_topics=optimal_k)\n",
        "\n",
        "# Print the topics\n",
        "for topic in topics:\n",
        "    print(topic)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LztMlTX6gbW8"
      },
      "outputs": [],
      "source": [
        "pip install pyLDAvis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCaAem69cMVZ"
      },
      "source": [
        "## (3) (10 points) Generate K topics by using  lda2vec, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here:\n",
        "\n",
        "https://nbviewer.org/github/cemoody/lda2vec/blob/master/examples/twenty_newsgroups/lda2vec/lda2vec.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FpUaGNkAcMVZ"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import lda2vec\n",
        "import lda2vec.utils\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Load your corpus, in this example, I'll use a sample dataset of news articles\n",
        "#corpus_df = pd.read_csv('news_corpus.csv')\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "print(newsgroups)\n",
        "corpus_df = newsgroups.data\n",
        "print(corpus_df)\n",
        "\n",
        "# Preprocess the corpus (tokenization, stopword removal, stemming, etc.)\n",
        "# Here's an example using spaCy library\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-']\n",
        "\n",
        "corpus = [tokenize(doc) for doc in corpus_df['text']]\n",
        "\n",
        "# Set up corpus dictionary and mapping for LDA2Vec\n",
        "vocab = lda2vec.utils.flatten(corpus)\n",
        "word2id = dict(zip(vocab, range(len(vocab))))\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "corpus = [np.array([word2id[word] for word in doc]) for doc in corpus]\n",
        "\n",
        "# Train LDA2Vec model and determine optimal number of topics using coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 20):\n",
        "    # Train LDA2Vec with k topics\n",
        "    model = lda2vec.LDA2Vec(n_topics=k, n_iter=500, random_state=42)\n",
        "    model.fit(corpus)\n",
        "\n",
        "    # Calculate coherence score\n",
        "    m = model.mixture_means\n",
        "    s = model.mixture_variances\n",
        "    topics = m.dot(s).argsort()[:, ::-1]\n",
        "    tokens = collections.defaultdict(list)\n",
        "    for i, topic in enumerate(topics):\n",
        "        tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "    coherence_score = lda2vec.utils.evaluate(tokens)\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Select number of topics with highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train LDA2Vec model with optimal number of topics\n",
        "model = lda2vec.LDA2Vec(n_topics=optimal_k, n_iter=500, random_state=42)\n",
        "model.fit(corpus)\n",
        "\n",
        "# Print topics and top 10 words in each topic\n",
        "m = model.mixture_means\n",
        "s = model.mixture_variances\n",
        "topics = m.dot(s).argsort()[:, ::-1]\n",
        "tokens = collections.defaultdict(list)\n",
        "for i, topic in enumerate(topics):\n",
        "    tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "for i, words in tokens.items():\n",
        "    print(\"Topic {}: {}\".format(i+1, \", \".join(words)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlZNWtOCGhSY"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import collections\n",
        "import random\n",
        "import lda2vec\n",
        "import lda2vec.utils\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from lda2vec import Corpus\n",
        "# Load your corpus, in this example, I'll use a sample dataset of news articles\n",
        "#corpus_df = pd.read_csv('news_corpus.csv')\n",
        "#categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "#newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "#print(newsgroups)\n",
        "corpus_df =documents = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "print(corpus_df)\n",
        "\n",
        "# Preprocess the corpus (tokenization, stopword removal, stemming, etc.)\n",
        "# Here's an example using spaCy library\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "\n",
        "def tokenize(text):\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.lemma_ != '-PRON-']\n",
        "\n",
        "corpus = [tokenize(doc) for doc in corpus_df['content']]\n",
        "\n",
        "# Set up corpus dictionary and mapping for LDA2Vec\n",
        "corpus1 = Corpus()\n",
        "vocab = corpus1.compact_to_flat(corpus)\n",
        "word2id = dict(zip(vocab, range(len(vocab))))\n",
        "id2word = {v: k for k, v in word2id.items()}\n",
        "corpus = [np.array([word2id[word] for word in doc]) for doc in corpus]\n",
        "\n",
        "# Train LDA2Vec model and determine optimal number of topics using coherence score\n",
        "coherence_scores = {}\n",
        "for k in range(2, 20):\n",
        "    # Train LDA2Vec with k topics\n",
        "    model = lda2vec.LDA2Vec(n_topics=k, n_iter=500, random_state=42)\n",
        "    model.fit(corpus)\n",
        "\n",
        "    # Calculate coherence score\n",
        "    m = model.mixture_means\n",
        "    s = model.mixture_variances\n",
        "    topics = m.dot(s).argsort()[:, ::-1]\n",
        "    tokens = collections.defaultdict(list)\n",
        "    for i, topic in enumerate(topics):\n",
        "        tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "    coherence_score = lda2vec.utils.evaluate(tokens)\n",
        "    coherence_scores[k] = coherence_score\n",
        "\n",
        "# Select number of topics with highest coherence score\n",
        "optimal_k = max(coherence_scores, key=coherence_scores.get)\n",
        "print(\"Optimal number of topics:\", optimal_k)\n",
        "\n",
        "# Train LDA2Vec model with optimal number of topics\n",
        "model = lda2vec.LDA2Vec(n_topics=optimal_k, n_iter=500, random_state=42)\n",
        "model.fit(corpus)\n",
        "\n",
        "# Print topics and top 10 words in each topic\n",
        "m = model.mixture_means\n",
        "s = model.mixture_variances\n",
        "topics = m.dot(s).argsort()[:, ::-1]\n",
        "tokens = collections.defaultdict(list)\n",
        "for i, topic in enumerate(topics):\n",
        "    tokens[topic[0]].extend([id2word[token] for token in topic[1:11]])\n",
        "for i, words in tokens.items():\n",
        "    print(\"Topic {}: {}\".format(i+1, \", \".join(words)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8RRh2h2iAw6"
      },
      "outputs": [],
      "source": [
        "pip install bertopic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gZ2G9scMVa"
      },
      "source": [
        "## (4) (10 points) Generate K topics by using BERTopic, the number of topics K should be decided by the coherence score, then summarize what are the topics. You may refer the code here: \n",
        "\n",
        "https://colab.research.google.com/drive/1FieRA9fLdkQEGDIMYl0I3MCjSUKVF8C-?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mg_t9_QacMVa"
      },
      "outputs": [],
      "source": [
        "# Write your code here\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gensim\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Load the corpus\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "texts = newsgroups.data\n",
        "# Create an instance of BERTopic\n",
        "model = BERTopic()\n",
        "# Determine the optimal number of topics based on coherence score\n",
        "coherence_scores = {}\n",
        "model.fit(texts)\n",
        "topics, _ = model.transform(texts)\n",
        "topic_words = model.get_topic_info()\n",
        "topics_words_list = [topic_words[topic][\"tokens\"].split() for topic in topic_words.index]\n",
        "dictionary = gensim.corpora.Dictionary(topics_words_list)\n",
        "corpus = [dictionary.doc2bow(words) for words in topics_words_list]\n",
        "    #coherence_scores[k] = model.get_coherence_per_topic()\n",
        "coherence_scores = []\n",
        "for k in range(2, 11):\n",
        "    # Create coherence model\n",
        "    coherence_model = gensim.models.CoherenceModel(topics=topics_words_list, corpus=corpus, dictionary=dictionary, coherence='c_v', num_topics=k)\n",
        "    \n",
        "    # Calculate coherence score\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "    print(f\"K={k}: Coherence Score={coherence_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3VewjwGcMVb"
      },
      "source": [
        "## (5) (10 extra points) Compare the results generated by the four topic modeling algorithms, which one is better? You should explain the reasons in details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osCRqGePcMVb"
      },
      "outputs": [],
      "source": [
        "# Write your answer here (no code needed for this question)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}